2025-04-15 16:04:28,435 - INFO - Loading environment variables...
2025-04-15 16:04:28,746 - INFO - Start Processing file: theory_evaluation\llm_handler.py
2025-04-15 16:04:31,498 - INFO - function_names - ['OpenAI_llm']
2025-04-15 16:04:31,498 - INFO - source_code_path - theory_evaluation\llm_handler.py
2025-04-15 16:04:31,499 - INFO - function_names - ['OpenAI_llm']
2025-04-15 16:04:31,503 - INFO - 
2025-04-15 16:04:31,503 - INFO - function_names - ['OpenAI_llm']
2025-04-15 16:05:05,693 - INFO - 

2025-04-15 16:05:05,693 - INFO - TEST CASE 1 Retry 0
2025-04-15 16:05:05,693 - INFO - ---------------
2025-04-15 16:05:05,694 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest


@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True, message="Test message", output="json")
        assert llm.message == "Test message"
        assert llm.output == "json"
        assert llm.client == mock_azure_openai.return_value

2025-04-15 16:05:05,696 - INFO - ---------------
2025-04-15 16:05:07,521 - INFO - Test Result 1- False
2025-04-15 16:05:07,521 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:12: in test_openai_llm_initialization
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - NameError: name 'patch'...
1 failed in 1.13s
2025-04-15 16:05:11,045 - INFO - TEST CASE 1 Retry 1
2025-04-15 16:05:11,045 - INFO - ---------------
2025-04-15 16:05:11,045 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True, message="Test message", output="json")
        assert llm.message == "Test message"
        assert llm.output == "json"
        assert llm.client == mock_azure_openai.return_value

2025-04-15 16:05:11,046 - INFO - ---------------
2025-04-15 16:05:13,285 - INFO - Test Result 2- False
2025-04-15 16:05:13,285 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:17: in test_openai_llm_initialization
    llm = OpenAI_llm(useAzureOpenAI=True, message="Test message", output="json")
E   NameError: name 'OpenAI_llm' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - NameError: name 'OpenAI...
1 failed in 1.30s
2025-04-15 16:05:16,334 - INFO - TEST CASE 1 Retry 2
2025-04-15 16:05:16,335 - INFO - ---------------
2025-04-15 16:05:16,335 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True, message="Test message", output="json")
        assert llm.message == "Test message"
        assert llm.output == "json"
        assert llm.client == mock_azure_openai.return_value

2025-04-15 16:05:16,335 - INFO - ---------------
2025-04-15 16:05:18,919 - INFO - Test Result 3- False
2025-04-15 16:05:18,919 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:10: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.54s
2025-04-15 16:05:18,920 - INFO - Failed after all retries for test case 1
2025-04-15 16:05:18,920 - INFO - 

2025-04-15 16:05:18,920 - INFO - TEST CASE 2 Retry 0
2025-04-15 16:05:18,920 - INFO - ---------------
2025-04-15 16:05:18,920 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest


@pytest.mark.asyncio
async def test_openai_json_completion():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything"})
        
        llm = OpenAI_llm(message="Test message", output="json")
        result = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert result == {"answer": "42", "explanation": "The answer to everything"}

2025-04-15 16:05:18,920 - INFO - ---------------
2025-04-15 16:05:20,971 - INFO - Test Result 1- False
2025-04-15 16:05:20,971 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_json_completion _________________________
temp\temp.py:12: in test_openai_json_completion
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_json_completion - NameError: name 'patch' is...
1 failed in 1.08s
2025-04-15 16:05:23,025 - INFO - TEST CASE 2 Retry 1
2025-04-15 16:05:23,026 - INFO - ---------------
2025-04-15 16:05:23,026 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch, AsyncMock
import pytest
import json

@pytest.mark.asyncio
async def test_openai_json_completion():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything"})
        
        llm = OpenAI_llm(message="Test message", output="json")
        result = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert result == {"answer": "42", "explanation": "The answer to everything"}

2025-04-15 16:05:23,026 - INFO - ---------------
2025-04-15 16:05:25,236 - INFO - Test Result 2- False
2025-04-15 16:05:25,237 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_json_completion _________________________
temp\temp.py:15: in test_openai_json_completion
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1479: in __enter__
    self.target = self.getter()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\pkgutil.py:528: in resolve_name
    result = getattr(result, p)
E   AttributeError: type object 'OpenAI_llm' has no attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_json_completion - AttributeError: type objec...
1 failed in 1.22s
2025-04-15 16:05:27,447 - INFO - TEST CASE 2 Retry 2
2025-04-15 16:05:27,448 - INFO - ---------------
2025-04-15 16:05:27,448 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch, AsyncMock
import pytest
import json
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_json_completion():
    with patch("openai.OpenAI.chat.completions.create", new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything"})
        
        llm = OpenAI_llm(message="Test message", output="json")
        result = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert result == {"answer": "42", "explanation": "The answer to everything"}

2025-04-15 16:05:27,448 - INFO - ---------------
2025-04-15 16:05:29,216 - INFO - Test Result 3- False
2025-04-15 16:05:29,216 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:12: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.08s
2025-04-15 16:05:29,216 - INFO - Failed after all retries for test case 2
2025-04-15 16:05:29,216 - INFO - 

2025-04-15 16:05:29,216 - INFO - TEST CASE 3 Retry 0
2025-04-15 16:05:29,217 - INFO - ---------------
2025-04-15 16:05:29,217 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest


@pytest.mark.asyncio
async def test_openai_streaming():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
        mock_create.return_value = [{"choices": [{"delta": {"content": "chunk1"}}]}, {"choices": [{"delta": {"content": "chunk2"}}]}]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        chunks = [chunk async for chunk in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}])]
        
        assert chunks == ["chunk1", "chunk2"]

2025-04-15 16:05:29,217 - INFO - ---------------
2025-04-15 16:05:31,034 - INFO - Test Result 1- False
2025-04-15 16:05:31,034 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_openai_streaming ____________________________
temp\temp.py:12: in test_openai_streaming
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_streaming - NameError: name 'patch' is not d...
1 failed in 1.06s
2025-04-15 16:05:33,260 - INFO - TEST CASE 3 Retry 1
2025-04-15 16:05:33,261 - INFO - ---------------
2025-04-15 16:05:33,261 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_openai_streaming():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
        mock_create.return_value = [{"choices": [{"delta": {"content": "chunk1"}}]}, {"choices": [{"delta": {"content": "chunk2"}}]}]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        chunks = [chunk async for chunk in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}])]
        
        assert chunks == ["chunk1", "chunk2"]

2025-04-15 16:05:33,261 - INFO - ---------------
2025-04-15 16:05:35,517 - INFO - Test Result 2- False
2025-04-15 16:05:35,517 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_openai_streaming ____________________________
temp\temp.py:13: in test_openai_streaming
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1479: in __enter__
    self.target = self.getter()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\pkgutil.py:528: in resolve_name
    result = getattr(result, p)
E   AttributeError: type object 'OpenAI_llm' has no attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_streaming - AttributeError: type object 'Ope...
1 failed in 1.27s
2025-04-15 16:05:37,472 - INFO - TEST CASE 3 Retry 2
2025-04-15 16:05:37,472 - INFO - ---------------
2025-04-15 16:05:37,473 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

import pytest
from unittest.mock import patch, AsyncMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_streaming():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
        mock_create.return_value = [{"choices": [{"delta": {"content": "chunk1"}}]}, {"choices": [{"delta": {"content": "chunk2"}}]}]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        chunks = [chunk async for chunk in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}])]
        
        assert chunks == ["chunk1", "chunk2"]

2025-04-15 16:05:37,473 - INFO - ---------------
2025-04-15 16:05:40,130 - INFO - Test Result 3- False
2025-04-15 16:05:40,130 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_openai_streaming ____________________________
temp\temp.py:15: in test_openai_streaming
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1479: in __enter__
    self.target = self.getter()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\pkgutil.py:528: in resolve_name
    result = getattr(result, p)
E   AttributeError: type object 'OpenAI_llm' has no attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_streaming - AttributeError: type object 'Ope...
1 failed in 1.56s
2025-04-15 16:05:40,130 - INFO - Failed after all retries for test case 3
2025-04-15 16:05:40,130 - INFO - 

2025-04-15 16:05:40,130 - INFO - TEST CASE 4 Retry 0
2025-04-15 16:05:40,130 - INFO - ---------------
2025-04-15 16:05:40,130 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest


@pytest.mark.asyncio
async def test_openai_chat_completion():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices[0].message.content = "Chat completion response"
        
        llm = OpenAI_llm(message="Test message", output=None)
        result = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert result == "Chat completion response"

2025-04-15 16:05:40,130 - INFO - ---------------
2025-04-15 16:05:42,427 - INFO - Test Result 1- False
2025-04-15 16:05:42,427 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_chat_completion _________________________
temp\temp.py:12: in test_openai_chat_completion
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_chat_completion - NameError: name 'patch' is...
1 failed in 1.14s
2025-04-15 16:05:46,104 - INFO - TEST CASE 4 Retry 1
2025-04-15 16:05:46,104 - INFO - ---------------
2025-04-15 16:05:46,105 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_openai_chat_completion():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices[0].message.content = "Chat completion response"
        
        llm = OpenAI_llm(message="Test message", output=None)
        result = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert result == "Chat completion response"

2025-04-15 16:05:46,105 - INFO - ---------------
2025-04-15 16:05:48,523 - INFO - Test Result 2- False
2025-04-15 16:05:48,524 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_chat_completion _________________________
temp\temp.py:13: in test_openai_chat_completion
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1479: in __enter__
    self.target = self.getter()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\pkgutil.py:528: in resolve_name
    result = getattr(result, p)
E   AttributeError: type object 'OpenAI_llm' has no attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_chat_completion - AttributeError: type objec...
1 failed in 1.42s
2025-04-15 16:05:50,303 - INFO - TEST CASE 4 Retry 2
2025-04-15 16:05:50,303 - INFO - ---------------
2025-04-15 16:05:50,303 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_openai_chat_completion():
    with patch("openai.OpenAI.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices[0].message.content = "Chat completion response"
        
        llm = OpenAI_llm(message="Test message", output=None)
        result = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert result == "Chat completion response"

2025-04-15 16:05:50,304 - INFO - ---------------
2025-04-15 16:05:52,357 - INFO - Test Result 3- False
2025-04-15 16:05:52,357 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_chat_completion _________________________
temp\temp.py:14: in test_openai_chat_completion
    with patch("openai.OpenAI.client.chat.completions.create", new_callable=AsyncMock) as mock_create:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1479: in __enter__
    self.target = self.getter()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\pkgutil.py:528: in resolve_name
    result = getattr(result, p)
E   AttributeError: type object 'OpenAI' has no attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_chat_completion - AttributeError: type objec...
1 failed in 1.11s
2025-04-15 16:05:52,357 - INFO - Failed after all retries for test case 4
2025-04-15 16:05:52,357 - INFO - 

2025-04-15 16:05:52,357 - INFO - TEST CASE 5 Retry 0
2025-04-15 16:05:52,357 - INFO - ---------------
2025-04-15 16:05:52,357 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest


@pytest.mark.asyncio
async def test_execute_text_generation():
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = ["response1", "response2"]
        
        llm = OpenAI_llm(message="Test message", mode="text_generation", output="json")
        responses = [response async for response in llm.execute()]
        
        assert responses == ["response1", "response2"]

2025-04-15 16:05:52,357 - INFO - ---------------
2025-04-15 16:05:54,673 - INFO - Test Result 1- False
2025-04-15 16:05:54,674 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_execute_text_generation _________________________
temp\temp.py:12: in test_execute_text_generation
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_text_generation - NameError: name 'patch' i...
1 failed in 1.34s
2025-04-15 16:05:56,625 - INFO - TEST CASE 5 Retry 1
2025-04-15 16:05:56,626 - INFO - ---------------
2025-04-15 16:05:56,627 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_execute_text_generation():
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = ["response1", "response2"]
        
        llm = OpenAI_llm(message="Test message", mode="text_generation", output="json")
        responses = [response async for response in llm.execute()]
        
        assert responses == ["response1", "response2"]

2025-04-15 16:05:56,627 - INFO - ---------------
2025-04-15 16:05:59,211 - INFO - Test Result 2- False
2025-04-15 16:05:59,212 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_execute_text_generation _________________________
temp\temp.py:17: in test_execute_text_generation
    llm = OpenAI_llm(message="Test message", mode="text_generation", output="json")
E   NameError: name 'OpenAI_llm' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_text_generation - NameError: name 'OpenAI_l...
1 failed in 1.59s
2025-04-15 16:06:01,568 - INFO - TEST CASE 5 Retry 2
2025-04-15 16:06:01,569 - INFO - ---------------
2025-04-15 16:06:01,571 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch, AsyncMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_execute_text_generation():
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = ["response1", "response2"]
        
        llm = OpenAI_llm(message="Test message", mode="text_generation", output="json")
        responses = [response async for response in llm.execute()]
        
        assert responses == ["response1", "response2"]

2025-04-15 16:06:01,572 - INFO - ---------------
2025-04-15 16:06:03,969 - INFO - Test Result 3- False
2025-04-15 16:06:03,969 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_execute_text_generation _________________________
temp\temp.py:21: in test_execute_text_generation
    assert responses == ["response1", "response2"]
E   assert ["Error: 'asy...ot coroutine"] == ['response1', 'response2']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'response1'
E     Right contains one more item: 'response2'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_text_generation
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_text_generation - assert ["Error: 'asy...ot...
1 failed, 1 warning in 1.48s
2025-04-15 16:06:03,970 - INFO - Failed after all retries for test case 5
2025-04-15 16:06:03,970 - INFO - 

2025-04-15 16:06:03,970 - INFO - TEST CASE 6 Retry 0
2025-04-15 16:06:03,970 - INFO - ---------------
2025-04-15 16:06:03,970 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest


@pytest.mark.asyncio
async def test_execute_vision():
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = ["response1", "response2"]
        
        llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image_input", output="json")
        responses = [response async for response in llm.execute()]
        
        assert responses == ["response1", "response2"]

2025-04-15 16:06:03,970 - INFO - ---------------
2025-04-15 16:06:05,694 - INFO - Test Result 1- False
2025-04-15 16:06:05,694 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________________ test_execute_vision _____________________________
temp\temp.py:12: in test_execute_vision
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_vision - NameError: name 'patch' is not def...
1 failed in 0.98s
2025-04-15 16:06:07,916 - INFO - TEST CASE 6 Retry 1
2025-04-15 16:06:07,917 - INFO - ---------------
2025-04-15 16:06:07,919 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_execute_vision():
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = ["response1", "response2"]
        
        llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image_input", output="json")
        responses = [response async for response in llm.execute()]
        
        assert responses == ["response1", "response2"]

2025-04-15 16:06:07,920 - INFO - ---------------
2025-04-15 16:06:09,728 - INFO - Test Result 2- False
2025-04-15 16:06:09,728 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________________ test_execute_vision _____________________________
temp\temp.py:17: in test_execute_vision
    llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image_input", output="json")
E   NameError: name 'OpenAI_llm' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_vision - NameError: name 'OpenAI_llm' is no...
1 failed in 0.99s
2025-04-15 16:06:12,831 - INFO - TEST CASE 6 Retry 2
2025-04-15 16:06:12,831 - INFO - ---------------
2025-04-15 16:06:12,831 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest

from unittest.mock import patch, AsyncMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_execute_vision():
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = ["response1", "response2"]
        
        llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image_input", output="json")
        responses = [response async for response in llm.execute()]
        
        assert responses == ["response1", "response2"]

2025-04-15 16:06:12,832 - INFO - ---------------
2025-04-15 16:06:15,874 - INFO - Test Result 3- False
2025-04-15 16:06:15,875 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________________ test_execute_vision _____________________________
temp\temp.py:21: in test_execute_vision
    assert responses == ["response1", "response2"]
E   assert ["Error: 'asy...ot coroutine"] == ['response1', 'response2']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'response1'
E     Right contains one more item: 'response2'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_vision
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_vision - assert ["Error: 'asy...ot coroutin...
1 failed, 1 warning in 2.03s
2025-04-15 16:06:15,876 - INFO - Failed after all retries for test case 6
2025-04-15 16:06:15,883 - INFO - End Processing file: theory_evaluation\llm_handler.py

2025-04-15 16:06:15,884 - INFO - Start Processing file: theory_evaluation\llm_utils.py
2025-04-15 16:06:16,381 - INFO - function_names - ['initialise_prompt', 'initialise_settings']
2025-04-15 16:06:16,381 - INFO - source_code_path - theory_evaluation\llm_utils.py
2025-04-15 16:06:16,381 - INFO - function_names - ['initialise_prompt', 'initialise_settings']
2025-04-15 16:06:16,381 - INFO - from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
2025-04-15 16:06:16,381 - INFO - function_names - ['initialise_prompt', 'initialise_settings']
2025-04-15 16:06:23,113 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 16:06:28,190 - INFO - End Processing file: theory_evaluation\llm_utils.py

2025-04-15 16:06:28,190 - INFO - Start Processing file: theory_evaluation\__init__.py
2025-04-15 16:06:28,191 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

