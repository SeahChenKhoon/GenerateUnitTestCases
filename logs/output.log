2025-04-15 16:55:12,688 - INFO - Loading environment variables...
2025-04-15 16:55:13,024 - INFO - Start Processing file: theory_evaluation\llm_handler.py
2025-04-15 16:55:35,365 - INFO - 

2025-04-15 16:55:35,366 - INFO - TEST CASE 1 Retry 0
2025-04-15 16:55:35,367 - INFO - ---------------
2025-04-15 16:55:35,367 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", return_value="mock_value"):
        llm = OpenAI_llm(useAzureOpenAI=True, message="Test message")
        assert llm.message == "Test message"
        assert llm.client == mock_azure_openai.return_value

2025-04-15 16:55:35,368 - INFO - ---------------
2025-04-15 16:55:37,256 - INFO - Test Result 1- False
2025-04-15 16:55:37,256 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:12: in test_openai_llm_initialization
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - NameError: name 'patch'...
1 failed in 1.15s
2025-04-15 16:55:41,419 - INFO - TEST CASE 1 Retry 1
2025-04-15 16:55:41,420 - INFO - ---------------
2025-04-15 16:55:41,420 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

from unittest.mock import patch

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", return_value="mock_value"):
        llm = OpenAI_llm(useAzureOpenAI=True, message="Test message")
        assert llm.message == "Test message"
        assert llm.client == mock_azure_openai.return_value

2025-04-15 16:55:41,420 - INFO - ---------------
2025-04-15 16:55:44,551 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 16:55:44,551 - INFO - New import Statements 2 - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
2025-04-15 16:55:44,551 - INFO - Test Result 2- True
2025-04-15 16:55:44,551 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
.                                                                        [100%]
1 passed in 1.27s
2025-04-15 16:55:44,551 - INFO - 

2025-04-15 16:55:44,552 - INFO - TEST CASE 2 Retry 0
2025-04-15 16:55:44,552 - INFO - ---------------
2025-04-15 16:55:44,552 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = '{"answer": "42", "explanation": "The answer to life."}'
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_JSON_Completion()
        assert result == {"answer": "42", "explanation": "The answer to life."}

2025-04-15 16:55:44,552 - INFO - ---------------
2025-04-15 16:55:47,850 - INFO - Test Result 1- False
2025-04-15 16:55:47,851 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_json_completion ____________________
temp\temp.py:13: in test_openai_llm_openai_json_completion
    mock_response = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_json_completion - NameError: name...
1 failed in 2.03s
2025-04-15 16:55:50,738 - INFO - TEST CASE 2 Retry 1
2025-04-15 16:55:50,738 - INFO - ---------------
2025-04-15 16:55:50,739 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

from unittest.mock import MagicMock, patch

@pytest.mark.asyncio
async def test_openai_llm_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = '{"answer": "42", "explanation": "The answer to life."}'
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_JSON_Completion()
        assert result == {"answer": "42", "explanation": "The answer to life."}

2025-04-15 16:55:50,739 - INFO - ---------------
2025-04-15 16:55:52,524 - INFO - Test Result 2- False
2025-04-15 16:55:52,525 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_json_completion ____________________
temp\temp.py:16: in test_openai_llm_openai_json_completion
    with patch.object(OpenAI_llm, "client") as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_json_completion - AttributeError:...
1 failed in 1.05s
2025-04-15 16:55:55,432 - INFO - TEST CASE 2 Retry 2
2025-04-15 16:55:55,433 - INFO - ---------------
2025-04-15 16:55:55,433 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

from unittest.mock import MagicMock, patch
import pytest

@pytest.mark.asyncio
async def test_openai_llm_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = '{"answer": "42", "explanation": "The answer to life."}'
    with patch.object(OpenAI_llm, "client", create=True) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}], model=llm.model_name, **llm.config)
        assert result == {"answer": "42", "explanation": "The answer to life."}

2025-04-15 16:55:55,433 - INFO - ---------------
2025-04-15 16:55:58,520 - INFO - Test Result 3- False
2025-04-15 16:55:58,521 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_json_completion ____________________
temp\temp.py:21: in test_openai_llm_openai_json_completion
    assert result == {"answer": "42", "explanation": "The answer to life."}
E   AssertionError: assert None == {'answer': '42', 'explanation': 'The answer to life.'}
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_JSON_Completion: Error code: 400 - {'error': {'message': 'you must provide a model parameter', 'type': 'invalid_request_error', 'param': None, 'code': None}}
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_json_completion - AssertionError:...
1 failed in 2.26s
2025-04-15 16:55:58,521 - INFO - Failed after all retries for test case 2
2025-04-15 16:55:58,521 - INFO - 

2025-04-15 16:55:58,521 - INFO - TEST CASE 3 Retry 0
2025-04-15 16:55:58,521 - INFO - ---------------
2025-04-15 16:55:58,521 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming data"
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming():
            assert data == "streaming data"

2025-04-15 16:55:58,522 - INFO - ---------------
2025-04-15 16:56:00,617 - INFO - Test Result 1- False
2025-04-15 16:56:00,618 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_openai_streaming _______________________
temp\temp.py:13: in test_openai_llm_openai_streaming
    mock_chunk = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_streaming - NameError: name 'Magi...
1 failed in 1.09s
2025-04-15 16:56:03,255 - INFO - TEST CASE 3 Retry 1
2025-04-15 16:56:03,255 - INFO - ---------------
2025-04-15 16:56:03,255 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

from unittest.mock import MagicMock, patch

@pytest.mark.asyncio
async def test_openai_llm_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming data"
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming():
            assert data == "streaming data"

2025-04-15 16:56:03,255 - INFO - ---------------
2025-04-15 16:56:05,590 - INFO - Test Result 2- False
2025-04-15 16:56:05,591 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_openai_streaming _______________________
temp\temp.py:16: in test_openai_llm_openai_streaming
    with patch.object(OpenAI_llm, "client") as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_streaming - AttributeError: <clas...
1 failed in 1.41s
2025-04-15 16:56:07,978 - INFO - TEST CASE 3 Retry 2
2025-04-15 16:56:07,979 - INFO - ---------------
2025-04-15 16:56:07,980 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

import pytest
from unittest.mock import MagicMock, patch
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices = [MagicMock()]
    mock_chunk.choices[0].delta.content = "streaming data"
    with patch.object(OpenAI_llm, "client", create=True) as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming():
            assert data == "streaming data"

2025-04-15 16:56:07,980 - INFO - ---------------
2025-04-15 16:56:10,271 - INFO - Test Result 3- False
2025-04-15 16:56:10,272 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_openai_streaming _______________________
temp\temp.py:23: in test_openai_llm_openai_streaming
    assert data == "streaming data"
E   AssertionError: assert 'Failed in _O...s to be given' == 'streaming data'
E     
E     - streaming data
E     + Failed in _OpenAI_Streaming: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_streaming - AssertionError: asser...
1 failed in 1.38s
2025-04-15 16:56:10,272 - INFO - Failed after all retries for test case 3
2025-04-15 16:56:10,272 - INFO - 

2025-04-15 16:56:10,272 - INFO - TEST CASE 4 Retry 0
2025-04-15 16:56:10,272 - INFO - ---------------
2025-04-15 16:56:10,273 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion"
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_Chat_Completion()
        assert result == "chat completion"

2025-04-15 16:56:10,273 - INFO - ---------------
2025-04-15 16:56:12,087 - INFO - Test Result 1- False
2025-04-15 16:56:12,088 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_chat_completion ____________________
temp\temp.py:13: in test_openai_llm_openai_chat_completion
    mock_response = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_chat_completion - NameError: name...
1 failed in 1.03s
2025-04-15 16:56:23,565 - INFO - TEST CASE 4 Retry 1
2025-04-15 16:56:23,566 - INFO - ---------------
2025-04-15 16:56:23,566 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

from unittest.mock import MagicMock, patch
import pytest

@pytest.mark.asyncio
async def test_openai_llm_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion"
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_Chat_Completion()
        assert result == "chat completion"

2025-04-15 16:56:23,566 - INFO - ---------------
2025-04-15 16:56:26,406 - INFO - Test Result 2- False
2025-04-15 16:56:26,406 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_chat_completion ____________________
temp\temp.py:17: in test_openai_llm_openai_chat_completion
    with patch.object(OpenAI_llm, "client") as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_chat_completion - AttributeError:...
1 failed in 1.84s
2025-04-15 16:56:28,259 - INFO - TEST CASE 4 Retry 2
2025-04-15 16:56:28,260 - INFO - ---------------
2025-04-15 16:56:28,260 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

from unittest.mock import MagicMock, patch
import pytest
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion"
    with patch.object(OpenAI_llm, "client", create=True) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_Chat_Completion()
        assert result == "chat completion"

2025-04-15 16:56:28,260 - INFO - ---------------
2025-04-15 16:56:30,285 - INFO - Test Result 3- False
2025-04-15 16:56:30,286 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:12: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.12s
2025-04-15 16:56:30,286 - INFO - Failed after all retries for test case 4
2025-04-15 16:56:30,286 - INFO - 

2025-04-15 16:56:30,286 - INFO - TEST CASE 5 Retry 0
2025-04-15 16:56:30,286 - INFO - ---------------
2025-04-15 16:56:30,286 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

2025-04-15 16:56:30,286 - INFO - ---------------
2025-04-15 16:56:32,172 - INFO - Test Result 1- False
2025-04-15 16:56:32,172 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:13: in test_openai_llm_execute_text_generation
    mock_response = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - NameError: nam...
1 failed in 1.04s
2025-04-15 16:56:34,140 - INFO - TEST CASE 5 Retry 1
2025-04-15 16:56:34,141 - INFO - ---------------
2025-04-15 16:56:34,141 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

from unittest.mock import MagicMock, patch

@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

2025-04-15 16:56:34,141 - INFO - ---------------
2025-04-15 16:56:35,995 - INFO - Test Result 2- False
2025-04-15 16:56:35,995 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:16: in test_openai_llm_execute_text_generation
    with patch.object(OpenAI_llm, "client") as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - AttributeError...
1 failed in 0.99s
2025-04-15 16:56:38,234 - INFO - TEST CASE 5 Retry 2
2025-04-15 16:56:38,234 - INFO - ---------------
2025-04-15 16:56:38,235 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

import pytest
from unittest.mock import MagicMock, patch
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    with patch.object(OpenAI_llm, "client", create=True) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

2025-04-15 16:56:38,235 - INFO - ---------------
2025-04-15 16:56:41,539 - INFO - Test Result 3- False
2025-04-15 16:56:41,540 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:22: in test_openai_llm_execute_text_generation
    assert response == "text generation response"
E   AssertionError: assert None == 'text generation response'
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_Chat_Completion: Error code: 400 - {'error': {'message': 'you must provide a model parameter', 'type': 'invalid_request_error', 'param': None, 'code': None}}
---------------------------- Captured log teardown ----------------------------
ERROR    asyncio:base_events.py:1871 Task was destroyed but it is pending!
task: <Task pending name='Task-4' coro=<<async_generator_athrow without __name__>()>>
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_execute_text_generation
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\asyncio\base_events.py:750: RuntimeWarning: coroutine method 'aclose' of 'OpenAI_llm._run' was never awaited
    self._ready.clear()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - AssertionError...
1 failed, 1 warning in 2.36s
2025-04-15 16:56:41,540 - INFO - Failed after all retries for test case 5
2025-04-15 16:56:41,540 - INFO - 

2025-04-15 16:56:41,540 - INFO - TEST CASE 6 Retry 0
2025-04-15 16:56:41,540 - INFO - ---------------
2025-04-15 16:56:41,540 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image")
        async for response in llm.execute():
            assert response == "vision response"

2025-04-15 16:56:41,540 - INFO - ---------------
2025-04-15 16:56:43,772 - INFO - Test Result 1- False
2025-04-15 16:56:43,772 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:13: in test_openai_llm_execute_vision
    mock_response = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - NameError: name 'MagicM...
1 failed in 1.38s
2025-04-15 16:56:46,672 - INFO - TEST CASE 6 Retry 1
2025-04-15 16:56:46,672 - INFO - ---------------
2025-04-15 16:56:46,673 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

from unittest.mock import MagicMock, patch

@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    with patch.object(OpenAI_llm, "client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image")
        async for response in llm.execute():
            assert response == "vision response"

2025-04-15 16:56:46,673 - INFO - ---------------
2025-04-15 16:56:48,986 - INFO - Test Result 2- False
2025-04-15 16:56:48,987 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:16: in test_openai_llm_execute_vision
    with patch.object(OpenAI_llm, "client") as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - AttributeError: <class ...
1 failed in 1.33s
2025-04-15 16:56:51,217 - INFO - TEST CASE 6 Retry 2
2025-04-15 16:56:51,217 - INFO - ---------------
2025-04-15 16:56:51,217 - INFO - 

from unittest.mock import patch
import asyncio
import json
import os
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

from unittest.mock import MagicMock, patch
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    with patch.object(OpenAI_llm, "client", create=True) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image")
        async for response in llm.execute():
            assert response == "vision response"

2025-04-15 16:56:51,217 - INFO - ---------------
2025-04-15 16:56:54,512 - INFO - Test Result 3- False
2025-04-15 16:56:54,512 - INFO - Test Error 3 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:21: in test_openai_llm_execute_vision
    assert response == "vision response"
E   AssertionError: assert None == 'vision response'
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_Chat_Completion: Error code: 400 - {'error': {'message': 'you must provide a model parameter', 'type': 'invalid_request_error', 'param': None, 'code': None}}
---------------------------- Captured log teardown ----------------------------
ERROR    asyncio:base_events.py:1871 Task was destroyed but it is pending!
task: <Task pending name='Task-4' coro=<<async_generator_athrow without __name__>()>>
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_execute_vision
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\asyncio\base_events.py:750: RuntimeWarning: coroutine method 'aclose' of 'OpenAI_llm._run' was never awaited
    self._ready.clear()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - AssertionError: assert ...
1 failed, 1 warning in 2.35s
2025-04-15 16:56:54,512 - INFO - Failed after all retries for test case 6
2025-04-15 16:56:54,513 - INFO - End Processing file: theory_evaluation\llm_handler.py

2025-04-15 16:56:54,513 - INFO - Start Processing file: theory_evaluation\llm_utils.py
2025-04-15 16:57:00,242 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 16:57:06,116 - INFO - End Processing file: theory_evaluation\llm_utils.py

2025-04-15 16:57:06,117 - INFO - Start Processing file: theory_evaluation\__init__.py
2025-04-15 16:57:06,118 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

