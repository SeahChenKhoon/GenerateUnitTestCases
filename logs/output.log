2025-04-15 16:27:32,581 - INFO - Loading environment variables...
2025-04-15 16:27:32,909 - INFO - Start Processing file: theory_evaluation\llm_handler.py
2025-04-15 16:27:49,925 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 16:28:09,352 - INFO - Verify No pytest in test_code - import pytest
import asyncio
import json
from unittest.mock import patch, AsyncMock, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
        assert llm.azure_endpoint == "mock_AZURE_OPENAI_ENDPOINT_SWEDEN"
        assert llm.api_version == "mock_AZURE_OPENAI_API_VERSION"
        assert llm.model_name == "mock_AZURE_OPENAI_DEPLOYMENT_NAME"

        llm = OpenAI_llm(useAzureOpenAI=False)
        assert llm.client == mock_openai.return_value
        assert llm.model_name == "mock_OPENAI_DEPLOYMENT_NAME"

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"key": "value"})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", new_callable=AsyncMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm()
        result = await llm._OpenAI_JSON_Completion()
        
        assert result == {"key": "value"}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streamed content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", new_callable=AsyncMock) as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(output="stream")
        result = [chunk async for chunk in llm._OpenAI_Streaming()]
        
        assert result == ["streamed content"]

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", new_callable=AsyncMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm()
        result = await llm._OpenAI_Chat_Completion()
        
        assert result == "chat content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "response content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", new_callable=AsyncMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(mode="text_generation")
        result = [response async for response in llm.execute()]
        
        assert result == ["response content"]

@pytest.mark.asyncio
async def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "response content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", new_callable=AsyncMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(mode="vision", image_input="mock_image_input")
        result = [response async for response in llm.execute()]
        
        assert result == ["response content"]
2025-04-15 16:28:09,355 - INFO - 

2025-04-15 16:28:09,356 - INFO - TEST CASE 1 Retry 0
2025-04-15 16:28:09,356 - INFO - ---------------
2025-04-15 16:28:09,356 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm


@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
        assert llm.azure_endpoint == "mock_AZURE_OPENAI_ENDPOINT_SWEDEN"
        assert llm.api_version == "mock_AZURE_OPENAI_API_VERSION"
        assert llm.model_name == "mock_AZURE_OPENAI_DEPLOYMENT_NAME"

2025-04-15 16:28:09,357 - INFO - ---------------
2025-04-15 16:28:11,580 - INFO - Test Result 1- False
2025-04-15 16:28:11,580 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:9: in <module>
    @pytest.mark.asyncio
E   NameError: name 'pytest' is not defined
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'pytest' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.25s
2025-04-15 16:28:14,582 - INFO - TEST CASE 1 Retry 1
2025-04-15 16:28:14,582 - INFO - ---------------
2025-04-15 16:28:14,582 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm

import pytest
from unittest.mock import patch

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
        assert llm.azure_endpoint == "mock_AZURE_OPENAI_ENDPOINT_SWEDEN"
        assert llm.api_version == "mock_AZURE_OPENAI_API_VERSION"
        assert llm.model_name == "mock_AZURE_OPENAI_DEPLOYMENT_NAME"

2025-04-15 16:28:14,583 - INFO - ---------------
2025-04-15 16:28:17,187 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 16:28:17,187 - INFO - New import Statements 2- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch
2025-04-15 16:28:17,187 - INFO - Test Result 2- True
2025-04-15 16:28:17,188 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
.                                                                        [100%]
1 passed in 0.98s
2025-04-15 16:28:17,188 - INFO - 

2025-04-15 16:28:17,188 - INFO - TEST CASE 2 Retry 0
2025-04-15 16:28:17,188 - INFO - ---------------
2025-04-15 16:28:17,189 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch


@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"key": "value"})

2025-04-15 16:28:17,189 - INFO - ---------------
2025-04-15 16:28:19,198 - INFO - Test Result 1- False
2025-04-15 16:28:19,198 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_json_completion _________________________
temp\temp.py:13: in test_openai_json_completion
    mock_response = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_json_completion - NameError: name 'MagicMock...
1 failed in 1.12s
2025-04-15 16:28:20,258 - INFO - TEST CASE 2 Retry 1
2025-04-15 16:28:20,259 - INFO - ---------------
2025-04-15 16:28:20,259 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch

from unittest.mock import MagicMock

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"key": "value"})

2025-04-15 16:28:20,259 - INFO - ---------------
2025-04-15 16:28:22,908 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 16:28:22,909 - INFO - New import Statements 2- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock
2025-04-15 16:28:22,909 - INFO - Test Result 2- True
2025-04-15 16:28:22,909 - INFO - Test Error 2 - 
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
.                                                                        [100%]
1 passed in 0.85s
2025-04-15 16:28:22,910 - INFO - 

2025-04-15 16:28:22,910 - INFO - TEST CASE 3 Retry 0
2025-04-15 16:28:22,910 - INFO - ---------------
2025-04-15 16:28:22,910 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock


@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streamed content"

2025-04-15 16:28:22,910 - INFO - ---------------
2025-04-15 16:28:24,970 - INFO - Test Result 1- True
2025-04-15 16:28:24,970 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
.                                                                        [100%]
1 passed in 1.02s
2025-04-15 16:28:24,970 - INFO - 

2025-04-15 16:28:24,970 - INFO - TEST CASE 4 Retry 0
2025-04-15 16:28:24,970 - INFO - ---------------
2025-04-15 16:28:24,970 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock


@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat content"

2025-04-15 16:28:24,970 - INFO - ---------------
2025-04-15 16:28:27,197 - INFO - Test Result 1- True
2025-04-15 16:28:27,197 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
.                                                                        [100%]
1 passed in 1.15s
2025-04-15 16:28:27,197 - INFO - 

2025-04-15 16:28:27,197 - INFO - TEST CASE 5 Retry 0
2025-04-15 16:28:27,197 - INFO - ---------------
2025-04-15 16:28:27,197 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock


@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "response content"

2025-04-15 16:28:27,197 - INFO - ---------------
2025-04-15 16:28:29,988 - INFO - Test Result 1- True
2025-04-15 16:28:29,989 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
.                                                                        [100%]
1 passed in 1.64s
2025-04-15 16:28:29,989 - INFO - 

2025-04-15 16:28:29,989 - INFO - TEST CASE 6 Retry 0
2025-04-15 16:28:29,989 - INFO - ---------------
2025-04-15 16:28:29,989 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock


@pytest.mark.asyncio
async def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "response content"

2025-04-15 16:28:29,989 - INFO - ---------------
2025-04-15 16:28:31,981 - INFO - Test Result 1- True
2025-04-15 16:28:31,982 - INFO - Test Error 1 - C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
.                                                                        [100%]
1 passed in 1.05s
2025-04-15 16:28:31,986 - INFO - End Processing file: theory_evaluation\llm_handler.py

2025-04-15 16:28:31,986 - INFO - Start Processing file: theory_evaluation\llm_utils.py
2025-04-15 16:28:38,917 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 16:28:43,912 - INFO - Verify No pytest in test_code - import os
import re
import yaml
import pytest
from unittest.mock import patch, mock_open
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings


@pytest.fixture
def mock_open_files():
    mock_files = {
        "./theory_evaluation/evaluator/prompts/agent/config.yaml": mock_open(read_data="key: value").return_value,
        "./theory_evaluation/evaluator/prompts/agent/prompt.txt": mock_open(read_data="This is a {$key} test.").return_value,
        "./theory_evaluation/evaluator/prompts/agent/llm_settings.yaml": mock_open(read_data="setting_key: setting_value").return_value,
    }
    return mock_files


def test_initialise_prompt_success(mock_open_files):
    with patch("builtins.open", side_effect=lambda file, mode: mock_open_files[file]):
        result = initialise_prompt("agent")
        assert result == "This is a value test."


def test_initialise_prompt_missing_placeholder(mock_open_files):
    mock_open_files["./theory_evaluation/evaluator/prompts/agent/prompt.txt"] = mock_open(read_data="This is a {$missing_key} test.").return_value
    with patch("builtins.open", side_effect=lambda file, mode: mock_open_files[file]):
        result = initialise_prompt("agent")
        assert result == "This is a {$missing_key} test."


def test_initialise_prompt_file_not_found():
    with patch("builtins.open", side_effect=FileNotFoundError):
        result = initialise_prompt("agent")
        assert result is None


def test_initialise_settings_success(mock_open_files):
    with patch("builtins.open", side_effect=lambda file, mode: mock_open_files[file]):
        result = initialise_settings("agent")
        assert result == {"setting_key": "setting_value"}


def test_initialise_settings_file_not_found():
    with patch("builtins.open", side_effect=FileNotFoundError):
        result = initialise_settings("agent")
        assert result is None
2025-04-15 16:28:43,916 - INFO - End Processing file: theory_evaluation\llm_utils.py

2025-04-15 16:28:43,917 - INFO - Start Processing file: theory_evaluation\__init__.py
2025-04-15 16:28:43,918 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

