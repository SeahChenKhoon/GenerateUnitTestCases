2025-04-15 10:58:14,291 - INFO - Loading environment variables...
2025-04-15 10:58:14,694 - INFO - [1mStart Processing file: theory_evaluation\llm_handler.py[0m
2025-04-15 10:58:26,095 - INFO - Hello World 1
2025-04-15 10:58:34,357 - INFO - Hello World 1
2025-04-15 10:58:34,358 - INFO - Hello World 1 - formatted_prompt - Extract all unit test cases excluding pytest fixtures from the following:
```
import asyncio
import json
import os
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        
        assert llm.message == "Test message"
        assert llm.output == "json"
        assert llm.mode == "text_generation"
        assert llm.verbose is True
        assert hasattr(llm, "client")
        assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
```
. Output only unit test cases. No Markdown formatting, explanations, or docstrings. Do NOT wrap your output in backticks
2025-04-15 10:58:42,998 - INFO - 

2025-04-15 10:58:42,999 - INFO - TEST CASE 1 Retry 0
2025-04-15 10:58:42,999 - INFO - ---------------
2025-04-15 10:58:42,999 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
i

2025-04-15 10:58:43,000 - INFO - ---------------
2025-04-15 10:58:44,826 - INFO - Test Result 1- False
2025-04-15 10:58:44,826 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    i
E   NameError: name 'i' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'i' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.09s
2025-04-15 10:58:46,028 - INFO - TEST CASE 1 Retry 1
2025-04-15 10:58:46,028 - INFO - ---------------
2025-04-15 10:58:46,028 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute():
    message = "Test message"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output=None, verbose=False)
    async for response in llm.execute():
        assert response is not None

2025-04-15 10:58:46,028 - INFO - ---------------
2025-04-15 10:58:48,528 - INFO - Test Result 2- False
2025-04-15 10:58:48,528 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.66s
2025-04-15 10:58:49,728 - INFO - TEST CASE 1 Retry 2
2025-04-15 10:58:49,729 - INFO - ---------------
2025-04-15 10:58:49,729 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 10:58:49,729 - INFO - ---------------
2025-04-15 10:58:52,065 - INFO - Test Result 3- False
2025-04-15 10:58:52,065 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.35s
2025-04-15 10:58:52,066 - INFO - Failed after all retries for test case 1
2025-04-15 10:58:52,066 - INFO - 

2025-04-15 10:58:52,066 - INFO - TEST CASE 2 Retry 0
2025-04-15 10:58:52,066 - INFO - ---------------
2025-04-15 10:58:52,066 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
m

2025-04-15 10:58:52,067 - INFO - ---------------
2025-04-15 10:58:54,565 - INFO - Test Result 1- False
2025-04-15 10:58:54,566 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    m
E   NameError: name 'm' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'm' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.53s
2025-04-15 10:58:55,766 - INFO - TEST CASE 2 Retry 1
2025-04-15 10:58:55,766 - INFO - ---------------
2025-04-15 10:58:55,766 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "Test message"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output=None, verbose=False)
    async for response in llm.execute():
        assert response is not None

2025-04-15 10:58:55,767 - INFO - ---------------
2025-04-15 10:58:57,529 - INFO - Test Result 2- False
2025-04-15 10:58:57,530 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:98: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.14s
2025-04-15 10:58:59,690 - INFO - TEST CASE 2 Retry 2
2025-04-15 10:58:59,690 - INFO - ---------------
2025-04-15 10:58:59,690 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert "Paris" in token

2025-04-15 10:58:59,691 - INFO - ---------------
2025-04-15 10:59:01,718 - INFO - Test Result 3- False
2025-04-15 10:59:01,719 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:101: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.18s
2025-04-15 10:59:01,719 - INFO - Failed after all retries for test case 2
2025-04-15 10:59:01,719 - INFO - 

2025-04-15 10:59:01,719 - INFO - TEST CASE 3 Retry 0
2025-04-15 10:59:01,719 - INFO - ---------------
2025-04-15 10:59:01,719 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
p

2025-04-15 10:59:01,719 - INFO - ---------------
2025-04-15 10:59:03,506 - INFO - Test Result 1- False
2025-04-15 10:59:03,506 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    p
E   NameError: name 'p' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'p' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.17s
2025-04-15 10:59:04,664 - INFO - TEST CASE 3 Retry 1
2025-04-15 10:59:04,665 - INFO - ---------------
2025-04-15 10:59:04,665 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm():
    message = "Test message"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="json", verbose=False)
    async for response in llm.execute():
        assert response is not None

2025-04-15 10:59:04,665 - INFO - ---------------
2025-04-15 10:59:06,752 - INFO - Test Result 2- False
2025-04-15 10:59:06,752 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.23s
2025-04-15 10:59:09,138 - INFO - TEST CASE 3 Retry 2
2025-04-15 10:59:09,138 - INFO - ---------------
2025-04-15 10:59:09,139 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    
    async for response in llm.execute():
        assert "Paris" in response

2025-04-15 10:59:09,140 - INFO - ---------------
2025-04-15 10:59:11,893 - INFO - Test Result 3- False
2025-04-15 10:59:11,893 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:102: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.66s
2025-04-15 10:59:11,893 - INFO - Failed after all retries for test case 3
2025-04-15 10:59:11,893 - INFO - 

2025-04-15 10:59:11,893 - INFO - TEST CASE 4 Retry 0
2025-04-15 10:59:11,894 - INFO - ---------------
2025-04-15 10:59:11,894 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
o

2025-04-15 10:59:11,894 - INFO - ---------------
2025-04-15 10:59:14,479 - INFO - Test Result 1- False
2025-04-15 10:59:14,479 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    o
E   NameError: name 'o' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'o' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.79s
2025-04-15 10:59:16,556 - INFO - TEST CASE 4 Retry 1
2025-04-15 10:59:16,557 - INFO - ---------------
2025-04-15 10:59:16,557 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute():
    message = "Test message"
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=True)
    async for token in llm.execute():
        assert token is not None

2025-04-15 10:59:16,557 - INFO - ---------------
2025-04-15 10:59:18,407 - INFO - Test Result 2- False
2025-04-15 10:59:18,407 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.21s
2025-04-15 10:59:19,967 - INFO - TEST CASE 4 Retry 2
2025-04-15 10:59:19,967 - INFO - ---------------
2025-04-15 10:59:19,967 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output=None, verbose=False)
    async for response in llm.execute():
        assert "Paris" in response

2025-04-15 10:59:19,968 - INFO - ---------------
2025-04-15 10:59:21,843 - INFO - Test Result 3- False
2025-04-15 10:59:21,843 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:102: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.18s
2025-04-15 10:59:21,843 - INFO - Failed after all retries for test case 4
2025-04-15 10:59:21,843 - INFO - 

2025-04-15 10:59:21,843 - INFO - TEST CASE 5 Retry 0
2025-04-15 10:59:21,843 - INFO - ---------------
2025-04-15 10:59:21,843 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
r

2025-04-15 10:59:21,843 - INFO - ---------------
2025-04-15 10:59:23,748 - INFO - Test Result 1- False
2025-04-15 10:59:23,749 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    r
E   NameError: name 'r' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'r' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.15s
2025-04-15 10:59:25,438 - INFO - TEST CASE 5 Retry 1
2025-04-15 10:59:25,438 - INFO - ---------------
2025-04-15 10:59:25,438 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute():
    message = "You are a helpful assistant. Does Azure OpenAI support customer managed keys?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=True)
    
    async for token in llm.execute():
        assert token is not None

2025-04-15 10:59:25,439 - INFO - ---------------
2025-04-15 10:59:27,574 - INFO - Test Result 2- False
2025-04-15 10:59:27,575 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.35s
2025-04-15 10:59:29,234 - INFO - TEST CASE 5 Retry 2
2025-04-15 10:59:29,235 - INFO - ---------------
2025-04-15 10:59:29,235 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. Does Azure OpenAI support customer managed keys?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=True)
    async for token in llm.execute():
        assert token is not None

2025-04-15 10:59:29,236 - INFO - ---------------
2025-04-15 10:59:31,815 - INFO - Test Result 3- False
2025-04-15 10:59:31,816 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.66s
2025-04-15 10:59:31,816 - INFO - Failed after all retries for test case 5
2025-04-15 10:59:31,816 - INFO - 

2025-04-15 10:59:31,816 - INFO - TEST CASE 6 Retry 0
2025-04-15 10:59:31,816 - INFO - ---------------
2025-04-15 10:59:31,816 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
t

2025-04-15 10:59:31,816 - INFO - ---------------
2025-04-15 10:59:34,603 - INFO - Test Result 1- False
2025-04-15 10:59:34,603 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    t
E   NameError: name 't' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 't' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.83s
2025-04-15 10:59:36,627 - INFO - TEST CASE 6 Retry 1
2025-04-15 10:59:36,628 - INFO - ---------------
2025-04-15 10:59:36,628 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute():
    message = "You are a helpful assistant."
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=True)
    async for token in llm.execute():
        assert token is not None

2025-04-15 10:59:36,628 - INFO - ---------------
2025-04-15 10:59:38,641 - INFO - Test Result 2- False
2025-04-15 10:59:38,641 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.14s
2025-04-15 10:59:40,325 - INFO - TEST CASE 6 Retry 2
2025-04-15 10:59:40,325 - INFO - ---------------
2025-04-15 10:59:40,325 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import sys
sys.path.insert(0, 'C:/ChenKhoon/JupyterNotebook/GenerateUnitTestCases/temp')
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output=None, verbose=False)
    async for response in llm.execute():
        assert "Paris" in response

2025-04-15 10:59:40,326 - INFO - ---------------
2025-04-15 10:59:42,177 - INFO - Test Result 3- False
2025-04-15 10:59:42,177 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:100: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.05s
2025-04-15 10:59:42,177 - INFO - Failed after all retries for test case 6
2025-04-15 10:59:42,177 - INFO - 

2025-04-15 10:59:42,177 - INFO - TEST CASE 7 Retry 0
2025-04-15 10:59:42,177 - INFO - ---------------
2025-04-15 10:59:42,177 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
 

2025-04-15 10:59:42,177 - INFO - ---------------
2025-04-15 10:59:43,975 - INFO - Test Result 1- True
2025-04-15 10:59:43,975 - INFO - Test Error 1 - ssssss                                                                   [100%]
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_llm_initialization
temp/temp.py::test_openai_json_completion
temp/temp.py::test_openai_streaming
temp/temp.py::test_openai_chat_completion
temp/temp.py::test_execute_text_generation
temp/temp.py::test_execute_vision_mode
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
6 skipped, 12 warnings in 1.01s
2025-04-15 10:59:43,976 - INFO - 

2025-04-15 10:59:43,976 - INFO - TEST CASE 8 Retry 0
2025-04-15 10:59:43,976 - INFO - ---------------
2025-04-15 10:59:43,976 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
a

2025-04-15 10:59:43,977 - INFO - ---------------
2025-04-15 10:59:46,423 - INFO - Test Result 1- False
2025-04-15 10:59:46,424 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    a
E   NameError: name 'a' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'a' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.27s
2025-04-15 10:59:48,691 - INFO - TEST CASE 8 Retry 1
2025-04-15 10:59:48,692 - INFO - ---------------
2025-04-15 10:59:48,692 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute():
    message = "Test message"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 10:59:48,692 - INFO - ---------------
2025-04-15 10:59:50,608 - INFO - Test Result 2- False
2025-04-15 10:59:50,608 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:98: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.18s
2025-04-15 10:59:51,875 - INFO - TEST CASE 8 Retry 2
2025-04-15 10:59:51,875 - INFO - ---------------
2025-04-15 10:59:51,875 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 10:59:51,875 - INFO - ---------------
2025-04-15 10:59:53,922 - INFO - Test Result 3- False
2025-04-15 10:59:53,922 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:98: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.33s
2025-04-15 10:59:53,922 - INFO - Failed after all retries for test case 8
2025-04-15 10:59:53,923 - INFO - 

2025-04-15 10:59:53,923 - INFO - TEST CASE 9 Retry 0
2025-04-15 10:59:53,923 - INFO - ---------------
2025-04-15 10:59:53,923 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
s

2025-04-15 10:59:53,923 - INFO - ---------------
2025-04-15 10:59:55,881 - INFO - Test Result 1- False
2025-04-15 10:59:55,881 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    s
E   NameError: name 's' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 's' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.19s
2025-04-15 10:59:57,019 - INFO - TEST CASE 9 Retry 1
2025-04-15 10:59:57,020 - INFO - ---------------
2025-04-15 10:59:57,020 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute():
    message = "Test message"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 10:59:57,020 - INFO - ---------------
2025-04-15 10:59:58,854 - INFO - Test Result 2- False
2025-04-15 10:59:58,854 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.24s
2025-04-15 11:00:00,103 - INFO - TEST CASE 9 Retry 2
2025-04-15 11:00:00,104 - INFO - ---------------
2025-04-15 11:00:00,104 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=True)
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:00:00,104 - INFO - ---------------
2025-04-15 11:00:03,199 - INFO - Test Result 3- False
2025-04-15 11:00:03,199 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.97s
2025-04-15 11:00:03,199 - INFO - Failed after all retries for test case 9
2025-04-15 11:00:03,199 - INFO - 

2025-04-15 11:00:03,199 - INFO - TEST CASE 10 Retry 0
2025-04-15 11:00:03,199 - INFO - ---------------
2025-04-15 11:00:03,199 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
y

2025-04-15 11:00:03,199 - INFO - ---------------
2025-04-15 11:00:05,391 - INFO - Test Result 1- False
2025-04-15 11:00:05,392 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    y
E   NameError: name 'y' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'y' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.22s
2025-04-15 11:00:07,818 - INFO - TEST CASE 10 Retry 1
2025-04-15 11:00:07,819 - INFO - ---------------
2025-04-15 11:00:07,819 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm():
    message = "Test message"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:00:07,819 - INFO - ---------------
2025-04-15 11:00:09,698 - INFO - Test Result 2- False
2025-04-15 11:00:09,698 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.13s
2025-04-15 11:00:11,441 - INFO - TEST CASE 10 Retry 2
2025-04-15 11:00:11,441 - INFO - ---------------
2025-04-15 11:00:11,441 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
from llm_handler import OpenAI_llm

import pytest
import asyncio

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. Does Azure OpenAI support customer managed keys?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=True)
    
    responses = []
    async for token in llm.execute():
        responses.append(token)
    
    assert len(responses) > 0

2025-04-15 11:00:11,442 - INFO - ---------------
2025-04-15 11:00:13,392 - INFO - Test Result 3- False
2025-04-15 11:00:13,392 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:97: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.16s
2025-04-15 11:00:13,392 - INFO - Failed after all retries for test case 10
2025-04-15 11:00:13,392 - INFO - 

2025-04-15 11:00:13,392 - INFO - TEST CASE 11 Retry 0
2025-04-15 11:00:13,393 - INFO - ---------------
2025-04-15 11:00:13,393 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
n

2025-04-15 11:00:13,393 - INFO - ---------------
2025-04-15 11:00:15,179 - INFO - Test Result 1- False
2025-04-15 11:00:15,179 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    n
E   NameError: name 'n' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'n' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.11s
2025-04-15 11:00:16,305 - INFO - TEST CASE 11 Retry 1
2025-04-15 11:00:16,306 - INFO - ---------------
2025-04-15 11:00:16,306 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_execute():
    message = "Test message"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:00:16,306 - INFO - ---------------
2025-04-15 11:00:18,021 - INFO - Test Result 2- False
2025-04-15 11:00:18,021 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.10s
2025-04-15 11:00:19,646 - INFO - TEST CASE 11 Retry 2
2025-04-15 11:00:19,647 - INFO - ---------------
2025-04-15 11:00:19,647 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert "Paris" in token

2025-04-15 11:00:19,647 - INFO - ---------------
2025-04-15 11:00:22,366 - INFO - Test Result 3- False
2025-04-15 11:00:22,367 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:101: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.61s
2025-04-15 11:00:22,367 - INFO - Failed after all retries for test case 11
2025-04-15 11:00:22,367 - INFO - 

2025-04-15 11:00:22,367 - INFO - TEST CASE 12 Retry 0
2025-04-15 11:00:22,367 - INFO - ---------------
2025-04-15 11:00:22,367 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
c

2025-04-15 11:00:22,367 - INFO - ---------------
2025-04-15 11:00:25,511 - INFO - Test Result 1- False
2025-04-15 11:00:25,511 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    c
E   NameError: name 'c' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'c' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 2.05s
2025-04-15 11:00:28,193 - INFO - TEST CASE 12 Retry 1
2025-04-15 11:00:28,194 - INFO - ---------------
2025-04-15 11:00:28,194 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm():
    message = "You are a helpful assistant. Does Azure OpenAI support customer managed keys?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=True)
    
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:00:28,194 - INFO - ---------------
2025-04-15 11:00:30,073 - INFO - Test Result 2- False
2025-04-15 11:00:30,073 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:99: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.12s
2025-04-15 11:00:31,868 - INFO - TEST CASE 12 Retry 2
2025-04-15 11:00:31,869 - INFO - ---------------
2025-04-15 11:00:31,869 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:00:31,869 - INFO - ---------------
2025-04-15 11:00:33,742 - INFO - Test Result 3- False
2025-04-15 11:00:33,742 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:102: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.18s
2025-04-15 11:00:33,743 - INFO - Failed after all retries for test case 12
2025-04-15 11:00:33,743 - INFO - 

2025-04-15 11:00:33,743 - INFO - TEST CASE 13 Retry 0
2025-04-15 11:00:33,743 - INFO - ---------------
2025-04-15 11:00:33,743 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
i

2025-04-15 11:00:33,743 - INFO - ---------------
2025-04-15 11:00:35,805 - INFO - Test Result 1- False
2025-04-15 11:00:35,806 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:97: in <module>
    i
E   NameError: name 'i' is not defined
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'i' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.15s
2025-04-15 11:00:37,079 - INFO - TEST CASE 13 Retry 1
2025-04-15 11:00:37,079 - INFO - ---------------
2025-04-15 11:00:37,080 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. Does Azure OpenAI support customer managed keys?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=True)
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:00:37,081 - INFO - ---------------
2025-04-15 11:00:39,848 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 11:00:39,848 - INFO - New import Statements 2- import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm
2025-04-15 11:00:39,849 - INFO - Test Result 2- True
2025-04-15 11:00:39,849 - INFO - Test Error 2 - 
sssssss                                                                  [100%]
============================== warnings summary ===============================
temp\temp.py:18
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:36
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:49
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:61
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:61: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:74
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:74: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:86
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:86: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:99
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:99: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_llm_initialization
temp/temp.py::test_openai_json_completion
temp/temp.py::test_openai_streaming
temp/temp.py::test_openai_chat_completion
temp/temp.py::test_execute_text_generation
temp/temp.py::test_execute_vision_mode
temp/temp.py::test_openai_llm_execution
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
7 skipped, 14 warnings in 0.99s
2025-04-15 11:00:39,849 - INFO - 

2025-04-15 11:00:39,850 - INFO - TEST CASE 14 Retry 0
2025-04-15 11:00:39,850 - INFO - ---------------
2025-04-15 11:00:39,850 - INFO - 
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
o

2025-04-15 11:00:39,850 - INFO - ---------------
2025-04-15 11:00:43,102 - INFO - Test Result 1- False
2025-04-15 11:00:43,102 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:94: in <module>
    o
E   NameError: name 'o' is not defined
============================== warnings summary ===============================
temp\temp.py:15
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:15: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:33
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:33: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:46
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:46: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:58
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:58: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:71
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:71: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:83
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:83: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'o' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 2.03s
2025-04-15 11:00:44,816 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 11:00:44,816 - INFO - TEST CASE 14 Retry 1
2025-04-15 11:00:44,816 - INFO - ---------------
2025-04-15 11:00:44,816 - INFO - 
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. Does Azure OpenAI support customer managed keys?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=True)

    responses = []
    async for token in llm.execute():
        responses.append(token)

    assert len(responses) > 0

2025-04-15 11:00:44,816 - INFO - ---------------
2025-04-15 11:00:47,023 - INFO - Test Result 2- False
2025-04-15 11:00:47,023 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:96: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:15
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:15: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:33
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:33: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:46
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:46: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:58
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:58: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:71
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:71: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:83
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:83: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.45s
2025-04-15 11:00:48,338 - INFO - TEST CASE 14 Retry 2
2025-04-15 11:00:48,339 - INFO - ---------------
2025-04-15 11:00:48,339 - INFO - 
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is 2+2?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:00:48,339 - INFO - ---------------
2025-04-15 11:00:50,085 - INFO - Test Result 3- False
2025-04-15 11:00:50,085 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:95: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:15
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:15: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:33
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:33: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:46
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:46: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:58
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:58: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:71
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:71: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:83
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:83: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.09s
2025-04-15 11:00:50,086 - INFO - Failed after all retries for test case 14
2025-04-15 11:00:50,086 - INFO - 

2025-04-15 11:00:50,086 - INFO - TEST CASE 15 Retry 0
2025-04-15 11:00:50,086 - INFO - ---------------
2025-04-15 11:00:50,086 - INFO - 
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"



2025-04-15 11:00:50,086 - INFO - ---------------
2025-04-15 11:00:51,652 - INFO - Test Result 1- True
2025-04-15 11:00:51,652 - INFO - Test Error 1 - ssssss                                                                   [100%]
============================== warnings summary ===============================
temp\temp.py:15
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:15: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:33
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:33: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:46
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:46: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:58
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:58: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:71
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:71: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:83
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:83: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_llm_initialization
temp/temp.py::test_openai_json_completion
temp/temp.py::test_openai_streaming
temp/temp.py::test_openai_chat_completion
temp/temp.py::test_execute_text_generation
temp/temp.py::test_execute_vision_mode
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
6 skipped, 12 warnings in 0.97s
2025-04-15 11:00:51,652 - INFO - 

2025-04-15 11:00:51,652 - INFO - TEST CASE 16 Retry 0
2025-04-15 11:00:51,653 - INFO - ---------------
2025-04-15 11:00:51,653 - INFO - 
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
i

2025-04-15 11:00:51,653 - INFO - ---------------
2025-04-15 11:00:53,587 - INFO - Test Result 1- False
2025-04-15 11:00:53,588 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:94: in <module>
    i
E   NameError: name 'i' is not defined
============================== warnings summary ===============================
temp\temp.py:15
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:15: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:33
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:33: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:46
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:46: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:58
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:58: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:71
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:71: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:83
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:83: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'i' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.26s
2025-04-15 11:00:55,841 - INFO - TEST CASE 16 Retry 1
2025-04-15 11:00:55,842 - INFO - ---------------
2025-04-15 11:00:55,842 - INFO - 
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from temp import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute():
    message = "Test message"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for response in llm.execute():
        assert response is not None

2025-04-15 11:00:55,842 - INFO - ---------------
2025-04-15 11:00:58,395 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 11:00:58,395 - INFO - New import Statements 2- import json
2025-04-15 11:00:58,395 - INFO - Test Result 2- True
2025-04-15 11:00:58,396 - INFO - Test Error 2 - 
sssssss                                                                  [100%]
============================== warnings summary ===============================
temp\temp.py:15
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:15: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:33
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:33: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:46
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:46: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:58
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:58: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:71
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:71: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:83
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:83: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:98
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:98: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_llm_initialization
temp/temp.py::test_openai_json_completion
temp/temp.py::test_openai_streaming
temp/temp.py::test_openai_chat_completion
temp/temp.py::test_execute_text_generation
temp/temp.py::test_execute_vision_mode
temp/temp.py::test_openai_llm_execute
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
7 skipped, 14 warnings in 1.11s
2025-04-15 11:00:58,396 - INFO - 

2025-04-15 11:00:58,396 - INFO - TEST CASE 17 Retry 0
2025-04-15 11:00:58,396 - INFO - ---------------
2025-04-15 11:00:58,396 - INFO - 
import json
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
m

2025-04-15 11:00:58,397 - INFO - ---------------
2025-04-15 11:01:00,254 - INFO - Test Result 1- False
2025-04-15 11:01:00,254 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:92: in <module>
    m
E   NameError: name 'm' is not defined
============================== warnings summary ===============================
temp\temp.py:13
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:31
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:31: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:44
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:44: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:56
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:56: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:69
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:69: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:81
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:81: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'm' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.10s
2025-04-15 11:01:01,649 - INFO - TEST CASE 17 Retry 1
2025-04-15 11:01:01,649 - INFO - ---------------
2025-04-15 11:01:01,649 - INFO - 
import json
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm():
    message = "You are a helpful assistant. Does Azure OpenAI support customer managed keys?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:01:01,650 - INFO - ---------------
2025-04-15 11:01:04,443 - INFO - Test Result 2- False
2025-04-15 11:01:04,444 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:94: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:13
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:31
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:31: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:44
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:44: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:56
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:56: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:69
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:69: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:81
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:81: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.72s
2025-04-15 11:01:06,197 - INFO - TEST CASE 17 Retry 2
2025-04-15 11:01:06,198 - INFO - ---------------
2025-04-15 11:01:06,198 - INFO - 
import json
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:01:06,198 - INFO - ---------------
2025-04-15 11:01:08,945 - INFO - Test Result 3- False
2025-04-15 11:01:08,946 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:93: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:13
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:31
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:31: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:44
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:44: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:56
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:56: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:69
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:69: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:81
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:81: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.86s
2025-04-15 11:01:08,947 - INFO - Failed after all retries for test case 17
2025-04-15 11:01:08,947 - INFO - 

2025-04-15 11:01:08,947 - INFO - TEST CASE 18 Retry 0
2025-04-15 11:01:08,947 - INFO - ---------------
2025-04-15 11:01:08,947 - INFO - 
import json
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
p

2025-04-15 11:01:08,947 - INFO - ---------------
2025-04-15 11:01:10,708 - INFO - Test Result 1- False
2025-04-15 11:01:10,708 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:92: in <module>
    p
E   NameError: name 'p' is not defined
============================== warnings summary ===============================
temp\temp.py:13
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:31
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:31: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:44
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:44: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:56
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:56: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:69
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:69: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:81
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:81: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'p' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.08s
2025-04-15 11:01:12,789 - INFO - TEST CASE 18 Retry 1
2025-04-15 11:01:12,789 - INFO - ---------------
2025-04-15 11:01:12,789 - INFO - 
import json
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm():
    message = "You are a helpful assistant. Does Azure OpenAI support customer managed keys?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=True, output="stream", verbose=True)
    async for token in llm.execute():
        assert token is not None

2025-04-15 11:01:12,790 - INFO - ---------------
2025-04-15 11:01:14,700 - INFO - Test Result 2- False
2025-04-15 11:01:14,700 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:94: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:13
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:31
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:31: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:44
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:44: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:56
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:56: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:69
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:69: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:81
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:81: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.12s
2025-04-15 11:01:16,417 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 11:01:16,418 - INFO - TEST CASE 18 Retry 2
2025-04-15 11:01:16,418 - INFO - ---------------
2025-04-15 11:01:16,418 - INFO - 
import json
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
import pytest
import asyncio
from llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execution():
    message = "You are a helpful assistant. What is the capital of France?"
    llm = OpenAI_llm(message=message, useAzureOpenAI=False, output="stream", verbose=False)

    responses = []
    async for token in llm.execute():
        responses.append(token)

    assert len(responses) > 0
    assert "Paris" in "".join(responses)

2025-04-15 11:01:16,418 - INFO - ---------------
2025-04-15 11:01:18,438 - INFO - Test Result 3- False
2025-04-15 11:01:18,438 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:94: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
============================== warnings summary ===============================
temp\temp.py:13
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:31
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:31: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:44
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:44: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:56
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:56: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:69
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:69: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:81
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:81: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.29s
2025-04-15 11:01:18,438 - INFO - Failed after all retries for test case 18
2025-04-15 11:01:18,438 - INFO - 

2025-04-15 11:01:18,438 - INFO - TEST CASE 19 Retry 0
2025-04-15 11:01:18,438 - INFO - ---------------
2025-04-15 11:01:18,438 - INFO - 
import json
import pytest
from unittest.mock import patch, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_llm():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"{key}_value"):
        yield mock_azure_openai, mock_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization(mock_openai_llm):
    mock_azure_openai, mock_openai = mock_openai_llm
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        message="Test message",
        output="json",
        mode="text_generation",
        verbose=True
    )
    
    assert llm.message == "Test message"
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.verbose is True
    assert hasattr(llm, "client")
    assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == {"answer": "42", "explanation": "The answer to everything."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        
        llm = OpenAI_llm(message="Test message", output="stream")
        async for data in llm._OpenAI_Streaming(messages=[{"role": "system", "content": llm.message}]):
            assert data == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat completion content"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message")
        content = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": llm.message}])
        
        assert content == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "text generation response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="text_generation")
        async for response in llm.execute():
            assert response == "text generation response"

@pytest.mark.asyncio
async def test_execute_vision_mode():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client") as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        
        llm = OpenAI_llm(message="Test message", output=None, mode="vision", image_input="image_data")
        async for response in llm.execute():
            assert response == "vision response"
o

2025-04-15 11:01:18,438 - INFO - ---------------
2025-04-15 11:01:20,422 - INFO - Test Result 1- False
2025-04-15 11:01:20,422 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:92: in <module>
    o
E   NameError: name 'o' is not defined
============================== warnings summary ===============================
temp\temp.py:13
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:31
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:31: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:44
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:44: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:56
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:56: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:69
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:69: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp\temp.py:81
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:81: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'o' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
6 warnings, 1 error in 1.25s
