2025-04-15 09:36:43,969 - INFO - Loading environment variables...
2025-04-15 09:36:44,318 - INFO - [1mStart Processing file: theory_evaluation\llm_handler.py[0m
2025-04-15 09:36:54,187 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:36:54,194 - INFO - 

2025-04-15 09:36:54,195 - INFO - TEST CASE 1 Retry 0
2025-04-15 09:36:54,195 - INFO - ---------------
2025-04-15 09:36:54,195 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

def test_openai_llm_initialization_with_openai(mock_openai):
    os.environ['OPENAI_API_KEY'] = 'test_key'
    llm = OpenAI_llm()
    assert llm.client is mock_openai.return_value
    assert llm.client.api_key == 'test_key'

2025-04-15 09:36:54,198 - INFO - ---------------
2025-04-15 09:36:57,673 - INFO - Test Result 1- False
2025-04-15 09:36:57,673 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
________ ERROR at setup of test_openai_llm_initialization_with_openai _________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 7
  def test_openai_llm_initialization_with_openai(mock_openai):
E       fixture 'mock_openai' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:7
=========================== short test summary info ===========================
ERROR temp/temp.py::test_openai_llm_initialization_with_openai
1 error in 2.05s
2025-04-15 09:36:59,173 - INFO - TEST CASE 1 Retry 1
2025-04-15 09:36:59,174 - INFO - ---------------
2025-04-15 09:36:59,174 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import os
import pytest
from unittest.mock import MagicMock, patch
from llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai():
    with patch('llm_handler.OpenAI', autospec=True) as mock:
        yield mock

def test_openai_llm_initialization_with_openai(mock_openai):
    os.environ['OPENAI_API_KEY'] = 'test_key'
    llm = OpenAI_llm()
    assert llm.client is mock_openai.return_value
    assert llm.client.api_key == 'test_key'

2025-04-15 09:36:59,174 - INFO - ---------------
2025-04-15 09:37:01,625 - INFO - Test Result 2- False
2025-04-15 09:37:01,625 - INFO - Test Error 2 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:10: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.34s
2025-04-15 09:37:05,237 - INFO - TEST CASE 1 Retry 2
2025-04-15 09:37:05,237 - INFO - ---------------
2025-04-15 09:37:05,237 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

from temp.llm_handler import OpenAI_llm

def test_openai_llm_initialization_with_openai(mock_openai):
    os.environ['OPENAI_API_KEY'] = 'test_key'
    llm = OpenAI_llm()
    assert llm.client is mock_openai.return_value
    assert llm.client.api_key == 'test_key'

2025-04-15 09:37:05,238 - INFO - ---------------
2025-04-15 09:37:07,348 - INFO - Test Result 3- False
2025-04-15 09:37:07,348 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:7: in <module>
    from temp.llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'temp.llm_handler'; 'temp' is not a package
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.17s
2025-04-15 09:37:07,349 - INFO - Failed after all retries for test case 1
2025-04-15 09:37:07,349 - INFO - 

2025-04-15 09:37:07,349 - INFO - TEST CASE 2 Retry 0
2025-04-15 09:37:07,349 - INFO - ---------------
2025-04-15 09:37:07,349 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

def test_openai_llm_initialization_with_azure_openai(mock_azure_openai):
    os.environ['AZURE_OPENAI_API_KEY_SWEDEN'] = 'test_key'
    os.environ['AZURE_OPENAI_ENDPOINT_SWEDEN'] = 'test_endpoint'
    os.environ['AZURE_OPENAI_API_VERSION'] = 'test_version'
    llm = OpenAI_llm(useAzureOpenAI=True)
    assert llm.client is mock_azure_openai.return_value
    assert llm.client.api_key == 'test_key'
    assert llm.client.azure_endpoint == 'test_endpoint'
    assert llm.client.api_version == 'test_version'

2025-04-15 09:37:07,349 - INFO - ---------------
2025-04-15 09:37:09,246 - INFO - Test Result 1- False
2025-04-15 09:37:09,247 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
_____ ERROR at setup of test_openai_llm_initialization_with_azure_openai ______
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 7
  def test_openai_llm_initialization_with_azure_openai(mock_azure_openai):
E       fixture 'mock_azure_openai' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:7
=========================== short test summary info ===========================
ERROR temp/temp.py::test_openai_llm_initialization_with_azure_openai
1 error in 1.12s
2025-04-15 09:37:11,465 - INFO - TEST CASE 2 Retry 1
2025-04-15 09:37:11,465 - INFO - ---------------
2025-04-15 09:37:11,466 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import MagicMock

@pytest.fixture
def mock_azure_openai():
    mock_client = MagicMock()
    mock_client.api_key = 'test_key'
    mock_client.azure_endpoint = 'test_endpoint'
    mock_client.api_version = 'test_version'
    return mock_client

def test_openai_llm_initialization_with_azure_openai(mock_azure_openai, monkeypatch):
    monkeypatch.setattr('openai.AzureOpenAI', lambda *args, **kwargs: mock_azure_openai)
    os.environ['AZURE_OPENAI_API_KEY_SWEDEN'] = 'test_key'
    os.environ['AZURE_OPENAI_ENDPOINT_SWEDEN'] = 'test_endpoint'
    os.environ['AZURE_OPENAI_API_VERSION'] = 'test_version'
    llm = OpenAI_llm(useAzureOpenAI=True)
    assert llm.client is mock_azure_openai
    assert llm.client.api_key == 'test_key'
    assert llm.client.azure_endpoint == 'test_endpoint'
    assert llm.client.api_version == 'test_version'

2025-04-15 09:37:11,466 - INFO - ---------------
2025-04-15 09:37:13,658 - INFO - Test Result 2- False
2025-04-15 09:37:13,659 - INFO - Test Error 2 - 
F                                                                        [100%]
================================== FAILURES ===================================
______________ test_openai_llm_initialization_with_azure_openai _______________
temp\temp.py:23: in test_openai_llm_initialization_with_azure_openai
    llm = OpenAI_llm(useAzureOpenAI=True)
E   NameError: name 'OpenAI_llm' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization_with_azure_openai - NameE...
1 failed in 1.40s
2025-04-15 09:37:15,436 - INFO - TEST CASE 2 Retry 2
2025-04-15 09:37:15,436 - INFO - ---------------
2025-04-15 09:37:15,436 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

from llm_handler import OpenAI_llm

def test_openai_llm_initialization_with_azure_openai(mock_azure_openai):
    os.environ['AZURE_OPENAI_API_KEY_SWEDEN'] = 'test_key'
    os.environ['AZURE_OPENAI_ENDPOINT_SWEDEN'] = 'test_endpoint'
    os.environ['AZURE_OPENAI_API_VERSION'] = 'test_version'
    llm = OpenAI_llm(useAzureOpenAI=True)
    assert llm.client is mock_azure_openai.return_value
    assert llm.client.api_key == 'test_key'
    assert llm.client.azure_endpoint == 'test_endpoint'
    assert llm.client.api_version == 'test_version'

2025-04-15 09:37:15,436 - INFO - ---------------
2025-04-15 09:37:17,402 - INFO - Test Result 3- False
2025-04-15 09:37:17,403 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:7: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.26s
2025-04-15 09:37:17,403 - INFO - Failed after all retries for test case 2
2025-04-15 09:37:17,403 - INFO - 

2025-04-15 09:37:17,403 - INFO - TEST CASE 3 Retry 0
2025-04-15 09:37:17,403 - INFO - ---------------
2025-04-15 09:37:17,403 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

def test_openai_llm_openai_json_completion(mock_openai):
    mock_response = AsyncMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    mock_openai.return_value.chat.completions.create.return_value = mock_response

2025-04-15 09:37:17,403 - INFO - ---------------
2025-04-15 09:37:19,426 - INFO - Test Result 1- False
2025-04-15 09:37:19,426 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
__________ ERROR at setup of test_openai_llm_openai_json_completion ___________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 7
  def test_openai_llm_openai_json_completion(mock_openai):
E       fixture 'mock_openai' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:7
=========================== short test summary info ===========================
ERROR temp/temp.py::test_openai_llm_openai_json_completion
1 error in 1.16s
2025-04-15 09:37:21,114 - INFO - TEST CASE 3 Retry 1
2025-04-15 09:37:21,115 - INFO - ---------------
2025-04-15 09:37:21,115 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
@patch('llm_handler.OpenAI_llm.client')
async def test_openai_llm_openai_json_completion(mock_openai):
    mock_response = AsyncMock()
    mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
    mock_openai.chat.completions.create.return_value = mock_response

    llm = OpenAI_llm(message="Test message", output="json")
    result = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": llm.message}], model=llm.model_name, **llm.config)
    
    assert result == {"answer": "42", "explanation": "The answer to everything."}

2025-04-15 09:37:21,115 - INFO - ---------------
2025-04-15 09:37:23,547 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:37:23,548 - INFO - New import Statements 2- import pytest
from unittest.mock import AsyncMock, patch
2025-04-15 09:37:23,548 - INFO - Test Result 2- True
2025-04-15 09:37:23,548 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp\temp.py:10
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:10: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_llm_openai_json_completion
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 2 warnings in 1.02s
2025-04-15 09:37:23,548 - INFO - 

2025-04-15 09:37:23,548 - INFO - TEST CASE 4 Retry 0
2025-04-15 09:37:23,549 - INFO - ---------------
2025-04-15 09:37:23,549 - INFO - 
import pytest
from unittest.mock import AsyncMock, patch
def test_openai_llm_openai_streaming(mock_openai):
    mock_chunk = AsyncMock()
    mock_chunk.choices[0].delta.content = "streamed content"
    mock_openai.return_value.chat.completions.create.return_value = [mock_chunk]

2025-04-15 09:37:23,549 - INFO - ---------------
2025-04-15 09:37:24,572 - INFO - Test Result 1- False
2025-04-15 09:37:24,572 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
_____________ ERROR at setup of test_openai_llm_openai_streaming ______________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 3
  def test_openai_llm_openai_streaming(mock_openai):
E       fixture 'mock_openai' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:3
=========================== short test summary info ===========================
ERROR temp/temp.py::test_openai_llm_openai_streaming
1 error in 0.23s
2025-04-15 09:37:26,264 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:37:26,265 - INFO - TEST CASE 4 Retry 1
2025-04-15 09:37:26,265 - INFO - ---------------
2025-04-15 09:37:26,265 - INFO - 
import pytest
from unittest.mock import AsyncMock, patch
from unittest.mock import AsyncMock, patch
import pytest

@pytest.mark.asyncio
@patch('openai.OpenAI')
async def test_openai_llm_openai_streaming(mock_openai):
    mock_chunk = AsyncMock()
    mock_chunk.choices[0].delta.content = "streamed content"
    mock_openai.return_value.chat.completions.create.return_value = [mock_chunk]

    llm = OpenAI_llm(message="Test message", output="stream")
    responses = []
    async for response in llm.execute():
        responses.append(response)

    assert responses == ["streamed content"]

2025-04-15 09:37:26,265 - INFO - ---------------
2025-04-15 09:37:28,104 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:37:28,105 - INFO - New import Statements 2- from unittest.mock import AsyncMock, patch
import pytest
2025-04-15 09:37:28,105 - INFO - Test Result 2- True
2025-04-15 09:37:28,105 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp\temp.py:6
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:6: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_llm_openai_streaming
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 2 warnings in 0.29s
2025-04-15 09:37:28,105 - INFO - 

2025-04-15 09:37:28,106 - INFO - TEST CASE 5 Retry 0
2025-04-15 09:37:28,106 - INFO - ---------------
2025-04-15 09:37:28,106 - INFO - 
from unittest.mock import AsyncMock, patch
import pytest
def test_openai_llm_openai_chat_completion(mock_openai):
    mock_response = AsyncMock()
    mock_response.choices[0].message.content = "chat content"
    mock_openai.return_value.chat.completions.create.return_value = mock_response

2025-04-15 09:37:28,106 - INFO - ---------------
2025-04-15 09:37:29,322 - INFO - Test Result 1- False
2025-04-15 09:37:29,322 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
__________ ERROR at setup of test_openai_llm_openai_chat_completion ___________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 3
  def test_openai_llm_openai_chat_completion(mock_openai):
E       fixture 'mock_openai' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:3
=========================== short test summary info ===========================
ERROR temp/temp.py::test_openai_llm_openai_chat_completion
1 error in 0.26s
2025-04-15 09:37:31,182 - INFO - TEST CASE 5 Retry 1
2025-04-15 09:37:31,182 - INFO - ---------------
2025-04-15 09:37:31,182 - INFO - 
from unittest.mock import AsyncMock, patch
import pytest
import pytest
from unittest.mock import AsyncMock, patch

@pytest.fixture
def mock_openai():
    with patch('openai.OpenAI') as mock:
        yield mock

@pytest.mark.asyncio
async def test_openai_llm_openai_chat_completion(mock_openai):
    mock_response = AsyncMock()
    mock_response.choices[0].message.content = "chat content"
    mock_openai.return_value.chat.completions.create.return_value = mock_response

    llm = OpenAI_llm(message="Test message")
    response = await llm._OpenAI_Chat_Completion()
    assert response == "chat content"

2025-04-15 09:37:31,183 - INFO - ---------------
2025-04-15 09:37:33,735 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:37:33,736 - INFO - New import Statements 2- from unittest.mock import AsyncMock, patch
import pytest
2025-04-15 09:37:33,736 - INFO - Test Result 2- True
2025-04-15 09:37:33,736 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp\temp.py:11
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:11: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_llm_openai_chat_completion
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 2 warnings in 1.13s
2025-04-15 09:37:33,737 - INFO - 

2025-04-15 09:37:33,737 - INFO - TEST CASE 6 Retry 0
2025-04-15 09:37:33,737 - INFO - ---------------
2025-04-15 09:37:33,737 - INFO - 
from unittest.mock import AsyncMock, patch
import pytest
def test_openai_llm_execute_text_generation(mock_openai):
    mock_response = AsyncMock()
    mock_response.choices[0].message.content = "generated text"
    mock_openai.return_value.chat.completions.create.return_value = mock_response

2025-04-15 09:37:33,737 - INFO - ---------------
2025-04-15 09:37:34,555 - INFO - Test Result 1- False
2025-04-15 09:37:34,555 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
__________ ERROR at setup of test_openai_llm_execute_text_generation __________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 3
  def test_openai_llm_execute_text_generation(mock_openai):
E       fixture 'mock_openai' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:3
=========================== short test summary info ===========================
ERROR temp/temp.py::test_openai_llm_execute_text_generation
1 error in 0.17s
2025-04-15 09:37:36,099 - INFO - TEST CASE 6 Retry 1
2025-04-15 09:37:36,100 - INFO - ---------------
2025-04-15 09:37:36,100 - INFO - 
from unittest.mock import AsyncMock, patch
import pytest
from unittest.mock import AsyncMock, patch

@patch('openai.OpenAI')
async def test_openai_llm_execute_text_generation(mock_openai):
    mock_response = AsyncMock()
    mock_response.choices[0].message.content = "generated text"
    mock_openai.return_value.chat.completions.create.return_value = mock_response

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False)
    async for response in llm.execute():
        assert response == "generated text"

2025-04-15 09:37:36,100 - INFO - ---------------
2025-04-15 09:37:37,379 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:37:37,379 - INFO - New import Statements 2- 
2025-04-15 09:37:37,380 - INFO - Test Result 2- True
2025-04-15 09:37:37,380 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_execute_text_generation
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 1 warning in 0.17s
2025-04-15 09:37:37,380 - INFO - 

2025-04-15 09:37:37,380 - INFO - TEST CASE 7 Retry 0
2025-04-15 09:37:37,380 - INFO - ---------------
2025-04-15 09:37:37,380 - INFO - 

def test_openai_llm_execute_vision(mock_openai):
    mock_response = AsyncMock()
    mock_response.choices[0].message.content = "vision text"
    mock_openai.return_value.chat.completions.create.return_value = mock_response

2025-04-15 09:37:37,381 - INFO - ---------------
2025-04-15 09:37:38,109 - INFO - Test Result 1- False
2025-04-15 09:37:38,109 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
______________ ERROR at setup of test_openai_llm_execute_vision _______________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 2
  def test_openai_llm_execute_vision(mock_openai):
E       fixture 'mock_openai' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:2
=========================== short test summary info ===========================
ERROR temp/temp.py::test_openai_llm_execute_vision
1 error in 0.09s
2025-04-15 09:37:41,566 - INFO - TEST CASE 7 Retry 1
2025-04-15 09:37:41,566 - INFO - ---------------
2025-04-15 09:37:41,566 - INFO - 

import pytest
from unittest.mock import AsyncMock, patch

@pytest.fixture
def mock_openai():
    with patch('openai.OpenAI') as mock:
        yield mock

@pytest.mark.asyncio
async def test_openai_llm_execute_vision(mock_openai):
    mock_response = AsyncMock()
    mock_response.choices[0].message.content = "vision text"
    mock_openai.return_value.chat.completions.create.return_value = mock_response

    llm = OpenAI_llm(
        message="Test message",
        useAzureOpenAI=False,
        mode="vision",
        image_input="base64encodedimage",
        output=None
    )

    responses = []
    async for response in llm.execute():
        responses.append(response)

    assert responses[0] == "vision text"

2025-04-15 09:37:41,567 - INFO - ---------------
2025-04-15 09:37:44,995 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:37:44,995 - INFO - New import Statements 2- from openai import OpenAI_llm
2025-04-15 09:37:44,995 - INFO - Test Result 2- True
2025-04-15 09:37:44,996 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp\temp.py:10
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:10: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_llm_execute_vision
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 2 warnings in 1.44s
2025-04-15 09:37:45,000 - INFO - [1mEnd Processing file: theory_evaluation\llm_handler.py[0m

2025-04-15 09:37:45,001 - INFO - [1mStart Processing file: theory_evaluation\llm_utils.py[0m
2025-04-15 09:37:50,629 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:37:50,634 - INFO - 

2025-04-15 09:37:50,635 - INFO - TEST CASE 1 Retry 0
2025-04-15 09:37:50,635 - INFO - ---------------
2025-04-15 09:37:50,635 - INFO - 
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
def test_initialise_prompt(mock_open_files):
    with patch("theory_evaluation.llm_utils.open", mock_open()) as mocked_open:
        mocked_open.side_effect = lambda file_path, mode='r': mock_open(read_data=mock_open_files[file_path]).return_value
        prompt = initialise_prompt("test_agent")
        assert prompt == "This is a sample test."

2025-04-15 09:37:50,635 - INFO - ---------------
2025-04-15 09:37:51,631 - INFO - Test Result 1- False
2025-04-15 09:37:51,632 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_initialise_prompt ___________________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 5
  def test_initialise_prompt(mock_open_files):
E       fixture 'mock_open_files' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:5
=========================== short test summary info ===========================
ERROR temp/temp.py::test_initialise_prompt
1 error in 0.19s
2025-04-15 09:37:53,470 - INFO - TEST CASE 1 Retry 1
2025-04-15 09:37:53,471 - INFO - ---------------
2025-04-15 09:37:53,471 - INFO - 
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest
from unittest.mock import patch, mock_open

@pytest.fixture
def mock_open_files():
    return {
        "./theory_evaluation/evaluator/prompts/test_agent/config.yaml": "key: value",
        "./theory_evaluation/evaluator/prompts/test_agent/prompt.txt": "This is a sample test."
    }

def test_initialise_prompt(mock_open_files):
    with patch("theory_evaluation.llm_utils.open", mock_open()) as mocked_open:
        mocked_open.side_effect = lambda file_path, mode='r': mock_open(read_data=mock_open_files[file_path]).return_value
        prompt = initialise_prompt("test_agent")
        assert prompt == "This is a sample test."

2025-04-15 09:37:53,471 - INFO - ---------------
2025-04-15 09:37:54,785 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:37:54,785 - INFO - New import Statements 2- import pytest
from unittest.mock import patch, mock_open
2025-04-15 09:37:54,786 - INFO - Test Result 2- True
2025-04-15 09:37:54,786 - INFO - Test Error 2 - 
.                                                                        [100%]
1 passed in 0.23s
2025-04-15 09:37:54,786 - INFO - 

2025-04-15 09:37:54,786 - INFO - TEST CASE 2 Retry 0
2025-04-15 09:37:54,786 - INFO - ---------------
2025-04-15 09:37:54,787 - INFO - 
import pytest
from unittest.mock import patch, mock_open
def test_initialise_prompt_missing_placeholder(mock_open_files):
    mock_open_files["./theory_evaluation/evaluator/prompts/test_agent/config.yaml"] = '{}'
    with patch("theory_evaluation.llm_utils.open", mock_open()) as mocked_open:
        mocked_open.side_effect = lambda file_path, mode='r': mock_open(read_data=mock_open_files[file_path]).return_value
        prompt = initialise_prompt("test_agent")
        assert prompt == "This is a {$placeholder} test."

2025-04-15 09:37:54,787 - INFO - ---------------
2025-04-15 09:37:55,649 - INFO - Test Result 1- False
2025-04-15 09:37:55,649 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
________ ERROR at setup of test_initialise_prompt_missing_placeholder _________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 3
  def test_initialise_prompt_missing_placeholder(mock_open_files):
E       fixture 'mock_open_files' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:3
=========================== short test summary info ===========================
ERROR temp/temp.py::test_initialise_prompt_missing_placeholder
1 error in 0.21s
2025-04-15 09:37:57,336 - INFO - TEST CASE 2 Retry 1
2025-04-15 09:37:57,336 - INFO - ---------------
2025-04-15 09:37:57,337 - INFO - 
import pytest
from unittest.mock import patch, mock_open
from unittest.mock import patch, mock_open

def test_initialise_prompt_missing_placeholder():
    mock_open_files = {
        "./theory_evaluation/evaluator/prompts/test_agent/config.yaml": '{}',
        "./theory_evaluation/evaluator/prompts/test_agent/prompt.txt": "This is a {$placeholder} test."
    }
    with patch("builtins.open", mock_open()) as mocked_open:
        mocked_open.side_effect = lambda file_path, mode='r': mock_open(read_data=mock_open_files[file_path]).return_value
        prompt = initialise_prompt("test_agent")
        assert prompt == "This is a {$placeholder} test."

2025-04-15 09:37:57,337 - INFO - ---------------
2025-04-15 09:37:58,290 - INFO - Test Result 2- False
2025-04-15 09:37:58,290 - INFO - Test Error 2 - 
F                                                                        [100%]
================================== FAILURES ===================================
_________________ test_initialise_prompt_missing_placeholder __________________
temp\temp.py:12: in test_initialise_prompt_missing_placeholder
    prompt = initialise_prompt("test_agent")
E   NameError: name 'initialise_prompt' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_prompt_missing_placeholder - NameError: ...
1 failed in 0.27s
2025-04-15 09:38:00,077 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 09:38:00,077 - INFO - TEST CASE 2 Retry 2
2025-04-15 09:38:00,077 - INFO - ---------------
2025-04-15 09:38:00,078 - INFO - 
import pytest
from unittest.mock import patch, mock_open
from unittest.mock import patch, mock_open

def test_initialise_prompt_missing_placeholder(mock_open_files):
    mock_open_files["./theory_evaluation/evaluator/prompts/test_agent/config.yaml"] = '{}'
    mock_open_files["./theory_evaluation/evaluator/prompts/test_agent/prompt.txt"] = "This is a {$placeholder} test."
    with patch("builtins.open", mock_open()) as mocked_open:
        mocked_open.side_effect = lambda file_path, mode='r': mock_open(read_data=mock_open_files[file_path]).return_value
        prompt = initialise_prompt("test_agent")
        assert prompt == "This is a {$placeholder} test."

2025-04-15 09:38:00,078 - INFO - ---------------
2025-04-15 09:38:00,806 - INFO - Test Result 3- False
2025-04-15 09:38:00,806 - INFO - Test Error 3 - 
E                                                                        [100%]
=================================== ERRORS ====================================
________ ERROR at setup of test_initialise_prompt_missing_placeholder _________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 5
  def test_initialise_prompt_missing_placeholder(mock_open_files):
E       fixture 'mock_open_files' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:5
=========================== short test summary info ===========================
ERROR temp/temp.py::test_initialise_prompt_missing_placeholder
1 error in 0.17s
2025-04-15 09:38:00,806 - INFO - Failed after all retries for test case 2
2025-04-15 09:38:00,806 - INFO - 

2025-04-15 09:38:00,806 - INFO - TEST CASE 3 Retry 0
2025-04-15 09:38:00,806 - INFO - ---------------
2025-04-15 09:38:00,806 - INFO - 
import pytest
from unittest.mock import patch, mock_open
def test_initialise_settings(mock_open_files):
    with patch("theory_evaluation.llm_utils.open", mock_open()) as mocked_open:
        mocked_open.side_effect = lambda file_path, mode='r': mock_open(read_data=mock_open_files[file_path]).return_value
        settings = initialise_settings("test_agent")
        assert settings == {"setting_key": "setting_value"}

2025-04-15 09:38:00,806 - INFO - ---------------
2025-04-15 09:38:01,580 - INFO - Test Result 1- False
2025-04-15 09:38:01,580 - INFO - Test Error 1 - E                                                                        [100%]
=================================== ERRORS ====================================
_________________ ERROR at setup of test_initialise_settings __________________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 3
  def test_initialise_settings(mock_open_files):
E       fixture 'mock_open_files' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:3
=========================== short test summary info ===========================
ERROR temp/temp.py::test_initialise_settings
1 error in 0.17s
2025-04-15 09:38:03,128 - INFO - TEST CASE 3 Retry 1
2025-04-15 09:38:03,129 - INFO - ---------------
2025-04-15 09:38:03,129 - INFO - 
import pytest
from unittest.mock import patch, mock_open
from unittest.mock import patch, mock_open

def test_initialise_settings():
    mock_open_files = {
        "./theory_evaluation/evaluator/prompts/test_agent/llm_settings.yaml": '{"setting_key": "setting_value"}'
    }
    with patch("builtins.open", mock_open()) as mocked_open:
        mocked_open.side_effect = lambda file_path, mode='r': mock_open(read_data=mock_open_files[file_path]).return_value
        settings = initialise_settings("test_agent")
        assert settings == {"setting_key": "setting_value"}

2025-04-15 09:38:03,129 - INFO - ---------------
2025-04-15 09:38:04,117 - INFO - Test Result 2- False
2025-04-15 09:38:04,117 - INFO - Test Error 2 - 
F                                                                        [100%]
================================== FAILURES ===================================
__________________________ test_initialise_settings ___________________________
temp\temp.py:11: in test_initialise_settings
    settings = initialise_settings("test_agent")
E   NameError: name 'initialise_settings' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_settings - NameError: name 'initialise_s...
1 failed in 0.26s
2025-04-15 09:38:05,476 - INFO - TEST CASE 3 Retry 2
2025-04-15 09:38:05,477 - INFO - ---------------
2025-04-15 09:38:05,477 - INFO - 
import pytest
from unittest.mock import patch, mock_open
from unittest.mock import patch, mock_open

def test_initialise_settings(mock_open_files):
    with patch("builtins.open", mock_open()) as mocked_open:
        mocked_open.side_effect = lambda file_path, mode='r': mock_open(read_data=mock_open_files[file_path]).return_value
        settings = initialise_settings("test_agent")
        assert settings == {"setting_key": "setting_value"}

2025-04-15 09:38:05,477 - INFO - ---------------
2025-04-15 09:38:06,528 - INFO - Test Result 3- False
2025-04-15 09:38:06,529 - INFO - Test Error 3 - 
E                                                                        [100%]
=================================== ERRORS ====================================
_________________ ERROR at setup of test_initialise_settings __________________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 5
  def test_initialise_settings(mock_open_files):
E       fixture 'mock_open_files' not found
>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:5
=========================== short test summary info ===========================
ERROR temp/temp.py::test_initialise_settings
1 error in 0.27s
2025-04-15 09:38:06,529 - INFO - Failed after all retries for test case 3
2025-04-15 09:38:06,532 - INFO - [1mEnd Processing file: theory_evaluation\llm_utils.py[0m

2025-04-15 09:38:06,532 - INFO - [1mStart Processing file: theory_evaluation\__init__.py[0m
2025-04-15 09:38:06,533 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

