2025-04-18 11:25:53,999 - INFO - Loading environment variables start
2025-04-18 11:25:54,006 - INFO - Loading environment variables completes
2025-04-18 11:25:54,007 - INFO - Initialising of LLM start
2025-04-18 11:25:54,411 - INFO - Initialising of LLM completes
2025-04-18 11:25:54,411 - INFO - Getting python file starts
2025-04-18 11:25:54,412 - INFO - Getting python file completes
2025-04-18 11:25:54,412 - INFO - 
Start Processing file: theory_evaluation\circle_utils.py
2025-04-18 11:25:54,415 - INFO - Extraction of function and class start
2025-04-18 11:25:54,415 - INFO - extraction of function and class complete
2025-04-18 11:25:54,415 - INFO - Generate Unit Test Case starts
2025-04-18 11:25:54,415 - INFO - Extract unique import start
2025-04-18 11:25:55,823 - INFO - Extract unique import complete
2025-04-18 11:25:55,824 - INFO - Update relative import start
2025-04-18 11:25:55,824 - INFO - Update relative import complete
2025-04-18 11:26:01,399 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-18 11:26:01,399 - INFO - Generate Unit Test Case complete
2025-04-18 11:26:01,404 - INFO - run_each_pytest_function_individually start
2025-04-18 11:26:03,152 - INFO - Number of test case to process - 0
2025-04-18 11:26:03,153 - INFO - Before Improvement

import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest

@pytest.fixture
def sample_data():
    return {"key": "value"}

@pytest.fixture
def setup_environment():
    # Setup code here
    env = {"setting": "configured"}
    yield env
    # Teardown code here

@pytest.fixture
def mock_database_connection():
    connection = create_mock_connection()
    yield connection
    connection.close()
2025-04-18 11:26:04,019 - INFO - After Improvement
import pytest

@pytest.fixture
def setup_environment():
    env = {"setting": "configured"}
    yield env

@pytest.fixture
def mock_database_connection():
    connection = create_mock_connection()
    yield connection
    connection.close()
2025-04-18 11:26:04,806 - INFO - Error in generating improved test cases
Test case:
import pytest

@pytest.fixture
def setup_environment():
    env = {"setting": "configured"}
    yield env

@pytest.fixture
def mock_database_connection():
    connection = create_mock_connection()
    yield connection
    connection.close()
Test error:
c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

no tests ran in 0.07s
2025-04-18 11:26:04,806 - INFO - run_each_pytest_function_individually complete
2025-04-18 11:26:04,807 - INFO - End Processing file: theory_evaluation\circle_utils.py

2025-04-18 11:26:04,807 - INFO - 
Start Processing file: theory_evaluation\config.py
2025-04-18 11:26:04,808 - INFO - Extraction of function and class start
2025-04-18 11:26:04,808 - INFO - extraction of function and class complete
2025-04-18 11:26:04,808 - INFO - Generate Unit Test Case starts
2025-04-18 11:26:04,808 - INFO - Extract unique import start
2025-04-18 11:26:05,336 - INFO - Extract unique import complete
2025-04-18 11:26:05,337 - INFO - Update relative import start
2025-04-18 11:26:05,338 - INFO - Update relative import complete
2025-04-18 11:26:06,651 - INFO - Generate Unit Test Case complete
2025-04-18 11:26:06,657 - INFO - run_each_pytest_function_individually start
2025-04-18 11:26:08,394 - INFO - Number of test case to process - 0
2025-04-18 11:26:08,395 - INFO - Verify No pytest in test_code - 
It looks like you're starting a prompt for generating unit tests. To help you further, could you please provide more details or specify the context in which you want to generate unit tests? For example, are you looking for unit tests for a specific programming language or framework, or do you have a particular function or module in mind that you need to test?
2025-04-18 11:26:08,396 - INFO - Before Improvement

from pydantic_settings import BaseSettings
from theory_evaluation.config import SETTINGS, Settings

@pytest.fixture
def sample_fixture():
    # Setup code
    data = {"key": "value"}
    yield data
    # Teardown code

@pytest.fixture
def another_fixture():
    # Setup code
    resource = open("file.txt", "w")
    yield resource
    # Teardown code
    resource.close()
2025-04-18 11:26:10,574 - INFO - After Improvement
from theory_evaluation.config import SETTINGS

@pytest.fixture
def sample_fixture():
    # Setup code
    data = {"key": "value"}
    yield data
    # Teardown code

@pytest.fixture
def another_fixture():
    # Setup code
    resource = open("file.txt", "w")
    yield resource
    # Teardown code
    resource.close()
2025-04-18 11:26:12,182 - INFO - Error in generating improved test cases
Test case:
from theory_evaluation.config import SETTINGS

@pytest.fixture
def sample_fixture():
    # Setup code
    data = {"key": "value"}
    yield data
    # Teardown code

@pytest.fixture
def another_fixture():
    # Setup code
    resource = open("file.txt", "w")
    yield resource
    # Teardown code
    resource.close()
Test error:
c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:3: in <module>
    @pytest.fixture
E   NameError: name 'pytest' is not defined
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'pytest' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 0.89s
2025-04-18 11:26:12,182 - INFO - run_each_pytest_function_individually complete
2025-04-18 11:26:12,184 - INFO - End Processing file: theory_evaluation\config.py

2025-04-18 11:26:12,184 - INFO - 
Start Processing file: theory_evaluation\llm_utils.py
2025-04-18 11:26:12,184 - INFO - Extraction of function and class start
2025-04-18 11:26:12,184 - INFO - extraction of function and class complete
2025-04-18 11:26:12,185 - INFO - Generate Unit Test Case starts
2025-04-18 11:26:12,185 - INFO - Extract unique import start
2025-04-18 11:26:12,698 - INFO - Extract unique import complete
2025-04-18 11:26:12,699 - INFO - Update relative import start
2025-04-18 11:26:12,701 - INFO - Update relative import complete
2025-04-18 11:26:13,782 - INFO - Generate Unit Test Case complete
2025-04-18 11:26:13,786 - INFO - run_each_pytest_function_individually start
2025-04-18 11:26:14,570 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-18 11:26:15,501 - INFO - Number of test case to process - 0
2025-04-18 11:26:15,501 - INFO - Verify No pytest in test_code - 
It looks like you started a prompt for generating unit tests but didn't complete it. If you need help with generating unit tests for a specific piece of code, please provide the code snippet or describe the functionality you want to test, and I can assist you in creating the appropriate unit tests.
2025-04-18 11:26:15,502 - INFO - Before Improvement

import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings

@pytest.fixture
def sample_fixture():
    return "sample data"

@pytest.fixture
def another_fixture():
    return {"key": "value"}
2025-04-18 11:26:16,396 - INFO - After Improvement
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings

@pytest.fixture
def sample_fixture():
    return "sample data"

@pytest.fixture
def another_fixture():
    return {"key": "value"}
2025-04-18 11:26:17,418 - INFO - Error in generating improved test cases
Test case:
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings

@pytest.fixture
def sample_fixture():
    return "sample data"

@pytest.fixture
def another_fixture():
    return {"key": "value"}
Test error:
c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:4: in <module>
    @pytest.fixture
E   NameError: name 'pytest' is not defined
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'pytest' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 0.25s
2025-04-18 11:26:17,419 - INFO - run_each_pytest_function_individually complete
2025-04-18 11:26:17,419 - INFO - End Processing file: theory_evaluation\llm_utils.py

2025-04-18 11:26:17,420 - INFO - 
Start Processing file: theory_evaluation\models.py
2025-04-18 11:26:17,420 - INFO - Extraction of function and class start
2025-04-18 11:26:17,420 - INFO - extraction of function and class complete
2025-04-18 11:26:17,420 - INFO - Generate Unit Test Case starts
2025-04-18 11:26:17,420 - INFO - Extract unique import start
2025-04-18 11:26:18,462 - INFO - Extract unique import complete
2025-04-18 11:26:18,463 - INFO - Update relative import start
2025-04-18 11:26:18,463 - INFO - Update relative import complete
2025-04-18 11:26:19,539 - INFO - Generate Unit Test Case complete
2025-04-18 11:26:19,546 - INFO - run_each_pytest_function_individually start
2025-04-18 11:26:20,341 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-18 11:26:21,274 - INFO - Number of test case to process - 0
2025-04-18 11:26:21,275 - INFO - Verify No pytest in test_code - 
It looks like you started a prompt for generating unit tests but didn't complete it. If you need help with generating unit tests, please provide more details about the code or functionality you want to test. This way, I can assist you in creating effective unit tests.
2025-04-18 11:26:21,275 - INFO - Before Improvement

from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import Base, ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog

@pytest.fixture
def sample_fixture():
    return {"key": "value"}

@pytest.fixture
def another_fixture():
    return [1, 2, 3]
2025-04-18 11:26:23,130 - INFO - After Improvement
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from theory_evaluation.models import Base, ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog

@pytest.fixture
def sample_fixture():
    return {"key": "value"}

@pytest.fixture
def another_fixture():
    return [1, 2, 3]
2025-04-18 11:26:24,571 - INFO - Error in generating improved test cases
Test case:
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from theory_evaluation.models import Base, ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog

@pytest.fixture
def sample_fixture():
    return {"key": "value"}

@pytest.fixture
def another_fixture():
    return [1, 2, 3]
Test error:
c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:14: in <module>
    @pytest.fixture
E   NameError: name 'pytest' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'pytest' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.75s
2025-04-18 11:26:24,571 - INFO - run_each_pytest_function_individually complete
2025-04-18 11:26:24,573 - INFO - End Processing file: theory_evaluation\models.py

2025-04-18 11:26:24,575 - INFO -                             filename  ...  percentage_passed (%)
1  theory_evaluation\circle_utils.py  ...                    0.0
2        theory_evaluation\config.py  ...                    0.0
3     theory_evaluation\llm_utils.py  ...                    0.0
4        theory_evaluation\models.py  ...                    0.0

[4 rows x 4 columns]
2025-04-18 11:26:24,602 - INFO - 
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
|    | filename                          |   total_test_cases_passed |   total_test_cases |   percentage_passed (%) |
+====+===================================+===========================+====================+=========================+
|  1 | theory_evaluation\circle_utils.py |                         0 |                  0 |                       0 |
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
|  2 | theory_evaluation\config.py       |                         0 |                  0 |                       0 |
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
|  3 | theory_evaluation\llm_utils.py    |                         0 |                  0 |                       0 |
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
|  4 | theory_evaluation\models.py       |                         0 |                  0 |                       0 |
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
