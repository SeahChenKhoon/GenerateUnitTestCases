2025-04-17 12:25:43,754 - INFO - Loading environment variables start
2025-04-17 12:25:43,759 - INFO - Loading environment variables completes
2025-04-17 12:25:43,759 - INFO - Initialising of LLM start
2025-04-17 12:25:44,102 - INFO - Initialising of LLM completes
2025-04-17 12:25:44,102 - INFO - Getting python file starts
2025-04-17 12:25:44,102 - INFO - Getting python file completes
2025-04-17 12:25:44,102 - INFO - 
Start Processing file: theory_evaluation\circle_utils.py
2025-04-17 12:25:44,103 - INFO - Extraction of function and class start
2025-04-17 12:25:44,103 - INFO - extraction of function and class complete
2025-04-17 12:25:44,103 - INFO - Generate Unit Test Case starts
2025-04-17 12:25:44,103 - INFO - Extract unique import start
2025-04-17 12:25:45,638 - INFO - Extract unique import complete
2025-04-17 12:25:45,638 - INFO - Update relative import start
2025-04-17 12:25:45,640 - INFO - Update relative import complete
2025-04-17 12:25:52,604 - INFO - Generate Unit Test Case complete
2025-04-17 12:25:52,609 - INFO - run_each_pytest_function_individually start
2025-04-17 12:25:57,129 - INFO - Number of test case to process - 6
2025-04-17 12:25:57,130 - INFO - 
TEST CASE 1 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_area_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 5.0
    expected_area = math.pi * radius ** 2

---------------
2025-04-17 12:25:58,146 - INFO - TEST CASE 1 Retry 0 - Result - Passed
2025-04-17 12:25:58,146 - INFO - 
TEST CASE 2 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_area_raises_value_error_for_negative_radius():
    # Arrange
    radius = -5.0

---------------
2025-04-17 12:25:58,921 - INFO - TEST CASE 2 Retry 0 - Result - Passed
2025-04-17 12:25:58,922 - INFO - 
TEST CASE 3 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_area_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_area = 0.0

---------------
2025-04-17 12:25:59,726 - INFO - TEST CASE 3 Retry 0 - Result - Passed
2025-04-17 12:25:59,726 - INFO - 
TEST CASE 4 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_circumference_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 5.0
    expected_circumference = 2 * math.pi * radius

---------------
2025-04-17 12:26:00,441 - INFO - TEST CASE 4 Retry 0 - Result - Passed
2025-04-17 12:26:00,441 - INFO - 
TEST CASE 5 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_circumference_raises_value_error_for_negative_radius():
    # Arrange
    radius = -5.0

---------------
2025-04-17 12:26:01,109 - INFO - TEST CASE 5 Retry 0 - Result - Passed
2025-04-17 12:26:01,109 - INFO - 
TEST CASE 6 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_circumference_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_circumference = 0.0

---------------
2025-04-17 12:26:01,885 - INFO - TEST CASE 6 Retry 0 - Result - Passed
2025-04-17 12:26:01,886 - INFO - Before Improvement
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest


import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest


def test_circle_area_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 5.0
    expected_area = math.pi * radius ** 2

def test_circle_area_raises_value_error_for_negative_radius():
    # Arrange
    radius = -5.0

def test_circle_area_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_area = 0.0

def test_circle_circumference_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 5.0
    expected_circumference = 2 * math.pi * radius

def test_circle_circumference_raises_value_error_for_negative_radius():
    # Arrange
    radius = -5.0

def test_circle_circumference_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_circumference = 0.0

2025-04-17 12:26:05,693 - INFO - After Improvement
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest

def test_circle_area_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 5.0
    expected_area = math.pi * radius ** 2

def test_circle_area_raises_value_error_for_negative_radius():
    # Arrange
    radius = -5.0

def test_circle_area_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_area = 0.0

def test_circle_circumference_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 5.0
    expected_circumference = 2 * math.pi * radius

def test_circle_circumference_raises_value_error_for_negative_radius():
    # Arrange
    radius = -5.0

def test_circle_circumference_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_circumference = 0.0
2025-04-17 12:26:06,605 - INFO - Improvement of test cases processed successfully
2025-04-17 12:26:06,607 - INFO - run_each_pytest_function_individually complete
2025-04-17 12:26:06,607 - INFO - End Processing file: theory_evaluation\circle_utils.py

2025-04-17 12:26:06,608 - INFO - 
Start Processing file: theory_evaluation\llm_handler.py
2025-04-17 12:26:06,608 - INFO - Extraction of function and class start
2025-04-17 12:26:06,609 - INFO - extraction of function and class complete
2025-04-17 12:26:06,609 - INFO - Generate Unit Test Case starts
2025-04-17 12:26:06,609 - INFO - Extract unique import start
2025-04-17 12:26:07,445 - INFO - Extract unique import complete
2025-04-17 12:26:07,446 - INFO - Update relative import start
2025-04-17 12:26:07,446 - INFO - Update relative import complete
2025-04-17 12:26:17,072 - INFO - Generate Unit Test Case complete
2025-04-17 12:26:17,082 - INFO - run_each_pytest_function_individually start
2025-04-17 12:26:27,393 - INFO - Number of test case to process - 12
2025-04-17 12:26:27,393 - INFO - 
TEST CASE 1 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_default_values():
    llm = OpenAI_llm()
    assert llm.message == DEFAULT_MESSAGE
    assert llm.image_input is None
    assert llm.azure_endpoint is None
    assert llm.api_version is None
    assert llm.model_name is None
    assert llm.max_retries == 3
    assert llm.output is None
    assert llm.mode == "text_generation"
    assert llm.config == DEFAULT_CONFIG
    assert llm.verbose is False

---------------
2025-04-17 12:26:29,908 - INFO - TEST CASE 1 Retry 0 - Result - Failed
2025-04-17 12:26:29,910 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_init_with_default_values ________________________
temp\temp.py:15: in test_init_with_default_values
    assert llm.message == DEFAULT_MESSAGE
E   NameError: name 'DEFAULT_MESSAGE' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_init_with_default_values - NameError: name 'DEFAULT...
1 failed in 1.55s
2025-04-17 12:26:31,316 - INFO - 
TEST CASE 1 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from llm_handler import OpenAI_llm, DEFAULT_MESSAGE, DEFAULT_CONFIG

def test_init_with_default_values():
    llm = OpenAI_llm()
    assert llm.message == DEFAULT_MESSAGE
    assert llm.image_input is None
    assert llm.azure_endpoint is None
    assert llm.api_version is None
    assert llm.model_name is None
    assert llm.max_retries == 3
    assert llm.output is None
    assert llm.mode == "text_generation"
    assert llm.config == DEFAULT_CONFIG
    assert llm.verbose is False

---------------
2025-04-17 12:26:33,660 - INFO - TEST CASE 1 Retry 1 - Result - Failed
2025-04-17 12:26:33,661 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:13: in <module>
    from llm_handler import OpenAI_llm, DEFAULT_MESSAGE, DEFAULT_CONFIG
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.43s
2025-04-17 12:26:35,598 - INFO - 
TEST CASE 1 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from llm_handler import OpenAI_llm, DEFAULT_MESSAGE, DEFAULT_CONFIG

def test_init_with_default_values():
    llm = OpenAI_llm()
    assert llm.message == DEFAULT_MESSAGE
    assert llm.image_input is None
    assert llm.azure_endpoint is None
    assert llm.api_version is None
    assert llm.model_name is None
    assert llm.max_retries == 3
    assert llm.output is None
    assert llm.mode == "text_generation"
    assert llm.config == DEFAULT_CONFIG
    assert llm.verbose is False

---------------
2025-04-17 12:26:37,790 - INFO - TEST CASE 1 Retry 2 - Result - Failed
2025-04-17 12:26:37,790 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:13: in <module>
    from llm_handler import OpenAI_llm, DEFAULT_MESSAGE, DEFAULT_CONFIG
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.23s
2025-04-17 12:26:41,527 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:26:41,528 - INFO - Failed after all retries for test case 1
2025-04-17 12:26:41,528 - INFO - 
TEST CASE 2 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_invalid_message():
    with pytest.raises(AssertionError, match="Prompt message must be inserted."):
        OpenAI_llm(message="")

---------------
2025-04-17 12:26:43,907 - INFO - TEST CASE 2 Retry 0 - Result - Passed
2025-04-17 12:26:43,908 - INFO - 
TEST CASE 3 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_invalid_output():
    with pytest.raises(AssertionError, match="Output must be either 'json', 'stream', or None"):
        OpenAI_llm(output="invalid")

---------------
2025-04-17 12:26:45,754 - INFO - TEST CASE 3 Retry 0 - Result - Passed
2025-04-17 12:26:45,754 - INFO - 
TEST CASE 4 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_invalid_mode():
    with pytest.raises(AssertionError, match="mode must be either 'text_generation' or 'vision'"):
        OpenAI_llm(mode="invalid")

---------------
2025-04-17 12:26:47,453 - INFO - TEST CASE 4 Retry 0 - Result - Passed
2025-04-17 12:26:47,454 - INFO - 
TEST CASE 5 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_uses_azure_openai(mock_openai, mock_azure_openai):
    llm = OpenAI_llm(useAzureOpenAI=True)
    mock_azure_openai.assert_called_once()
    mock_openai.assert_not_called()

---------------
2025-04-17 12:26:49,704 - INFO - TEST CASE 5 Retry 0 - Result - Failed
2025-04-17 12:26:49,705 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
________________ ERROR at setup of test_init_uses_azure_openai ________________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 13
  def test_init_uses_azure_openai(mock_openai, mock_azure_openai):
E       fixture 'mock_openai' not found
>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, default_llm, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, temp/temp.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13
=========================== short test summary info ===========================
ERROR temp/temp.py::test_init_uses_azure_openai
1 error in 1.26s
2025-04-17 12:26:51,660 - INFO - 
TEST CASE 5 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch

@patch('openai.AzureOpenAI')
@patch('openai.OpenAI')
def test_init_uses_azure_openai(mock_openai, mock_azure_openai):
    llm = OpenAI_llm(useAzureOpenAI=True)
    mock_azure_openai.assert_called_once()
    mock_openai.assert_not_called()

---------------
2025-04-17 12:26:53,811 - INFO - TEST CASE 5 Retry 1 - Result - Failed
2025-04-17 12:26:53,811 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_init_uses_azure_openai _________________________
temp\temp.py:18: in test_init_uses_azure_openai
    llm = OpenAI_llm(useAzureOpenAI=True)
theory_evaluation\llm_handler.py:63: in __init__
    self.client = AzureOpenAI(
.venv\Lib\site-packages\openai\lib\azure.py:194: in __init__
    raise OpenAIError(
E   openai.OpenAIError: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.
=========================== short test summary info ===========================
FAILED temp/temp.py::test_init_uses_azure_openai - openai.OpenAIError: Missin...
1 failed in 1.26s
2025-04-17 12:26:56,073 - INFO - 
TEST CASE 5 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch

@patch('openai.AzureOpenAI', autospec=True)
@patch('openai.OpenAI', autospec=True)
def test_init_uses_azure_openai(mock_openai, mock_azure_openai):
    mock_azure_openai.return_value = None
    llm = OpenAI_llm(useAzureOpenAI=True)
    mock_azure_openai.assert_called_once_with(
        azure_endpoint=llm.azure_endpoint,
        api_key=os.getenv("AZURE_OPENAI_API_KEY_SWEDEN"),
        api_version=llm.api_version,
        max_retries=llm.max_retries,
    )
    mock_openai.assert_not_called()

---------------
2025-04-17 12:26:58,163 - INFO - TEST CASE 5 Retry 2 - Result - Failed
2025-04-17 12:26:58,165 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_init_uses_azure_openai _________________________
temp\temp.py:19: in test_init_uses_azure_openai
    llm = OpenAI_llm(useAzureOpenAI=True)
theory_evaluation\llm_handler.py:63: in __init__
    self.client = AzureOpenAI(
.venv\Lib\site-packages\openai\lib\azure.py:194: in __init__
    raise OpenAIError(
E   openai.OpenAIError: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.
=========================== short test summary info ===========================
FAILED temp/temp.py::test_init_uses_azure_openai - openai.OpenAIError: Missin...
1 failed in 1.20s
2025-04-17 12:27:00,583 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:27:00,585 - INFO - Failed after all retries for test case 5
2025-04-17 12:27:00,585 - INFO - 
TEST CASE 6 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_uses_openai(mock_openai, mock_azure_openai):
    llm = OpenAI_llm(useAzureOpenAI=False)
    mock_openai.assert_called_once()
    mock_azure_openai.assert_not_called()

---------------
2025-04-17 12:27:02,470 - INFO - TEST CASE 6 Retry 0 - Result - Failed
2025-04-17 12:27:02,470 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
___________________ ERROR at setup of test_init_uses_openai ___________________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 13
  def test_init_uses_openai(mock_openai, mock_azure_openai):
E       fixture 'mock_openai' not found
>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, default_llm, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, temp/temp.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13
=========================== short test summary info ===========================
ERROR temp/temp.py::test_init_uses_openai
1 error in 0.98s
2025-04-17 12:27:03,492 - INFO - 
TEST CASE 6 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch

@patch('llm_handler.OpenAI')
@patch('llm_handler.AzureOpenAI')
def test_init_uses_openai(mock_azure_openai, mock_openai):
    llm = OpenAI_llm(useAzureOpenAI=False)
    mock_openai.assert_called_once()
    mock_azure_openai.assert_not_called()

---------------
2025-04-17 12:27:05,876 - INFO - TEST CASE 6 Retry 1 - Result - Failed
2025-04-17 12:27:05,876 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_init_uses_openai ____________________________
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1421: in patched
    with self.decoration_helper(patched,
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\contextlib.py:141: in __enter__
    return next(self.gen)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1403: in decoration_helper
    arg = exit_stack.enter_context(patching)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\contextlib.py:530: in enter_context
    result = _enter(cm)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1479: in __enter__
    self.target = self.getter()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_init_uses_openai - ModuleNotFoundError: No module n...
1 failed in 1.39s
2025-04-17 12:27:07,564 - INFO - 
TEST CASE 6 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch
import sys

sys.modules['llm_handler'] = __import__('__main__')

@patch('llm_handler.OpenAI')
@patch('llm_handler.AzureOpenAI')
def test_init_uses_openai(mock_azure_openai, mock_openai):
    llm = OpenAI_llm(useAzureOpenAI=False)
    mock_openai.assert_called_once()
    mock_azure_openai.assert_not_called()

---------------
2025-04-17 12:27:10,126 - INFO - TEST CASE 6 Retry 2 - Result - Failed
2025-04-17 12:27:10,127 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_init_uses_openai ____________________________
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1421: in patched
    with self.decoration_helper(patched,
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\contextlib.py:141: in __enter__
    return next(self.gen)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1403: in decoration_helper
    arg = exit_stack.enter_context(patching)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\contextlib.py:530: in enter_context
    result = _enter(cm)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <module '__main__' from 'C:\\ChenKhoon\\JupyterNotebook\\GenerateUnitTestCases\\.venv\\Scripts\\pytest.exe\\__main__.py'> does not have the attribute 'AzureOpenAI'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_init_uses_openai - AttributeError: <module '__main_...
1 failed in 1.65s
2025-04-17 12:27:11,881 - INFO - Failed after all retries for test case 6
2025-04-17 12:27:11,882 - INFO - 
TEST CASE 7 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

async def test_run_json_output(mock_json_completion, default_llm):
    default_llm.output = "json"
    mock_json_completion.return_value = {"answer": "test"}
    result = [response async for response in default_llm._run()]
    assert result == [{"answer": "test"}]

---------------
2025-04-17 12:27:13,962 - INFO - TEST CASE 7 Retry 0 - Result - Failed
2025-04-17 12:27:13,962 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
___________________ ERROR at setup of test_run_json_output ____________________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 13
  async def test_run_json_output(mock_json_completion, default_llm):
      default_llm.output = "json"
E       fixture 'mock_json_completion' not found
>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, default_llm, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, temp/temp.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13
=========================== short test summary info ===========================
ERROR temp/temp.py::test_run_json_output
1 error in 1.03s
2025-04-17 12:27:15,619 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:27:15,620 - INFO - 
TEST CASE 7 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import AsyncMock

@pytest.fixture
def mock_json_completion():
    mock = AsyncMock()
    mock.return_value = {"answer": "test"}
    return mock

@pytest.fixture
def default_llm():
    from llm_handler import OpenAI_llm
    return OpenAI_llm(output="json")

@pytest.mark.asyncio
async def test_run_json_output(mock_json_completion, default_llm):
    default_llm._OpenAI_JSON_Completion = mock_json_completion
    result = [response async for response in default_llm._run()]
    assert result == [{"answer": "test"}]

---------------
2025-04-17 12:27:17,562 - INFO - TEST CASE 7 Retry 1 - Result - Failed
2025-04-17 12:27:17,562 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
___________________ ERROR at setup of test_run_json_output ____________________
temp\temp.py:24: in default_llm
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py::test_run_json_output - ModuleNotFoundError: No module nam...
1 error in 1.06s
2025-04-17 12:27:19,821 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:27:19,822 - INFO - 
TEST CASE 7 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import AsyncMock

@pytest.fixture
def mock_json_completion():
    mock = AsyncMock()
    mock.return_value = {"answer": "test"}
    return mock

@pytest.fixture
def default_llm():
    from llm_handler import OpenAI_llm
    return OpenAI_llm(message="You are a helpful assistant designed to answer and give tips on any queries.", output="json")

@pytest.mark.asyncio
async def test_run_json_output(mock_json_completion, default_llm):
    default_llm._OpenAI_JSON_Completion = mock_json_completion
    result = [response async for response in default_llm._run()]
    assert result == [{"answer": "test"}]

---------------
2025-04-17 12:27:21,934 - INFO - TEST CASE 7 Retry 2 - Result - Failed
2025-04-17 12:27:21,934 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
___________________ ERROR at setup of test_run_json_output ____________________
temp\temp.py:24: in default_llm
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py::test_run_json_output - ModuleNotFoundError: No module nam...
1 error in 1.16s
2025-04-17 12:27:24,027 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:27:24,027 - INFO - Failed after all retries for test case 7
2025-04-17 12:27:24,027 - INFO - 
TEST CASE 8 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

async def test_run_stream_output(mock_streaming, default_llm):
    default_llm.output = "stream"
    mock_streaming.return_value = AsyncMock()
    mock_streaming.return_value.__aiter__.return_value = ["chunk1", "chunk2"]
    result = [response async for response in default_llm._run()]
    assert result == ["chunk1", "chunk2"]

---------------
2025-04-17 12:27:26,320 - INFO - TEST CASE 8 Retry 0 - Result - Failed
2025-04-17 12:27:26,320 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_run_stream_output ___________________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 13
  async def test_run_stream_output(mock_streaming, default_llm):
      default_llm.output = "stream"
E       fixture 'mock_streaming' not found
>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, default_llm, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, temp/temp.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13
=========================== short test summary info ===========================
ERROR temp/temp.py::test_run_stream_output
1 error in 1.40s
2025-04-17 12:27:29,267 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:27:29,267 - INFO - 
TEST CASE 8 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import AsyncMock, patch
from llm_handler import OpenAI_llm

@pytest.fixture
def default_llm():
    return OpenAI_llm(message="Test message", output="stream")

@pytest.fixture
def mock_streaming():
    with patch('llm_handler.OpenAI_llm._OpenAI_Streaming', new_callable=AsyncMock) as mock:
        yield mock

@pytest.mark.asyncio
async def test_run_stream_output(mock_streaming, default_llm):
    default_llm.output = "stream"
    mock_streaming.return_value = AsyncMock()
    mock_streaming.return_value.__aiter__.return_value = ["chunk1", "chunk2"]
    result = [response async for response in default_llm._run()]
    assert result == ["chunk1", "chunk2"]

---------------
2025-04-17 12:27:31,410 - INFO - TEST CASE 8 Retry 1 - Result - Failed
2025-04-17 12:27:31,410 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:15: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.29s
2025-04-17 12:27:33,995 - INFO - 
TEST CASE 8 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import AsyncMock, patch
import sys
import os

# Add the directory containing llm_handler.py to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from llm_handler import OpenAI_llm

@pytest.fixture
def default_llm():
    return OpenAI_llm(message="Test message", output="stream")

@pytest.fixture
def mock_streaming():
    with patch('llm_handler.OpenAI_llm._OpenAI_Streaming', new_callable=AsyncMock) as mock:
        yield mock

@pytest.mark.asyncio
async def test_run_stream_output(mock_streaming, default_llm):
    default_llm.output = "stream"
    mock_streaming.return_value = AsyncMock()
    mock_streaming.return_value.__aiter__.return_value = ["chunk1", "chunk2"]
    result = [response async for response in default_llm._run()]
    assert result == ["chunk1", "chunk2"]

---------------
2025-04-17 12:27:36,067 - INFO - TEST CASE 8 Retry 2 - Result - Failed
2025-04-17 12:27:36,068 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:21: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.16s
2025-04-17 12:27:38,404 - INFO - Failed after all retries for test case 8
2025-04-17 12:27:38,404 - INFO - 
TEST CASE 9 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

async def test_run_default_output(mock_chat_completion, default_llm):
    mock_chat_completion.return_value = "response"
    result = [response async for response in default_llm._run()]
    assert result == ["response"]

---------------
2025-04-17 12:27:41,001 - INFO - TEST CASE 9 Retry 0 - Result - Failed
2025-04-17 12:27:41,001 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_run_default_output __________________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 13
  async def test_run_default_output(mock_chat_completion, default_llm):
      mock_chat_completion.return_value = "response"
      result = [response async for response in default_llm._run()]
      assert result == ["response"]
E       fixture 'mock_chat_completion' not found
>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, default_llm, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, temp/temp.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13
=========================== short test summary info ===========================
ERROR temp/temp.py::test_run_default_output
1 error in 1.83s
2025-04-17 12:27:42,585 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:27:42,585 - INFO - 
TEST CASE 9 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import AsyncMock

@pytest.fixture
def mock_chat_completion():
    return AsyncMock()

@pytest.fixture
def default_llm():
    return OpenAI_llm(message="Test message", useAzureOpenAI=False)

@pytest.mark.asyncio
async def test_run_default_output(mock_chat_completion, default_llm):
    mock_chat_completion.return_value = "response"
    default_llm._OpenAI_Chat_Completion = mock_chat_completion
    result = [response async for response in default_llm._run()]
    assert result == ["response"]

---------------
2025-04-17 12:27:44,787 - INFO - TEST CASE 9 Retry 1 - Result - Passed
2025-04-17 12:27:44,787 - INFO - 
TEST CASE 10 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

async def test_execute_text_generation_mode(mock_run, default_llm):
    mock_run.return_value = AsyncMock()
    mock_run.return_value.__aiter__.return_value = ["response"]
    result = [response async for response in default_llm.execute()]
    assert result == ["response"]

---------------
2025-04-17 12:27:46,751 - INFO - TEST CASE 10 Retry 0 - Result - Failed
2025-04-17 12:27:46,751 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_____________ ERROR at setup of test_execute_text_generation_mode _____________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 13
  async def test_execute_text_generation_mode(mock_run, default_llm):
      mock_run.return_value = AsyncMock()
      mock_run.return_value.__aiter__.return_value = ["response"]
      result = [response async for response in default_llm.execute()]
      assert result == ["response"]
E       fixture 'mock_run' not found
>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, default_llm, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, temp/temp.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13
=========================== short test summary info ===========================
ERROR temp/temp.py::test_execute_text_generation_mode
1 error in 1.03s
2025-04-17 12:27:48,612 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:27:48,612 - INFO - 
TEST CASE 10 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import AsyncMock, patch
from llm_handler import OpenAI_llm

@pytest.fixture
def default_llm():
    return OpenAI_llm(message="Test message", useAzureOpenAI=False, output="stream")

@pytest.mark.asyncio
@patch('llm_handler.OpenAI_llm._run', new_callable=AsyncMock)
async def test_execute_text_generation_mode(mock_run, default_llm):
    mock_run.return_value = AsyncMock()
    mock_run.return_value.__aiter__.return_value = ["response"]
    result = [response async for response in default_llm.execute()]
    assert result == ["response"]

---------------
2025-04-17 12:27:50,860 - INFO - TEST CASE 10 Retry 1 - Result - Failed
2025-04-17 12:27:50,861 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:15: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.21s
2025-04-17 12:27:53,021 - INFO - 
TEST CASE 10 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import sys
import os
import pytest
from unittest.mock import AsyncMock, patch

# Add the directory containing llm_handler.py to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from llm_handler import OpenAI_llm

@pytest.fixture
def default_llm():
    return OpenAI_llm(message="Test message", useAzureOpenAI=False, output="stream")

@pytest.mark.asyncio
@patch('llm_handler.OpenAI_llm._run', new_callable=AsyncMock)
async def test_execute_text_generation_mode(mock_run, default_llm):
    mock_run.return_value = AsyncMock()
    mock_run.return_value.__aiter__.return_value = ["response"]
    result = [response async for response in default_llm.execute()]
    assert result == ["response"]

---------------
2025-04-17 12:27:55,129 - INFO - TEST CASE 10 Retry 2 - Result - Failed
2025-04-17 12:27:55,129 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:21: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.20s
2025-04-17 12:27:58,161 - INFO - Failed after all retries for test case 10
2025-04-17 12:27:58,161 - INFO - 
TEST CASE 11 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

async def test_execute_vision_mode(mock_run, default_llm):
    default_llm.mode = "vision"
    default_llm.image_input = "image_data"
    mock_run.return_value = AsyncMock()
    mock_run.return_value.__aiter__.return_value = ["response"]
    result = [response async for response in default_llm.execute()]
    assert result == ["response"]

---------------
2025-04-17 12:27:59,927 - INFO - TEST CASE 11 Retry 0 - Result - Failed
2025-04-17 12:27:59,928 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_________________ ERROR at setup of test_execute_vision_mode __________________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 13
  async def test_execute_vision_mode(mock_run, default_llm):
      default_llm.mode = "vision"
E       fixture 'mock_run' not found
>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, default_llm, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, temp/temp.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13
=========================== short test summary info ===========================
ERROR temp/temp.py::test_execute_vision_mode
1 error in 0.91s
2025-04-17 12:28:01,781 - INFO - 
TEST CASE 11 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import AsyncMock, patch
import pytest

@pytest.fixture
def default_llm():
    from llm_handler import OpenAI_llm
    return OpenAI_llm()

@patch('llm_handler.OpenAI_llm._run', new_callable=AsyncMock)
async def test_execute_vision_mode(mock_run, default_llm):
    default_llm.mode = "vision"
    default_llm.image_input = "image_data"
    mock_run.return_value = AsyncMock()
    mock_run.return_value.__aiter__.return_value = ["response"]
    result = [response async for response in default_llm.execute()]
    assert result == ["response"]

---------------
2025-04-17 12:28:03,552 - INFO - TEST CASE 11 Retry 1 - Result - Failed
2025-04-17 12:28:03,552 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_________________ ERROR at setup of test_execute_vision_mode __________________
temp\temp.py:18: in default_llm
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py::test_execute_vision_mode - ModuleNotFoundError: No module...
1 error in 1.04s
2025-04-17 12:28:06,044 - INFO - 
TEST CASE 11 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import AsyncMock, patch
import pytest
import sys
from pathlib import Path

# Add the directory containing llm_handler.py to the system path
sys.path.append(str(Path(__file__).resolve().parent.parent))

@pytest.fixture
def default_llm():
    from llm_handler import OpenAI_llm
    return OpenAI_llm()

@patch('llm_handler.OpenAI_llm._run', new_callable=AsyncMock)
async def test_execute_vision_mode(mock_run, default_llm):
    default_llm.mode = "vision"
    default_llm.image_input = "image_data"
    mock_run.return_value = AsyncMock()
    mock_run.return_value.__aiter__.return_value = ["response"]
    result = [response async for response in default_llm.execute()]
    assert result == ["response"]

---------------
2025-04-17 12:28:08,126 - INFO - TEST CASE 11 Retry 2 - Result - Failed
2025-04-17 12:28:08,126 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_________________ ERROR at setup of test_execute_vision_mode __________________
temp\temp.py:23: in default_llm
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py::test_execute_vision_mode - ModuleNotFoundError: No module...
1 error in 1.21s
2025-04-17 12:28:10,432 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:28:10,433 - INFO - Failed after all retries for test case 11
2025-04-17 12:28:10,433 - INFO - 
TEST CASE 12 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

async def test_execute_handles_exception(mock_run, default_llm):
    mock_run.side_effect = Exception("Test exception")
    result = [response async for response in default_llm.execute()]
    assert result == ["Error: Test exception"]

---------------
2025-04-17 12:28:12,286 - INFO - TEST CASE 12 Retry 0 - Result - Failed
2025-04-17 12:28:12,286 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
______________ ERROR at setup of test_execute_handles_exception _______________
file C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py, line 13
  async def test_execute_handles_exception(mock_run, default_llm):
      mock_run.side_effect = Exception("Test exception")
      result = [response async for response in default_llm.execute()]
      assert result == ["Error: Test exception"]
E       fixture 'mock_run' not found
>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, default_llm, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, temp/temp.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:13
=========================== short test summary info ===========================
ERROR temp/temp.py::test_execute_handles_exception
1 error in 1.08s
2025-04-17 12:28:14,416 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:28:14,417 - INFO - 
TEST CASE 12 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import AsyncMock

@pytest.fixture
def mock_run():
    return AsyncMock()

@pytest.fixture
def default_llm():
    return OpenAI_llm(message="Test message", useAzureOpenAI=False, output="stream")

@pytest.mark.asyncio
async def test_execute_handles_exception(mock_run, default_llm):
    default_llm._run = mock_run
    mock_run.side_effect = Exception("Test exception")
    result = [response async for response in default_llm.execute()]
    assert result == ["Error: Test exception"]

---------------
2025-04-17 12:28:16,848 - INFO - TEST CASE 12 Retry 1 - Result - Failed
2025-04-17 12:28:16,848 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_execute_handles_exception ________________________
temp\temp.py:29: in test_execute_handles_exception
    assert result == ["Error: Test exception"]
E   assert ["Error: 'asy...ot coroutine"] == ['Error: Test exception']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'Error: Test exception'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_handles_exception
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_handles_exception - assert ["Error: 'asy......
1 failed, 1 warning in 1.63s
2025-04-17 12:28:18,770 - INFO - 
TEST CASE 12 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import AsyncMock

@pytest.fixture
def mock_run():
    return AsyncMock()

@pytest.fixture
def default_llm(mock_run):
    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False, output="stream")
    llm._run = mock_run
    return llm

@pytest.mark.asyncio
async def test_execute_handles_exception(default_llm, mock_run):
    mock_run.side_effect = Exception("Test exception")
    result = [response async for response in default_llm.execute()]
    assert result == ["Error: Test exception"]

---------------
2025-04-17 12:28:21,127 - INFO - TEST CASE 12 Retry 2 - Result - Failed
2025-04-17 12:28:21,127 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_execute_handles_exception ________________________
temp\temp.py:30: in test_execute_handles_exception
    assert result == ["Error: Test exception"]
E   assert ["Error: 'asy...ot coroutine"] == ['Error: Test exception']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'Error: Test exception'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_handles_exception
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_handles_exception - assert ["Error: 'asy......
1 failed, 1 warning in 1.53s
2025-04-17 12:28:24,032 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:28:24,033 - INFO - Failed after all retries for test case 12
2025-04-17 12:28:24,033 - INFO - Before Improvement
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()
def test_init_with_invalid_message():
    with pytest.raises(AssertionError, match="Prompt message must be inserted."):
        OpenAI_llm(message="")

def test_init_with_invalid_output():
    with pytest.raises(AssertionError, match="Output must be either 'json', 'stream', or None"):
        OpenAI_llm(output="invalid")

def test_init_with_invalid_mode():
    with pytest.raises(AssertionError, match="mode must be either 'text_generation' or 'vision'"):
        OpenAI_llm(mode="invalid")

import pytest
from unittest.mock import AsyncMock

@pytest.fixture
def mock_chat_completion():
    return AsyncMock()

@pytest.fixture
def default_llm():
    return OpenAI_llm(message="Test message", useAzureOpenAI=False)

@pytest.mark.asyncio
async def test_run_default_output(mock_chat_completion, default_llm):
    mock_chat_completion.return_value = "response"
    default_llm._OpenAI_Chat_Completion = mock_chat_completion
    result = [response async for response in default_llm._run()]
    assert result == ["response"]

2025-04-17 12:28:27,142 - INFO - After Improvement
import pytest
from unittest.mock import AsyncMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_invalid_message():
    with pytest.raises(AssertionError, match="Prompt message must be inserted."):
        OpenAI_llm(message="")

def test_init_with_invalid_output():
    with pytest.raises(AssertionError, match="Output must be either 'json', 'stream', or None"):
        OpenAI_llm(output="invalid")

def test_init_with_invalid_mode():
    with pytest.raises(AssertionError, match="mode must be either 'text_generation' or 'vision'"):
        OpenAI_llm(mode="invalid")

@pytest.fixture
def mock_chat_completion():
    return AsyncMock()

@pytest.fixture
def default_llm():
    return OpenAI_llm(message="Test message", useAzureOpenAI=False)

@pytest.mark.asyncio
async def test_run_default_output(mock_chat_completion, default_llm):
    mock_chat_completion.return_value = "response"
    default_llm._OpenAI_Chat_Completion = mock_chat_completion
    result = [response async for response in default_llm._run()]
    assert result == ["response"]
2025-04-17 12:28:29,216 - INFO - Improvement of test cases processed successfully
2025-04-17 12:28:29,216 - INFO - run_each_pytest_function_individually complete
2025-04-17 12:28:29,217 - ERROR - Failed processing theory_evaluation\llm_handler.py: 'str' object has no attribute 'relative_to'
2025-04-17 12:28:29,217 - INFO - End Processing file: theory_evaluation\llm_handler.py

2025-04-17 12:28:29,217 - INFO - 
Start Processing file: theory_evaluation\llm_utils.py
2025-04-17 12:28:29,218 - INFO - Extraction of function and class start
2025-04-17 12:28:29,219 - INFO - extraction of function and class complete
2025-04-17 12:28:29,219 - INFO - Generate Unit Test Case starts
2025-04-17 12:28:29,219 - INFO - Extract unique import start
2025-04-17 12:28:29,795 - INFO - Extract unique import complete
2025-04-17 12:28:29,796 - INFO - Update relative import start
2025-04-17 12:28:29,797 - INFO - Update relative import complete
2025-04-17 12:28:37,561 - INFO - Generate Unit Test Case complete
2025-04-17 12:28:37,565 - INFO - run_each_pytest_function_individually start
2025-04-17 12:28:43,804 - INFO - Number of test case to process - 4
2025-04-17 12:28:43,804 - INFO - 
TEST CASE 1 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_prompt_returns_correct_structure():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_yaml = "key1: value1\nkey2: value2"
    prompt_txt = "This is a prompt with {$key1} and {$key2}."
    expected_prompt = "This is a prompt with value1 and value2."

---------------
2025-04-17 12:28:44,624 - INFO - TEST CASE 1 Retry 0 - Result - Passed
2025-04-17 12:28:44,624 - INFO - 
TEST CASE 2 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_prompt_raises_exception_on_missing_config():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    prompt_txt = "This is a prompt with {$key1}."

---------------
2025-04-17 12:28:45,333 - INFO - TEST CASE 2 Retry 0 - Result - Passed
2025-04-17 12:28:45,334 - INFO - 
TEST CASE 3 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_settings_returns_correct_settings():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    settings_yaml = "setting1: value1\nsetting2: value2"
    expected_settings = {"setting1": "value1", "setting2": "value2"}

---------------
2025-04-17 12:28:46,160 - INFO - TEST CASE 3 Retry 0 - Result - Passed
2025-04-17 12:28:46,161 - INFO - 
TEST CASE 4 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_settings_raises_exception_on_missing_file():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"

---------------
2025-04-17 12:28:46,797 - INFO - TEST CASE 4 Retry 0 - Result - Passed
2025-04-17 12:28:46,798 - INFO - Before Improvement
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest


import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest


def test_initialise_prompt_returns_correct_structure():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_yaml = "key1: value1\nkey2: value2"
    prompt_txt = "This is a prompt with {$key1} and {$key2}."
    expected_prompt = "This is a prompt with value1 and value2."

def test_initialise_prompt_raises_exception_on_missing_config():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    prompt_txt = "This is a prompt with {$key1}."

def test_initialise_settings_returns_correct_settings():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    settings_yaml = "setting1: value1\nsetting2: value2"
    expected_settings = {"setting1": "value1", "setting2": "value2"}

def test_initialise_settings_raises_exception_on_missing_file():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"

2025-04-17 12:28:49,644 - INFO - After Improvement
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest


def test_initialise_prompt_returns_correct_structure():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_yaml = "key1: value1\nkey2: value2"
    prompt_txt = "This is a prompt with {$key1} and {$key2}."
    expected_prompt = "This is a prompt with value1 and value2."

def test_initialise_prompt_raises_exception_on_missing_config():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    prompt_txt = "This is a prompt with {$key1}."

def test_initialise_settings_returns_correct_settings():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    settings_yaml = "setting1: value1\nsetting2: value2"
    expected_settings = {"setting1": "value1", "setting2": "value2"}

def test_initialise_settings_raises_exception_on_missing_file():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
2025-04-17 12:28:50,536 - INFO - Improvement of test cases processed successfully
2025-04-17 12:28:50,537 - INFO - run_each_pytest_function_individually complete
2025-04-17 12:28:50,537 - INFO - End Processing file: theory_evaluation\llm_utils.py

2025-04-17 12:28:50,537 - INFO - 
Start Processing file: theory_evaluation\models.py
2025-04-17 12:28:50,537 - INFO - Extraction of function and class start
2025-04-17 12:28:50,537 - INFO - extraction of function and class complete
2025-04-17 12:28:50,537 - INFO - Generate Unit Test Case starts
2025-04-17 12:28:50,537 - INFO - Extract unique import start
2025-04-17 12:28:51,843 - INFO - Extract unique import complete
2025-04-17 12:28:51,843 - INFO - Update relative import start
2025-04-17 12:28:51,844 - INFO - Update relative import complete
2025-04-17 12:29:02,516 - INFO - Generate Unit Test Case complete
2025-04-17 12:29:02,523 - INFO - run_each_pytest_function_individually start
2025-04-17 12:29:11,231 - INFO - Number of test case to process - 6
2025-04-17 12:29:11,232 - INFO - 
TEST CASE 1 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

def test_user_info_creation(setup_database):
    session = setup_database
    user = UserInfo(
        first_name="John",
        last_name="Doe",
        email="john.doe@example.com",
        github_username="johndoe",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )
    session.add(user)
    session.commit()
    assert user.id is not None
    assert user.email == "john.doe@example.com"

---------------
2025-04-17 12:29:12,974 - INFO - TEST CASE 1 Retry 0 - Result - Failed
2025-04-17 12:29:12,974 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_user_info_creation __________________
temp\temp.py:22: in setup_database
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_user_info_creation - NameError: name 'Base' is not d...
1 warning, 1 error in 0.94s
2025-04-17 12:29:15,362 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:29:15,363 - INFO - 
TEST CASE 1 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from your_module import Base, UserInfo  # Replace 'your_module' with the actual module name

@pytest.fixture(scope="function")
def setup_database():
    engine = create_engine('sqlite:///:memory:')  # Use an in-memory SQLite database for testing
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_user_info_creation(setup_database):
    session = setup_database
    user = UserInfo(
        first_name="John",
        last_name="Doe",
        email="john.doe@example.com",
        github_username="johndoe",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )
    session.add(user)
    session.commit()
    assert user.id is not None
    assert user.email == "john.doe@example.com"

---------------
2025-04-17 12:29:17,172 - INFO - TEST CASE 1 Retry 1 - Result - Failed
2025-04-17 12:29:17,173 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:32: in <module>
    from your_module import Base, UserInfo  # Replace 'your_module' with the actual module name
E   ModuleNotFoundError: No module named 'your_module'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.84s
2025-04-17 12:29:19,856 - INFO - 
TEST CASE 1 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from your_actual_module_name import Base, UserInfo  # Replace 'your_actual_module_name' with the actual module name

@pytest.fixture(scope="function")
def setup_database():
    engine = create_engine('sqlite:///:memory:')  # Use an in-memory SQLite database for testing
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_user_info_creation(setup_database):
    session = setup_database
    user = UserInfo(
        first_name="John",
        last_name="Doe",
        email="john.doe@example.com",
        github_username="johndoe",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )
    session.add(user)
    session.commit()
    assert user.id is not None
    assert user.email == "john.doe@example.com"

---------------
2025-04-17 12:29:21,436 - INFO - TEST CASE 1 Retry 2 - Result - Failed
2025-04-17 12:29:21,437 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:32: in <module>
    from your_actual_module_name import Base, UserInfo  # Replace 'your_actual_module_name' with the actual module name
E   ModuleNotFoundError: No module named 'your_actual_module_name'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.77s
2025-04-17 12:29:24,246 - INFO - Failed after all retries for test case 1
2025-04-17 12:29:24,247 - INFO - 
TEST CASE 2 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

def test_user_info_unique_email_constraint(setup_database):
    session = setup_database
    user1 = UserInfo(
        first_name="Jane",
        last_name="Doe",
        email="jane.doe@example.com",
        github_username="janedoe",
        status=1
    )
    user2 = UserInfo(
        first_name="Jane",
        last_name="Doe",
        email="jane.doe@example.com",
        github_username="janedoe2",
        status=1
    )
    session.add(user1)
    session.commit()
    session.add(user2)
    with pytest.raises(IntegrityError):
        session.commit()
    session.rollback()

---------------
2025-04-17 12:29:25,808 - INFO - TEST CASE 2 Retry 0 - Result - Failed
2025-04-17 12:29:25,808 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________ ERROR at setup of test_user_info_unique_email_constraint ___________
temp\temp.py:22: in setup_database
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_user_info_unique_email_constraint - NameError: name ...
1 warning, 1 error in 0.74s
2025-04-17 12:29:28,355 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:29:28,356 - INFO - 
TEST CASE 2 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine

# Assuming the database URL is defined somewhere
DATABASE_URL = "postgresql://user:password@localhost/test_db"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

@pytest.fixture(scope="function")
def setup_database():
    Base.metadata.create_all(bind=engine)
    session = SessionLocal()
    yield session
    session.close()
    Base.metadata.drop_all(bind=engine)

def test_user_info_unique_email_constraint(setup_database):
    session = setup_database
    user1 = UserInfo(
        first_name="Jane",
        last_name="Doe",
        email="jane.doe@example.com",
        github_username="janedoe",
        status=1
    )
    user2 = UserInfo(
        first_name="Jane",
        last_name="Doe",
        email="jane.doe@example.com",
        github_username="janedoe2",
        status=1
    )
    session.add(user1)
    session.commit()
    session.add(user2)
    with pytest.raises(IntegrityError):
        session.commit()
    session.rollback()

---------------
2025-04-17 12:29:30,134 - INFO - TEST CASE 2 Retry 1 - Result - Failed
2025-04-17 12:29:30,134 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:37: in <module>
    engine = create_engine(DATABASE_URL)
.venv\Lib\site-packages\sqlalchemy\util\deprecations.py:281: in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
.venv\Lib\site-packages\sqlalchemy\engine\create.py:602: in create_engine
    dbapi = dbapi_meth(**dbapi_args)
.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\psycopg2.py:696: in import_dbapi
    import psycopg2
E   ModuleNotFoundError: No module named 'psycopg2'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.96s
2025-04-17 12:29:33,419 - INFO - 
TEST CASE 2 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base

# Assuming the database URL is defined somewhere
DATABASE_URL = "postgresql://user:password@localhost/test_db"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

@pytest.fixture(scope="function")
def setup_database():
    Base.metadata.create_all(bind=engine)
    session = SessionLocal()
    yield session
    session.close()
    Base.metadata.drop_all(bind=engine)

def test_user_info_unique_email_constraint(setup_database):
    session = setup_database
    user1 = UserInfo(
        first_name="Jane",
        last_name="Doe",
        email="jane.doe@example.com",
        github_username="janedoe",
        status=1
    )
    user2 = UserInfo(
        first_name="Jane",
        last_name="Doe",
        email="jane.doe@example.com",
        github_username="janedoe2",
        status=1
    )
    session.add(user1)
    session.commit()
    session.add(user2)
    with pytest.raises(IntegrityError):
        session.commit()
    session.rollback()

---------------
2025-04-17 12:29:35,483 - INFO - TEST CASE 2 Retry 2 - Result - Failed
2025-04-17 12:29:35,484 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:38: in <module>
    engine = create_engine(DATABASE_URL)
.venv\Lib\site-packages\sqlalchemy\util\deprecations.py:281: in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
.venv\Lib\site-packages\sqlalchemy\engine\create.py:602: in create_engine
    dbapi = dbapi_meth(**dbapi_args)
.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\psycopg2.py:696: in import_dbapi
    import psycopg2
E   ModuleNotFoundError: No module named 'psycopg2'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.89s
2025-04-17 12:29:40,385 - INFO - Failed after all retries for test case 2
2025-04-17 12:29:40,385 - INFO - 
TEST CASE 3 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

def test_projects_creation(setup_database):
    session = setup_database
    project = Projects(
        repo_name="test_repo",
        problem_statement={"key": "value"},
        bloblink="http://example.com/blob",
        mini_project_flag=1
    )
    session.add(project)
    session.commit()
    assert project.id is not None
    assert project.repo_name == "test_repo"

---------------
2025-04-17 12:29:41,866 - INFO - TEST CASE 3 Retry 0 - Result - Failed
2025-04-17 12:29:41,866 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_projects_creation ___________________
temp\temp.py:22: in setup_database
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_projects_creation - NameError: name 'Base' is not de...
1 warning, 1 error in 0.70s
2025-04-17 12:29:43,622 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:29:43,623 - INFO - 
TEST CASE 3 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

@pytest.fixture(scope="function")
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_projects_creation(setup_database):
    session = setup_database
    project = Projects(
        repo_name="test_repo",
        problem_statement={"key": "value"},
        bloblink="http://example.com/blob",
        mini_project_flag=1
    )
    session.add(project)
    session.commit()
    assert project.id is not None
    assert project.repo_name == "test_repo"

---------------
2025-04-17 12:29:45,170 - INFO - TEST CASE 3 Retry 1 - Result - Failed
2025-04-17 12:29:45,170 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_projects_creation ___________________
temp\temp.py:36: in setup_database
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_projects_creation - NameError: name 'Base' is not de...
1 warning, 1 error in 0.73s
2025-04-17 12:29:47,528 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:29:47,529 - INFO - 
TEST CASE 3 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from your_module_name import Base, Projects  # Replace 'your_module_name' with the actual module name where Base and Projects are defined

@pytest.fixture(scope="function")
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_projects_creation(setup_database):
    session = setup_database
    project = Projects(
        repo_name="test_repo",
        problem_statement={"key": "value"},
        bloblink="http://example.com/blob",
        mini_project_flag=1
    )
    session.add(project)
    session.commit()
    assert project.id is not None
    assert project.repo_name == "test_repo"

---------------
2025-04-17 12:29:49,312 - INFO - TEST CASE 3 Retry 2 - Result - Failed
2025-04-17 12:29:49,313 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:32: in <module>
    from your_module_name import Base, Projects  # Replace 'your_module_name' with the actual module name where Base and Projects are defined
E   ModuleNotFoundError: No module named 'your_module_name'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.81s
2025-04-17 12:29:51,647 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:29:51,647 - INFO - Failed after all retries for test case 3
2025-04-17 12:29:51,648 - INFO - 
TEST CASE 4 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

def test_user_repo_unique_constraint(setup_database):
    session = setup_database
    user_repo1 = UserRepo(
        user_id=1,
        psid=1,
        github_username="uniqueuser",
        repo_name="uniquerepo",
        github_url="http://github.com/uniqueuser/uniquerepo"
    )
    user_repo2 = UserRepo(
        user_id=1,
        psid=2,
        github_username="uniqueuser",
        repo_name="uniquerepo",
        github_url="http://github.com/uniqueuser/uniquerepo2"
    )
    session.add(user_repo1)
    session.commit()
    session.add(user_repo2)
    with pytest.raises(IntegrityError):
        session.commit()
    session.rollback()

---------------
2025-04-17 12:29:53,388 - INFO - TEST CASE 4 Retry 0 - Result - Failed
2025-04-17 12:29:53,388 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_____________ ERROR at setup of test_user_repo_unique_constraint ______________
temp\temp.py:22: in setup_database
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_user_repo_unique_constraint - NameError: name 'Base'...
1 warning, 1 error in 0.82s
2025-04-17 12:29:56,957 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:29:56,957 - INFO - 
TEST CASE 4 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import IntegrityError

# Assuming the Base and UserRepo are imported from the source code
# from your_module import Base, UserRepo

@pytest.fixture(scope="function")
def setup_database():
    # Create an in-memory SQLite database for testing
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_user_repo_unique_constraint(setup_database):
    session = setup_database
    user_repo1 = UserRepo(
        user_id=1,
        psid=1,
        github_username="uniqueuser",
        repo_name="uniquerepo",
        github_url="http://github.com/uniqueuser/uniquerepo"
    )
    user_repo2 = UserRepo(
        user_id=1,
        psid=2,
        github_username="uniqueuser",
        repo_name="uniquerepo",
        github_url="http://github.com/uniqueuser/uniquerepo2"
    )
    session.add(user_repo1)
    session.commit()
    session.add(user_repo2)
    with pytest.raises(IntegrityError):
        session.commit()
    session.rollback()

---------------
2025-04-17 12:29:58,683 - INFO - TEST CASE 4 Retry 1 - Result - Failed
2025-04-17 12:29:58,684 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_____________ ERROR at setup of test_user_repo_unique_constraint ______________
temp\temp.py:41: in setup_database
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_user_repo_unique_constraint - NameError: name 'Base'...
1 warning, 1 error in 0.78s
2025-04-17 12:30:01,201 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:30:01,201 - INFO - 
TEST CASE 4 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import IntegrityError
from your_module import Base, UserRepo  # Ensure this import matches your actual module name

@pytest.fixture(scope="function")
def setup_database():
    # Create an in-memory SQLite database for testing
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_user_repo_unique_constraint(setup_database):
    session = setup_database
    user_repo1 = UserRepo(
        user_id=1,
        psid=1,
        github_username="uniqueuser",
        repo_name="uniquerepo",
        github_url="http://github.com/uniqueuser/uniquerepo"
    )
    user_repo2 = UserRepo(
        user_id=1,
        psid=2,
        github_username="uniqueuser",
        repo_name="uniquerepo",
        github_url="http://github.com/uniqueuser/uniquerepo2"
    )
    session.add(user_repo1)
    session.commit()
    session.add(user_repo2)
    with pytest.raises(IntegrityError):
        session.commit()
    session.rollback()

---------------
2025-04-17 12:30:02,500 - INFO - TEST CASE 4 Retry 2 - Result - Failed
2025-04-17 12:30:02,500 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:33: in <module>
    from your_module import Base, UserRepo  # Ensure this import matches your actual module name
E   ModuleNotFoundError: No module named 'your_module'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.57s
2025-04-17 12:30:06,505 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:30:06,505 - INFO - Failed after all retries for test case 4
2025-04-17 12:30:06,505 - INFO - 
TEST CASE 5 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

def test_curriculum_creation(setup_database):
    session = setup_database
    curriculum = Curriculum(
        question="What is Python?",
        marking_scheme="Correct if Python is described as a programming language.",
        model_answer="Python is a programming language."
    )
    session.add(curriculum)
    session.commit()
    assert curriculum.id is not None
    assert curriculum.question == "What is Python?"

---------------
2025-04-17 12:30:08,049 - INFO - TEST CASE 5 Retry 0 - Result - Failed
2025-04-17 12:30:08,050 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_________________ ERROR at setup of test_curriculum_creation __________________
temp\temp.py:22: in setup_database
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_curriculum_creation - NameError: name 'Base' is not ...
1 warning, 1 error in 0.81s
2025-04-17 12:30:10,752 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:30:10,752 - INFO - 
TEST CASE 5 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from your_module_name import Base, Curriculum  # Replace 'your_module_name' with the actual module name

@pytest.fixture(scope="function")
def setup_database():
    engine = create_engine('sqlite:///:memory:')  # Use an in-memory SQLite database for testing
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_curriculum_creation(setup_database):
    session = setup_database
    curriculum = Curriculum(
        question="What is Python?",
        marking_scheme="Correct if Python is described as a programming language.",
        model_answer="Python is a programming language."
    )
    session.add(curriculum)
    session.commit()
    assert curriculum.id is not None
    assert curriculum.question == "What is Python?"

---------------
2025-04-17 12:30:12,497 - INFO - TEST CASE 5 Retry 1 - Result - Failed
2025-04-17 12:30:12,497 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:32: in <module>
    from your_module_name import Base, Curriculum  # Replace 'your_module_name' with the actual module name
E   ModuleNotFoundError: No module named 'your_module_name'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.90s
2025-04-17 12:30:15,169 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:30:15,170 - INFO - 
TEST CASE 5 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from your_actual_module_name import Base, Curriculum  # Replace 'your_actual_module_name' with the actual module name

@pytest.fixture(scope="function")
def setup_database():
    engine = create_engine('sqlite:///:memory:')  # Use an in-memory SQLite database for testing
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_curriculum_creation(setup_database):
    session = setup_database
    curriculum = Curriculum(
        question="What is Python?",
        marking_scheme="Correct if Python is described as a programming language.",
        model_answer="Python is a programming language."
    )
    session.add(curriculum)
    session.commit()
    assert curriculum.id is not None
    assert curriculum.question == "What is Python?"

---------------
2025-04-17 12:30:16,958 - INFO - TEST CASE 5 Retry 2 - Result - Failed
2025-04-17 12:30:16,958 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:32: in <module>
    from your_actual_module_name import Base, Curriculum  # Replace 'your_actual_module_name' with the actual module name
E   ModuleNotFoundError: No module named 'your_actual_module_name'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.90s
2025-04-17 12:30:20,052 - INFO - Failed after all retries for test case 5
2025-04-17 12:30:20,053 - INFO - 
TEST CASE 6 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

def test_theory_eval_user_performance_creation(setup_database):
    session = setup_database
    performance = TheoryEvalUserPerformance(
        email="john.doe@example.com",
        question_id=uuid.uuid4(),
        user_response="Python is a snake.",
        llm_evaluation="Incorrect",
        llm_score=0.0,
        user_grade="F",
        user_attempts=1,
        llm_evaluation_status=0
    )
    session.add(performance)
    session.commit()
    assert performance.id is not None
    assert performance.user_grade == "F"

---------------
2025-04-17 12:30:21,515 - INFO - TEST CASE 6 Retry 0 - Result - Failed
2025-04-17 12:30:21,516 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
________ ERROR at setup of test_theory_eval_user_performance_creation _________
temp\temp.py:22: in setup_database
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_theory_eval_user_performance_creation - NameError: n...
1 warning, 1 error in 0.78s
2025-04-17 12:30:24,163 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:30:24,164 - INFO - 
TEST CASE 6 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from theory_evaluation.models import Base, TheoryEvalUserPerformance
import uuid

@pytest.fixture(scope="function")
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_theory_eval_user_performance_creation(setup_database):
    session = setup_database
    performance = TheoryEvalUserPerformance(
        email="john.doe@example.com",
        question_id=uuid.uuid4(),
        user_response="Python is a snake.",
        llm_evaluation="Incorrect",
        llm_score=0.0,
        user_grade="F",
        user_attempts=1,
        llm_evaluation_status=0
    )
    session.add(performance)
    session.commit()
    assert performance.id is not None
    assert performance.user_grade == "F"

---------------
2025-04-17 12:30:26,630 - INFO - TEST CASE 6 Retry 1 - Result - Failed
2025-04-17 12:30:26,630 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
________ ERROR at setup of test_theory_eval_user_performance_creation _________
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:137: in _compiler_dispatch
    meth = getter(visitor)
E   AttributeError: 'SQLiteTypeCompiler' object has no attribute 'visit_JSONB'. Did you mean: 'visit_JSON'?

The above exception was the direct cause of the following exception:
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:6716: in visit_create_table
    processed = self.process(
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:932: in process
    return obj._compiler_dispatch(self, **kwargs)
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:141: in _compiler_dispatch
    return meth(self, **kw)  # type: ignore  # noqa: E501
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:6747: in visit_create_column
    text = self.get_column_specification(column, first_pk=first_pk)
.venv\Lib\site-packages\sqlalchemy\dialects\sqlite\base.py:1598: in get_column_specification
    coltype = self.dialect.type_compiler_instance.process(
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:977: in process
    return type_._compiler_dispatch(self, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:139: in _compiler_dispatch
    return visitor.visit_unsupported_compilation(self, err, **kw)  # type: ignore  # noqa: E501
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:982: in visit_unsupported_compilation
    raise exc.UnsupportedCompilationError(self, element) from err
E   sqlalchemy.exc.UnsupportedCompilationError: Compiler <sqlalchemy.dialects.sqlite.base.SQLiteTypeCompiler object at 0x00000176FD4241A0> can't render element of type JSONB (Background on this error at: https://sqlalche.me/e/20/l7de)

The above exception was the direct cause of the following exception:
temp\temp.py:38: in setup_database
    Base.metadata.create_all(engine)
.venv\Lib\site-packages\sqlalchemy\sql\schema.py:5925: in create_all
    bind._run_ddl_visitor(
.venv\Lib\site-packages\sqlalchemy\engine\base.py:3249: in _run_ddl_visitor
    conn._run_ddl_visitor(visitorcallable, element, **kwargs)
.venv\Lib\site-packages\sqlalchemy\engine\base.py:2456: in _run_ddl_visitor
    visitorcallable(self.dialect, self, **kwargs).traverse_single(element)
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:664: in traverse_single
    return meth(obj, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:981: in visit_metadata
    self.traverse_single(
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:664: in traverse_single
    return meth(obj, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:1019: in visit_table
    )._invoke_with(self.connection)
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:321: in _invoke_with
    return bind.execute(self)
.venv\Lib\site-packages\sqlalchemy\engine\base.py:1416: in execute
    return meth(
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:187: in _execute_on_connection
    return connection._execute_ddl(
.venv\Lib\site-packages\sqlalchemy\engine\base.py:1524: in _execute_ddl
    compiled = ddl.compile(
.venv\Lib\site-packages\sqlalchemy\sql\elements.py:308: in compile
    return self._compiler(dialect, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:76: in _compiler
    return dialect.ddl_compiler(dialect, self, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:886: in __init__
    self.string = self.process(self.statement, **compile_kwargs)
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:932: in process
    return obj._compiler_dispatch(self, **kwargs)
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:141: in _compiler_dispatch
    return meth(self, **kw)  # type: ignore  # noqa: E501
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:6726: in visit_create_table
    raise exc.CompileError(
E   sqlalchemy.exc.CompileError: (in table 'projects', column 'problem_statement'): Compiler <sqlalchemy.dialects.sqlite.base.SQLiteTypeCompiler object at 0x00000176FD4241A0> can't render element of type JSONB
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_theory_eval_user_performance_creation - sqlalchemy.e...
1 warning, 1 error in 1.61s
2025-04-17 12:30:28,930 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:30:28,930 - INFO - 
TEST CASE 6 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from theory_evaluation.models import Base, TheoryEvalUserPerformance
import uuid

@pytest.fixture(scope="function")
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_theory_eval_user_performance_creation(setup_database):
    session = setup_database
    performance = TheoryEvalUserPerformance(
        email="john.doe@example.com",
        question_id=uuid.uuid4(),
        user_response="Python is a snake.",
        llm_evaluation="Incorrect",
        llm_score=0.0,
        user_grade="F",
        user_attempts=1,
        llm_evaluation_status=0
    )
    session.add(performance)
    session.commit()
    assert performance.id is not None
    assert performance.user_grade == "F"

---------------
2025-04-17 12:30:30,780 - INFO - TEST CASE 6 Retry 2 - Result - Failed
2025-04-17 12:30:30,781 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
________ ERROR at setup of test_theory_eval_user_performance_creation _________
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:137: in _compiler_dispatch
    meth = getter(visitor)
E   AttributeError: 'SQLiteTypeCompiler' object has no attribute 'visit_JSONB'. Did you mean: 'visit_JSON'?

The above exception was the direct cause of the following exception:
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:6716: in visit_create_table
    processed = self.process(
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:932: in process
    return obj._compiler_dispatch(self, **kwargs)
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:141: in _compiler_dispatch
    return meth(self, **kw)  # type: ignore  # noqa: E501
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:6747: in visit_create_column
    text = self.get_column_specification(column, first_pk=first_pk)
.venv\Lib\site-packages\sqlalchemy\dialects\sqlite\base.py:1598: in get_column_specification
    coltype = self.dialect.type_compiler_instance.process(
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:977: in process
    return type_._compiler_dispatch(self, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:139: in _compiler_dispatch
    return visitor.visit_unsupported_compilation(self, err, **kw)  # type: ignore  # noqa: E501
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:982: in visit_unsupported_compilation
    raise exc.UnsupportedCompilationError(self, element) from err
E   sqlalchemy.exc.UnsupportedCompilationError: Compiler <sqlalchemy.dialects.sqlite.base.SQLiteTypeCompiler object at 0x000002666C6E41A0> can't render element of type JSONB (Background on this error at: https://sqlalche.me/e/20/l7de)

The above exception was the direct cause of the following exception:
temp\temp.py:38: in setup_database
    Base.metadata.create_all(engine)
.venv\Lib\site-packages\sqlalchemy\sql\schema.py:5925: in create_all
    bind._run_ddl_visitor(
.venv\Lib\site-packages\sqlalchemy\engine\base.py:3249: in _run_ddl_visitor
    conn._run_ddl_visitor(visitorcallable, element, **kwargs)
.venv\Lib\site-packages\sqlalchemy\engine\base.py:2456: in _run_ddl_visitor
    visitorcallable(self.dialect, self, **kwargs).traverse_single(element)
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:664: in traverse_single
    return meth(obj, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:981: in visit_metadata
    self.traverse_single(
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:664: in traverse_single
    return meth(obj, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:1019: in visit_table
    )._invoke_with(self.connection)
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:321: in _invoke_with
    return bind.execute(self)
.venv\Lib\site-packages\sqlalchemy\engine\base.py:1416: in execute
    return meth(
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:187: in _execute_on_connection
    return connection._execute_ddl(
.venv\Lib\site-packages\sqlalchemy\engine\base.py:1524: in _execute_ddl
    compiled = ddl.compile(
.venv\Lib\site-packages\sqlalchemy\sql\elements.py:308: in compile
    return self._compiler(dialect, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\ddl.py:76: in _compiler
    return dialect.ddl_compiler(dialect, self, **kw)
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:886: in __init__
    self.string = self.process(self.statement, **compile_kwargs)
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:932: in process
    return obj._compiler_dispatch(self, **kwargs)
.venv\Lib\site-packages\sqlalchemy\sql\visitors.py:141: in _compiler_dispatch
    return meth(self, **kw)  # type: ignore  # noqa: E501
.venv\Lib\site-packages\sqlalchemy\sql\compiler.py:6726: in visit_create_table
    raise exc.CompileError(
E   sqlalchemy.exc.CompileError: (in table 'projects', column 'problem_statement'): Compiler <sqlalchemy.dialects.sqlite.base.SQLiteTypeCompiler object at 0x000002666C6E41A0> can't render element of type JSONB
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_theory_eval_user_performance_creation - sqlalchemy.e...
1 warning, 1 error in 1.04s
2025-04-17 12:30:34,132 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:30:34,133 - INFO - Failed after all retries for test case 6
2025-04-17 12:30:34,133 - INFO - Before Improvement
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)
2025-04-17 12:30:35,980 - INFO - After Improvement
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)
2025-04-17 12:30:37,503 - INFO - Error in generating improved test cases
Test case:
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def setup_database():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)
Test error:
c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 warning in 0.70s
2025-04-17 12:30:37,504 - INFO - run_each_pytest_function_individually complete
2025-04-17 12:30:37,504 - ERROR - Failed processing theory_evaluation\models.py: 'str' object has no attribute 'relative_to'
2025-04-17 12:30:37,505 - INFO - End Processing file: theory_evaluation\models.py

2025-04-17 12:30:37,505 - ERROR - Unhandled error: division by zero
