2025-04-17 12:33:13,013 - INFO - Loading environment variables start
2025-04-17 12:33:13,019 - INFO - Loading environment variables completes
2025-04-17 12:33:13,019 - INFO - Initialising of LLM start
2025-04-17 12:33:13,354 - INFO - Initialising of LLM completes
2025-04-17 12:33:13,354 - INFO - Getting python file starts
2025-04-17 12:33:13,355 - INFO - Getting python file completes
2025-04-17 12:33:13,355 - INFO - 
Start Processing file: theory_evaluation\circle_utils.py
2025-04-17 12:33:13,355 - INFO - Extraction of function and class start
2025-04-17 12:33:13,355 - INFO - extraction of function and class complete
2025-04-17 12:33:13,355 - INFO - Generate Unit Test Case starts
2025-04-17 12:33:13,355 - INFO - Extract unique import start
2025-04-17 12:33:14,789 - INFO - Extract unique import complete
2025-04-17 12:33:14,790 - INFO - Update relative import start
2025-04-17 12:33:14,791 - INFO - Update relative import complete
2025-04-17 12:33:19,110 - INFO - Generate Unit Test Case complete
2025-04-17 12:33:19,115 - INFO - run_each_pytest_function_individually start
2025-04-17 12:33:23,286 - INFO - Number of test case to process - 6
2025-04-17 12:33:23,286 - INFO - 
TEST CASE 1 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_area_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 3.0
    expected_area = math.pi * radius ** 2

---------------
2025-04-17 12:33:23,974 - INFO - TEST CASE 1 Retry 0 - Result - Passed
2025-04-17 12:33:23,975 - INFO - 
TEST CASE 2 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_area_raises_value_error_for_negative_radius():
    # Arrange
    radius = -1.0

---------------
2025-04-17 12:33:24,650 - INFO - TEST CASE 2 Retry 0 - Result - Passed
2025-04-17 12:33:24,650 - INFO - 
TEST CASE 3 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_area_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_area = 0.0

---------------
2025-04-17 12:33:25,174 - INFO - TEST CASE 3 Retry 0 - Result - Passed
2025-04-17 12:33:25,175 - INFO - 
TEST CASE 4 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_circumference_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 3.0
    expected_circumference = 2 * math.pi * radius

---------------
2025-04-17 12:33:25,903 - INFO - TEST CASE 4 Retry 0 - Result - Passed
2025-04-17 12:33:25,903 - INFO - 
TEST CASE 5 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_circumference_raises_value_error_for_negative_radius():
    # Arrange
    radius = -1.0

---------------
2025-04-17 12:33:26,723 - INFO - TEST CASE 5 Retry 0 - Result - Passed
2025-04-17 12:33:26,723 - INFO - 
TEST CASE 6 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_circumference_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_circumference = 0.0

---------------
2025-04-17 12:33:27,269 - INFO - TEST CASE 6 Retry 0 - Result - Passed
2025-04-17 12:33:27,270 - INFO - Before Improvement
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest


import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest


def test_circle_area_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 3.0
    expected_area = math.pi * radius ** 2

def test_circle_area_raises_value_error_for_negative_radius():
    # Arrange
    radius = -1.0

def test_circle_area_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_area = 0.0

def test_circle_circumference_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 3.0
    expected_circumference = 2 * math.pi * radius

def test_circle_circumference_raises_value_error_for_negative_radius():
    # Arrange
    radius = -1.0

def test_circle_circumference_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_circumference = 0.0

2025-04-17 12:33:30,115 - INFO - After Improvement
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest


def test_circle_area_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 3.0
    expected_area = math.pi * radius ** 2

def test_circle_area_raises_value_error_for_negative_radius():
    # Arrange
    radius = -1.0

def test_circle_area_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_area = 0.0

def test_circle_circumference_returns_correct_value_for_positive_radius():
    # Arrange
    radius = 3.0
    expected_circumference = 2 * math.pi * radius

def test_circle_circumference_raises_value_error_for_negative_radius():
    # Arrange
    radius = -1.0

def test_circle_circumference_returns_zero_for_zero_radius():
    # Arrange
    radius = 0.0
    expected_circumference = 0.0
2025-04-17 12:33:30,860 - INFO - Improvement of test cases processed successfully
2025-04-17 12:33:30,861 - INFO - run_each_pytest_function_individually complete
2025-04-17 12:33:30,861 - INFO - End Processing file: theory_evaluation\circle_utils.py

2025-04-17 12:33:30,861 - INFO - 
Start Processing file: theory_evaluation\llm_handler.py
2025-04-17 12:33:30,862 - INFO - Extraction of function and class start
2025-04-17 12:33:30,863 - INFO - extraction of function and class complete
2025-04-17 12:33:30,864 - INFO - Generate Unit Test Case starts
2025-04-17 12:33:30,864 - INFO - Extract unique import start
2025-04-17 12:33:31,834 - INFO - Extract unique import complete
2025-04-17 12:33:31,835 - INFO - Update relative import start
2025-04-17 12:33:31,835 - INFO - Update relative import complete
2025-04-17 12:33:46,772 - INFO - Generate Unit Test Case complete
2025-04-17 12:33:46,777 - INFO - run_each_pytest_function_individually start
2025-04-17 12:33:59,760 - INFO - Number of test case to process - 14
2025-04-17 12:33:59,761 - INFO - 
TEST CASE 1 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_default_values():
    llm = OpenAI_llm()
    assert llm.message == DEFAULT_MESSAGE
    assert llm.image_input is None
    assert llm.azure_endpoint == os.getenv("AZURE_OPENAI_ENDPOINT_SWEDEN")
    assert llm.api_version == os.getenv("AZURE_OPENAI_API_VERSION")
    assert llm.model_name == os.getenv("OPENAI_DEPLOYMENT_NAME")
    assert llm.max_retries == 3
    assert llm.output is None
    assert llm.mode == "text_generation"
    assert llm.config == DEFAULT_CONFIG
    assert llm.verbose is False

---------------
2025-04-17 12:34:02,700 - INFO - TEST CASE 1 Retry 0 - Result - Failed
2025-04-17 12:34:02,700 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_init_with_default_values ________________________
temp\temp.py:15: in test_init_with_default_values
    assert llm.message == DEFAULT_MESSAGE
E   NameError: name 'DEFAULT_MESSAGE' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_init_with_default_values - NameError: name 'DEFAULT...
1 failed in 1.87s
2025-04-17 12:34:04,763 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:34:04,763 - INFO - 
TEST CASE 1 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import os
from llm_handler import OpenAI_llm, DEFAULT_MESSAGE, DEFAULT_CONFIG

def test_init_with_default_values():
    llm = OpenAI_llm()
    assert llm.message == DEFAULT_MESSAGE
    assert llm.image_input is None
    assert llm.azure_endpoint == os.getenv("AZURE_OPENAI_ENDPOINT_SWEDEN")
    assert llm.api_version == os.getenv("AZURE_OPENAI_API_VERSION")
    assert llm.model_name == os.getenv("OPENAI_DEPLOYMENT_NAME")
    assert llm.max_retries == 3
    assert llm.output is None
    assert llm.mode == "text_generation"
    assert llm.config == DEFAULT_CONFIG
    assert llm.verbose is False

---------------
2025-04-17 12:34:06,686 - INFO - TEST CASE 1 Retry 1 - Result - Failed
2025-04-17 12:34:06,686 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:14: in <module>
    from llm_handler import OpenAI_llm, DEFAULT_MESSAGE, DEFAULT_CONFIG
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.02s
2025-04-17 12:34:08,788 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:34:08,788 - INFO - 
TEST CASE 1 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import os
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from llm_handler import OpenAI_llm, DEFAULT_MESSAGE, DEFAULT_CONFIG

def test_init_with_default_values():
    llm = OpenAI_llm()
    assert llm.message == DEFAULT_MESSAGE
    assert llm.image_input is None
    assert llm.azure_endpoint == os.getenv("AZURE_OPENAI_ENDPOINT_SWEDEN")
    assert llm.api_version == os.getenv("AZURE_OPENAI_API_VERSION")
    assert llm.model_name == os.getenv("OPENAI_DEPLOYMENT_NAME")
    assert llm.max_retries == 3
    assert llm.output is None
    assert llm.mode == "text_generation"
    assert llm.config == DEFAULT_CONFIG
    assert llm.verbose is False

---------------
2025-04-17 12:34:10,871 - INFO - TEST CASE 1 Retry 2 - Result - Failed
2025-04-17 12:34:10,872 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:16: in <module>
    from llm_handler import OpenAI_llm, DEFAULT_MESSAGE, DEFAULT_CONFIG
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.20s
2025-04-17 12:34:13,004 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:34:13,005 - INFO - Failed after all retries for test case 1
2025-04-17 12:34:13,005 - INFO - 
TEST CASE 2 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_custom_values():
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="custom_endpoint",
        message="Custom message",
        image_input="image_data",
        api_version="v1",
        model_name="custom_model",
        max_retries=5,
        output="json",
        mode="vision",
        config={"temperature": 0.5},
        verbose=True
    )
    assert llm.message == "Custom message"
    assert llm.image_input == "image_data"
    assert llm.azure_endpoint == "custom_endpoint"
    assert llm.api_version == "v1"
    assert llm.model_name == "custom_model"
    assert llm.max_retries == 5
    assert llm.output == "json"
    assert llm.mode == "vision"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True

---------------
2025-04-17 12:34:14,997 - INFO - TEST CASE 2 Retry 0 - Result - Failed
2025-04-17 12:34:14,997 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_init_with_custom_values _________________________
temp\temp.py:14: in test_init_with_custom_values
    llm = OpenAI_llm(
theory_evaluation\llm_handler.py:63: in __init__
    self.client = AzureOpenAI(
.venv\Lib\site-packages\openai\lib\azure.py:194: in __init__
    raise OpenAIError(
E   openai.OpenAIError: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.
=========================== short test summary info ===========================
FAILED temp/temp.py::test_init_with_custom_values - openai.OpenAIError: Missi...
1 failed in 1.16s
2025-04-17 12:34:18,371 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:34:18,372 - INFO - 
TEST CASE 2 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import os

def test_init_with_custom_values():
    # Set environment variables for the test
    os.environ["AZURE_OPENAI_API_KEY_SWEDEN"] = "your_api_key_here"
    os.environ["AZURE_OPENAI_ENDPOINT_SWEDEN"] = "custom_endpoint"
    os.environ["AZURE_OPENAI_API_VERSION"] = "v1"
    os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"] = "custom_model"

    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="custom_endpoint",
        message="Custom message",
        image_input="image_data",
        api_version="v1",
        model_name="custom_model",
        max_retries=5,
        output="json",
        mode="vision",
        config={"temperature": 0.5},
        verbose=True
    )
    assert llm.message == "Custom message"
    assert llm.image_input == "image_data"
    assert llm.azure_endpoint == "custom_endpoint"
    assert llm.api_version == "v1"
    assert llm.model_name == "custom_model"
    assert llm.max_retries == 5
    assert llm.output == "json"
    assert llm.mode == "vision"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True

---------------
2025-04-17 12:34:20,534 - INFO - TEST CASE 2 Retry 1 - Result - Passed
2025-04-17 12:34:20,535 - INFO - 
TEST CASE 3 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_invalid_message():
    with pytest.raises(AssertionError, match="Prompt message must be inserted."):
        OpenAI_llm(message="")

---------------
2025-04-17 12:34:22,142 - INFO - TEST CASE 3 Retry 0 - Result - Passed
2025-04-17 12:34:22,142 - INFO - 
TEST CASE 4 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_invalid_output():
    with pytest.raises(AssertionError, match="Output must be either 'json', 'stream', or None"):
        OpenAI_llm(output="invalid")

---------------
2025-04-17 12:34:23,662 - INFO - TEST CASE 4 Retry 0 - Result - Passed
2025-04-17 12:34:23,662 - INFO - 
TEST CASE 5 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_invalid_mode():
    with pytest.raises(AssertionError, match="mode must be either 'text_generation' or 'vision'"):
        OpenAI_llm(mode="invalid")

---------------
2025-04-17 12:34:25,195 - INFO - TEST CASE 5 Retry 0 - Result - Passed
2025-04-17 12:34:25,196 - INFO - 
TEST CASE 6 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

@pytest.mark.asyncio
async def test_OpenAI_JSON_Completion_success(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content='{"key": "value"}'))]
        result = await default_llm._OpenAI_JSON_Completion()
        assert result == {"key": "value"}

---------------
2025-04-17 12:34:27,350 - INFO - TEST CASE 6 Retry 0 - Result - Failed
2025-04-17 12:34:27,350 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________ test_OpenAI_JSON_Completion_success _____________________
temp\temp.py:15: in test_OpenAI_JSON_Completion_success
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_JSON_Completion_success - NameError: name 'p...
1 failed in 1.37s
2025-04-17 12:34:29,919 - INFO - 
TEST CASE 6 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_JSON_Completion_success(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content='{"key": "value"}'))]
        result = await default_llm._OpenAI_JSON_Completion()
        assert result == {"key": "value"}

---------------
2025-04-17 12:34:32,211 - INFO - TEST CASE 6 Retry 1 - Result - Failed
2025-04-17 12:34:32,211 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________ test_OpenAI_JSON_Completion_success _____________________
temp\temp.py:21: in test_OpenAI_JSON_Completion_success
    assert result == {"key": "value"}
E   AssertionError: assert None == {'key': 'value'}
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_JSON_Completion: 'coroutine' object has no attribute 'choices'
============================== warnings summary ===============================
temp/temp.py::test_OpenAI_JSON_Completion_success
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:20: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    result = await default_llm._OpenAI_JSON_Completion()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_JSON_Completion_success - AssertionError: as...
1 failed, 1 warning in 1.51s
2025-04-17 12:34:33,591 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:34:33,591 - INFO - 
TEST CASE 6 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_JSON_Completion_success(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content='{"key": "value"}'))]
        result = await default_llm._OpenAI_JSON_Completion()
        assert result == {"key": "value"}

---------------
2025-04-17 12:34:36,345 - INFO - TEST CASE 6 Retry 2 - Result - Failed
2025-04-17 12:34:36,346 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________ test_OpenAI_JSON_Completion_success _____________________
temp\temp.py:21: in test_OpenAI_JSON_Completion_success
    assert result == {"key": "value"}
E   AssertionError: assert None == {'key': 'value'}
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_JSON_Completion: 'coroutine' object has no attribute 'choices'
============================== warnings summary ===============================
temp/temp.py::test_OpenAI_JSON_Completion_success
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:20: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    result = await default_llm._OpenAI_JSON_Completion()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_JSON_Completion_success - AssertionError: as...
1 failed, 1 warning in 1.57s
2025-04-17 12:34:38,225 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:34:38,226 - INFO - Failed after all retries for test case 6
2025-04-17 12:34:38,226 - INFO - 
TEST CASE 7 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

@pytest.mark.asyncio
async def test_OpenAI_JSON_Completion_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = await default_llm._OpenAI_JSON_Completion()
        assert result is None

---------------
2025-04-17 12:34:42,269 - INFO - TEST CASE 7 Retry 0 - Result - Failed
2025-04-17 12:34:42,269 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________ test_OpenAI_JSON_Completion_failure _____________________
temp\temp.py:15: in test_OpenAI_JSON_Completion_failure
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_JSON_Completion_failure - NameError: name 'p...
1 failed in 2.75s
2025-04-17 12:34:43,391 - INFO - 
TEST CASE 7 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_JSON_Completion_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = await default_llm._OpenAI_JSON_Completion()
        assert result is None

---------------
2025-04-17 12:34:47,912 - INFO - TEST CASE 7 Retry 1 - Result - Passed
2025-04-17 12:34:47,912 - INFO - 
TEST CASE 8 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

@pytest.mark.asyncio
async def test_OpenAI_Streaming_success(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value = [{"choices": [{"delta": {"content": "chunk"}}]}]
        result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
        assert result == ["chunk"]

---------------
2025-04-17 12:34:52,151 - INFO - TEST CASE 8 Retry 0 - Result - Failed
2025-04-17 12:34:52,151 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_OpenAI_Streaming_success ________________________
temp\temp.py:15: in test_OpenAI_Streaming_success
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Streaming_success - NameError: name 'patch' ...
1 failed in 2.80s
2025-04-17 12:34:54,715 - INFO - 
TEST CASE 8 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_Streaming_success(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value = [{"choices": [{"delta": {"content": "chunk"}}]}]
        result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
        assert result == ["chunk"]

---------------
2025-04-17 12:35:00,357 - INFO - TEST CASE 8 Retry 1 - Result - Failed
2025-04-17 12:35:00,357 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_OpenAI_Streaming_success ________________________
temp\temp.py:21: in test_OpenAI_Streaming_success
    assert result == ["chunk"]
E   assert ['Failed in _...not iterable'] == ['chunk']
E     
E     At index 0 diff: "Failed in _OpenAI_Streaming: 'coroutine' object is not iterable" != 'chunk'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_OpenAI_Streaming_success
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:20: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Streaming_success - assert ['Failed in _...n...
1 failed, 1 warning in 3.48s
2025-04-17 12:35:04,097 - INFO - 
TEST CASE 8 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_Streaming_success(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value = AsyncMock()
        mock_create.return_value.__aiter__.return_value = [{"choices": [{"delta": {"content": "chunk"}}]}]
        result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
        assert result == ["chunk"]

---------------
2025-04-17 12:35:07,760 - INFO - TEST CASE 8 Retry 2 - Result - Failed
2025-04-17 12:35:07,761 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_OpenAI_Streaming_success ________________________
temp\temp.py:22: in test_OpenAI_Streaming_success
    assert result == ["chunk"]
E   assert ['Failed in _...not iterable'] == ['chunk']
E     
E     At index 0 diff: "Failed in _OpenAI_Streaming: 'coroutine' object is not iterable" != 'chunk'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_OpenAI_Streaming_success
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:21: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Streaming_success - assert ['Failed in _...n...
1 failed, 1 warning in 2.27s
2025-04-17 12:35:09,459 - INFO - Failed after all retries for test case 8
2025-04-17 12:35:09,460 - INFO - 
TEST CASE 9 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

@pytest.mark.asyncio
async def test_OpenAI_Streaming_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
        assert result == ["Failed in _OpenAI_Streaming: Test exception"]

---------------
2025-04-17 12:35:13,468 - INFO - TEST CASE 9 Retry 0 - Result - Failed
2025-04-17 12:35:13,469 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_OpenAI_Streaming_failure ________________________
temp\temp.py:15: in test_OpenAI_Streaming_failure
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Streaming_failure - NameError: name 'patch' ...
1 failed in 2.54s
2025-04-17 12:35:15,951 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:35:15,952 - INFO - 
TEST CASE 9 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_OpenAI_Streaming_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
        assert result == ["Failed in _OpenAI_Streaming: Test exception"]

---------------
2025-04-17 12:35:21,021 - INFO - TEST CASE 9 Retry 1 - Result - Failed
2025-04-17 12:35:21,021 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_OpenAI_Streaming_failure ________________________
temp\temp.py:21: in test_OpenAI_Streaming_failure
    assert result == ["Failed in _OpenAI_Streaming: Test exception"]
E   assert ['Failed in _...not iterable'] == ['Failed in _...st exception']
E     
E     At index 0 diff: "Failed in _OpenAI_Streaming: 'coroutine' object is not iterable" != 'Failed in _OpenAI_Streaming: Test exception'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_OpenAI_Streaming_failure
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:20: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Streaming_failure - assert ['Failed in _...n...
1 failed, 1 warning in 3.27s
2025-04-17 12:35:23,678 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:35:23,679 - INFO - 
TEST CASE 9 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

import pytest
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_OpenAI_Streaming_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
        assert result == ["Failed in _OpenAI_Streaming: Test exception"]

---------------
2025-04-17 12:35:28,044 - INFO - TEST CASE 9 Retry 2 - Result - Failed
2025-04-17 12:35:28,045 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_OpenAI_Streaming_failure ________________________
temp\temp.py:21: in test_OpenAI_Streaming_failure
    assert result == ["Failed in _OpenAI_Streaming: Test exception"]
E   assert ['Failed in _...not iterable'] == ['Failed in _...st exception']
E     
E     At index 0 diff: "Failed in _OpenAI_Streaming: 'coroutine' object is not iterable" != 'Failed in _OpenAI_Streaming: Test exception'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_OpenAI_Streaming_failure
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:20: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    result = [chunk async for chunk in default_llm._OpenAI_Streaming()]
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Streaming_failure - assert ['Failed in _...n...
1 failed, 1 warning in 2.86s
2025-04-17 12:35:29,763 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:35:29,764 - INFO - Failed after all retries for test case 9
2025-04-17 12:35:29,764 - INFO - 
TEST CASE 10 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

@pytest.mark.asyncio
async def test_OpenAI_Chat_Completion_success(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content="response"))]
        result = await default_llm._OpenAI_Chat_Completion()
        assert result == "response"

---------------
2025-04-17 12:35:33,648 - INFO - TEST CASE 10 Retry 0 - Result - Failed
2025-04-17 12:35:33,648 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________ test_OpenAI_Chat_Completion_success _____________________
temp\temp.py:15: in test_OpenAI_Chat_Completion_success
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Chat_Completion_success - NameError: name 'p...
1 failed in 2.70s
2025-04-17 12:35:35,124 - INFO - 
TEST CASE 10 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_Chat_Completion_success(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content="response"))]
        result = await default_llm._OpenAI_Chat_Completion()
        assert result == "response"

---------------
2025-04-17 12:35:40,484 - INFO - TEST CASE 10 Retry 1 - Result - Failed
2025-04-17 12:35:40,484 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________ test_OpenAI_Chat_Completion_success _____________________
temp\temp.py:21: in test_OpenAI_Chat_Completion_success
    assert result == "response"
E   AssertionError: assert None == 'response'
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_Chat_Completion: 'coroutine' object has no attribute 'choices'
============================== warnings summary ===============================
temp/temp.py::test_OpenAI_Chat_Completion_success
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:20: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    result = await default_llm._OpenAI_Chat_Completion()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Chat_Completion_success - AssertionError: as...
1 failed, 1 warning in 3.68s
2025-04-17 12:35:42,857 - INFO - 
TEST CASE 10 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_Chat_Completion_success(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content="response"))]
        result = await default_llm._OpenAI_Chat_Completion()
        assert result == "response"

---------------
2025-04-17 12:35:46,982 - INFO - TEST CASE 10 Retry 2 - Result - Failed
2025-04-17 12:35:46,982 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________ test_OpenAI_Chat_Completion_success _____________________
temp\temp.py:21: in test_OpenAI_Chat_Completion_success
    assert result == "response"
E   AssertionError: assert None == 'response'
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_Chat_Completion: 'coroutine' object has no attribute 'choices'
============================== warnings summary ===============================
temp/temp.py::test_OpenAI_Chat_Completion_success
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:20: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    result = await default_llm._OpenAI_Chat_Completion()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Chat_Completion_success - AssertionError: as...
1 failed, 1 warning in 2.85s
2025-04-17 12:35:48,431 - INFO - Failed after all retries for test case 10
2025-04-17 12:35:48,431 - INFO - 
TEST CASE 11 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

@pytest.mark.asyncio
async def test_OpenAI_Chat_Completion_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = await default_llm._OpenAI_Chat_Completion()
        assert result is None

---------------
2025-04-17 12:35:52,195 - INFO - TEST CASE 11 Retry 0 - Result - Failed
2025-04-17 12:35:52,195 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________ test_OpenAI_Chat_Completion_failure _____________________
temp\temp.py:15: in test_OpenAI_Chat_Completion_failure
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_OpenAI_Chat_Completion_failure - NameError: name 'p...
1 failed in 2.45s
2025-04-17 12:35:54,227 - INFO - 
TEST CASE 11 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_Chat_Completion_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = await default_llm._OpenAI_Chat_Completion()
        assert result is None

---------------
2025-04-17 12:35:59,276 - INFO - TEST CASE 11 Retry 1 - Result - Passed
2025-04-17 12:35:59,276 - INFO - 
TEST CASE 12 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

@pytest.mark.asyncio
async def test_execute_text_generation(default_llm):
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock(__aiter__=lambda s: iter(["response"]))
        result = [response async for response in default_llm.execute()]
        assert result == ["response"]

---------------
2025-04-17 12:36:03,301 - INFO - TEST CASE 12 Retry 0 - Result - Failed
2025-04-17 12:36:03,302 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_execute_text_generation _________________________
temp\temp.py:15: in test_execute_text_generation
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_text_generation - NameError: name 'patch' i...
1 failed in 2.85s
2025-04-17 12:36:05,431 - INFO - 
TEST CASE 12 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_execute_text_generation(default_llm):
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock(__aiter__=lambda s: iter(["response"]))
        result = [response async for response in default_llm.execute()]
        assert result == ["response"]

---------------
2025-04-17 12:36:09,704 - INFO - TEST CASE 12 Retry 1 - Result - Failed
2025-04-17 12:36:09,705 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_execute_text_generation _________________________
temp\temp.py:20: in test_execute_text_generation
    assert result == ["response"]
E   assert ["Error: 'asy...ot coroutine"] == ['response']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'response'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_text_generation
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_text_generation - assert ["Error: 'asy...ot...
1 failed, 1 warning in 2.81s
2025-04-17 12:36:11,204 - INFO - 
TEST CASE 12 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_execute_text_generation(default_llm):
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value.__aiter__.return_value = iter(["response"])
        result = [response async for response in default_llm.execute()]
        assert result == ["response"]

---------------
2025-04-17 12:36:15,061 - INFO - TEST CASE 12 Retry 2 - Result - Failed
2025-04-17 12:36:15,061 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_execute_text_generation _________________________
temp\temp.py:21: in test_execute_text_generation
    assert result == ["response"]
E   assert ["Error: 'asy...ot coroutine"] == ['response']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'response'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_text_generation
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_text_generation - assert ["Error: 'asy...ot...
1 failed, 1 warning in 2.46s
2025-04-17 12:36:16,498 - INFO - Failed after all retries for test case 12
2025-04-17 12:36:16,498 - INFO - 
TEST CASE 13 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

@pytest.mark.asyncio
async def test_execute_vision(default_llm):
    default_llm.mode = "vision"
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock(__aiter__=lambda s: iter(["response"]))
        result = [response async for response in default_llm.execute()]
        assert result == ["response"]

---------------
2025-04-17 12:36:20,298 - INFO - TEST CASE 13 Retry 0 - Result - Failed
2025-04-17 12:36:20,298 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________________ test_execute_vision _____________________________
temp\temp.py:16: in test_execute_vision
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_vision - NameError: name 'patch' is not def...
1 failed in 2.59s
2025-04-17 12:36:21,691 - INFO - 
TEST CASE 13 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_execute_vision(default_llm):
    default_llm.mode = "vision"
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock(__aiter__=lambda s: iter(["response"]))
        result = [response async for response in default_llm.execute()]
        assert result == ["response"]

---------------
2025-04-17 12:36:26,181 - INFO - TEST CASE 13 Retry 1 - Result - Failed
2025-04-17 12:36:26,182 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________________ test_execute_vision _____________________________
temp\temp.py:21: in test_execute_vision
    assert result == ["response"]
E   assert ["Error: 'asy...ot coroutine"] == ['response']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'response'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_vision
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_vision - assert ["Error: 'asy...ot coroutin...
1 failed, 1 warning in 3.10s
2025-04-17 12:36:28,006 - INFO - 
TEST CASE 13 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_execute_vision(default_llm):
    default_llm.mode = "vision"
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock(__aiter__=lambda s: iter(["response"]))
        result = [response async for response in default_llm.execute()]
        assert result == ["response"]

---------------
2025-04-17 12:36:32,037 - INFO - TEST CASE 13 Retry 2 - Result - Failed
2025-04-17 12:36:32,037 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________________ test_execute_vision _____________________________
temp\temp.py:22: in test_execute_vision
    assert result == ["response"]
E   assert ["Error: 'asy...ot coroutine"] == ['response']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'response'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_vision
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_vision - assert ["Error: 'asy...ot coroutin...
1 failed, 1 warning in 2.89s
2025-04-17 12:36:33,781 - INFO - Failed after all retries for test case 13
2025-04-17 12:36:33,782 - INFO - 
TEST CASE 14 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

@pytest.mark.asyncio
async def test_execute_failure(default_llm):
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
        mock_run.side_effect = Exception("Test exception")
        result = [response async for response in default_llm.execute()]
        assert result == ["Error: Test exception"]

---------------
2025-04-17 12:36:37,981 - INFO - TEST CASE 14 Retry 0 - Result - Failed
2025-04-17 12:36:37,981 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_execute_failure _____________________________
temp\temp.py:15: in test_execute_failure
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_failure - NameError: name 'patch' is not de...
1 failed in 2.90s
2025-04-17 12:36:39,446 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:36:39,447 - INFO - 
TEST CASE 14 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_execute_failure(default_llm):
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
        mock_run.side_effect = Exception("Test exception")
        result = [response async for response in default_llm.execute()]
        assert result == ["Error: Test exception"]

---------------
2025-04-17 12:36:43,832 - INFO - TEST CASE 14 Retry 1 - Result - Failed
2025-04-17 12:36:43,832 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_execute_failure _____________________________
temp\temp.py:21: in test_execute_failure
    assert result == ["Error: Test exception"]
E   assert ["Error: 'asy...ot coroutine"] == ['Error: Test exception']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'Error: Test exception'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_failure
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_failure - assert ["Error: 'asy...ot corouti...
1 failed, 1 warning in 2.85s
2025-04-17 12:36:45,436 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:36:45,436 - INFO - 
TEST CASE 14 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_execute_failure(default_llm):
    with patch.object(default_llm, '_run', new_callable=AsyncMock) as mock_run:
        mock_run.side_effect = Exception("Test exception")
        result = [response async for response in default_llm.execute()]
        assert result == ["Error: Test exception"]

---------------
2025-04-17 12:36:49,223 - INFO - TEST CASE 14 Retry 2 - Result - Failed
2025-04-17 12:36:49,224 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_execute_failure _____________________________
temp\temp.py:21: in test_execute_failure
    assert result == ["Error: Test exception"]
E   assert ["Error: 'asy...ot coroutine"] == ['Error: Test exception']
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != 'Error: Test exception'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_execute_failure
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_failure - assert ["Error: 'asy...ot corouti...
1 failed, 1 warning in 2.21s
2025-04-17 12:36:51,200 - INFO - Failed after all retries for test case 14
2025-04-17 12:36:51,200 - INFO - Before Improvement
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def default_llm():
    return OpenAI_llm()
import os

def test_init_with_custom_values():
    # Set environment variables for the test
    os.environ["AZURE_OPENAI_API_KEY_SWEDEN"] = "your_api_key_here"
    os.environ["AZURE_OPENAI_ENDPOINT_SWEDEN"] = "custom_endpoint"
    os.environ["AZURE_OPENAI_API_VERSION"] = "v1"
    os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"] = "custom_model"

    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="custom_endpoint",
        message="Custom message",
        image_input="image_data",
        api_version="v1",
        model_name="custom_model",
        max_retries=5,
        output="json",
        mode="vision",
        config={"temperature": 0.5},
        verbose=True
    )
    assert llm.message == "Custom message"
    assert llm.image_input == "image_data"
    assert llm.azure_endpoint == "custom_endpoint"
    assert llm.api_version == "v1"
    assert llm.model_name == "custom_model"
    assert llm.max_retries == 5
    assert llm.output == "json"
    assert llm.mode == "vision"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True

def test_init_with_invalid_message():
    with pytest.raises(AssertionError, match="Prompt message must be inserted."):
        OpenAI_llm(message="")

def test_init_with_invalid_output():
    with pytest.raises(AssertionError, match="Output must be either 'json', 'stream', or None"):
        OpenAI_llm(output="invalid")

def test_init_with_invalid_mode():
    with pytest.raises(AssertionError, match="mode must be either 'text_generation' or 'vision'"):
        OpenAI_llm(mode="invalid")

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_JSON_Completion_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = await default_llm._OpenAI_JSON_Completion()
        assert result is None

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_OpenAI_Chat_Completion_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = await default_llm._OpenAI_Chat_Completion()
        assert result is None

2025-04-17 12:36:57,092 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:36:57,093 - INFO - After Improvement
import os
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch, AsyncMock

@pytest.fixture
def default_llm():
    return OpenAI_llm()

def test_init_with_custom_values():
    os.environ["AZURE_OPENAI_API_KEY_SWEDEN"] = "your_api_key_here"
    os.environ["AZURE_OPENAI_ENDPOINT_SWEDEN"] = "custom_endpoint"
    os.environ["AZURE_OPENAI_API_VERSION"] = "v1"
    os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"] = "custom_model"

    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="custom_endpoint",
        message="Custom message",
        image_input="image_data",
        api_version="v1",
        model_name="custom_model",
        max_retries=5,
        output="json",
        mode="vision",
        config={"temperature": 0.5},
        verbose=True
    )
    assert llm.message == "Custom message"
    assert llm.image_input == "image_data"
    assert llm.azure_endpoint == "custom_endpoint"
    assert llm.api_version == "v1"
    assert llm.model_name == "custom_model"
    assert llm.max_retries == 5
    assert llm.output == "json"
    assert llm.mode == "vision"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True

def test_init_with_invalid_message():
    with pytest.raises(AssertionError, match="Prompt message must be inserted."):
        OpenAI_llm(message="")

def test_init_with_invalid_output():
    with pytest.raises(AssertionError, match="Output must be either 'json', 'stream', or None"):
        OpenAI_llm(output="invalid")

def test_init_with_invalid_mode():
    with pytest.raises(AssertionError, match="mode must be either 'text_generation' or 'vision'"):
        OpenAI_llm(mode="invalid")

@pytest.mark.asyncio
async def test_OpenAI_JSON_Completion_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = await default_llm._OpenAI_JSON_Completion()
        assert result is None

@pytest.mark.asyncio
async def test_OpenAI_Chat_Completion_failure(default_llm):
    with patch.object(default_llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.side_effect = Exception("Test exception")
        result = await default_llm._OpenAI_Chat_Completion()
        assert result is None
2025-04-17 12:37:03,637 - INFO - Improvement of test cases processed successfully
2025-04-17 12:37:03,638 - INFO - run_each_pytest_function_individually complete
2025-04-17 12:37:03,639 - ERROR - Failed processing theory_evaluation\llm_handler.py: 'str' object has no attribute 'relative_to'
2025-04-17 12:37:03,639 - INFO - End Processing file: theory_evaluation\llm_handler.py

2025-04-17 12:37:03,640 - INFO - 
Start Processing file: theory_evaluation\llm_utils.py
2025-04-17 12:37:03,641 - INFO - Extraction of function and class start
2025-04-17 12:37:03,641 - INFO - extraction of function and class complete
2025-04-17 12:37:03,642 - INFO - Generate Unit Test Case starts
2025-04-17 12:37:03,642 - INFO - Extract unique import start
2025-04-17 12:37:05,941 - INFO - Extract unique import complete
2025-04-17 12:37:05,942 - INFO - Update relative import start
2025-04-17 12:37:05,942 - INFO - Update relative import complete
2025-04-17 12:37:14,187 - INFO - Generate Unit Test Case complete
2025-04-17 12:37:14,192 - INFO - run_each_pytest_function_individually start
2025-04-17 12:37:22,610 - INFO - Number of test case to process - 5
2025-04-17 12:37:22,610 - INFO - 
TEST CASE 1 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_prompt_returns_correct_prompt_structure():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_content = "key1: value1\nkey2: value2"
    prompt_content = "This is a test prompt with {$key1} and {$key2}."
    expected_prompt = "This is a test prompt with value1 and value2."

---------------
2025-04-17 12:37:23,755 - INFO - TEST CASE 1 Retry 0 - Result - Passed
2025-04-17 12:37:23,755 - INFO - 
TEST CASE 2 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_prompt_handles_missing_placeholder():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_content = "key1: value1"
    prompt_content = "This is a test prompt with {$key1} and {$key2}."
    expected_prompt = "This is a test prompt with value1 and {$key2}."

---------------
2025-04-17 12:37:25,074 - INFO - TEST CASE 2 Retry 0 - Result - Passed
2025-04-17 12:37:25,075 - INFO - 
TEST CASE 3 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_prompt_handles_file_not_found():
    agent = "test_agent"
    with patch("builtins.open", side_effect=FileNotFoundError):
        with pytest.raises(FileNotFoundError):
            initialise_prompt(agent)

---------------
2025-04-17 12:37:26,469 - INFO - TEST CASE 3 Retry 0 - Result - Failed
2025-04-17 12:37:26,469 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________ test_initialise_prompt_handles_file_not_found ________________
temp\temp.py:11: in test_initialise_prompt_handles_file_not_found
    with patch("builtins.open", side_effect=FileNotFoundError):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_prompt_handles_file_not_found - NameErro...
1 failed in 0.33s
2025-04-17 12:37:27,482 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:37:27,483 - INFO - 
TEST CASE 3 Retry 1
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



from unittest.mock import patch
import pytest

def test_initialise_prompt_handles_file_not_found():
    agent = "test_agent"
    with patch("builtins.open", side_effect=FileNotFoundError):
        with pytest.raises(FileNotFoundError):
            initialise_prompt(agent)

---------------
2025-04-17 12:37:29,103 - INFO - TEST CASE 3 Retry 1 - Result - Failed
2025-04-17 12:37:29,103 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________ test_initialise_prompt_handles_file_not_found ________________
temp\temp.py:15: in test_initialise_prompt_handles_file_not_found
    with pytest.raises(FileNotFoundError):
E   Failed: DID NOT RAISE <class 'FileNotFoundError'>
---------------------------- Captured stdout call -----------------------------
: No configuration path to the prompt given.
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_prompt_handles_file_not_found - Failed: ...
1 failed in 0.31s
2025-04-17 12:37:30,266 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:37:30,266 - INFO - 
TEST CASE 3 Retry 2
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



from unittest.mock import patch
import pytest

def test_initialise_prompt_handles_file_not_found():
    agent = "test_agent"
    with patch("builtins.open", side_effect=FileNotFoundError):
        with pytest.raises(FileNotFoundError):
            initialise_prompt(agent)

---------------
2025-04-17 12:37:31,336 - INFO - TEST CASE 3 Retry 2 - Result - Failed
2025-04-17 12:37:31,336 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________ test_initialise_prompt_handles_file_not_found ________________
temp\temp.py:15: in test_initialise_prompt_handles_file_not_found
    with pytest.raises(FileNotFoundError):
E   Failed: DID NOT RAISE <class 'FileNotFoundError'>
---------------------------- Captured stdout call -----------------------------
: No configuration path to the prompt given.
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_prompt_handles_file_not_found - Failed: ...
1 failed in 0.19s
2025-04-17 12:37:32,225 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:37:32,225 - INFO - Failed after all retries for test case 3
2025-04-17 12:37:32,226 - INFO - 
TEST CASE 4 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_settings_returns_correct_settings():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    settings_content = "setting1: value1\nsetting2: value2"
    expected_settings = {"setting1": "value1", "setting2": "value2"}

---------------
2025-04-17 12:37:33,627 - INFO - TEST CASE 4 Retry 0 - Result - Passed
2025-04-17 12:37:33,627 - INFO - 
TEST CASE 5 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_settings_handles_file_not_found():
    agent = "test_agent"
    with patch("builtins.open", side_effect=FileNotFoundError):
        with pytest.raises(FileNotFoundError):
            initialise_settings(agent)

---------------
2025-04-17 12:37:35,078 - INFO - TEST CASE 5 Retry 0 - Result - Failed
2025-04-17 12:37:35,079 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________ test_initialise_settings_handles_file_not_found _______________
temp\temp.py:11: in test_initialise_settings_handles_file_not_found
    with patch("builtins.open", side_effect=FileNotFoundError):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_settings_handles_file_not_found - NameEr...
1 failed in 0.27s
2025-04-17 12:37:36,246 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:37:36,247 - INFO - 
TEST CASE 5 Retry 1
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



from unittest.mock import patch
import pytest

def test_initialise_settings_handles_file_not_found():
    agent = "test_agent"
    with patch("builtins.open", side_effect=FileNotFoundError):
        with pytest.raises(FileNotFoundError):
            initialise_settings(agent)

---------------
2025-04-17 12:37:38,195 - INFO - TEST CASE 5 Retry 1 - Result - Failed
2025-04-17 12:37:38,195 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________ test_initialise_settings_handles_file_not_found _______________
temp\temp.py:15: in test_initialise_settings_handles_file_not_found
    with pytest.raises(FileNotFoundError):
E   Failed: DID NOT RAISE <class 'FileNotFoundError'>
---------------------------- Captured stdout call -----------------------------
: No configuration path to the llm settings given.
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_settings_handles_file_not_found - Failed...
1 failed in 0.27s
2025-04-17 12:37:39,234 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:37:39,235 - INFO - 
TEST CASE 5 Retry 2
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



from unittest.mock import patch
import pytest

def test_initialise_settings_handles_file_not_found():
    agent = "test_agent"
    with patch("builtins.open", side_effect=FileNotFoundError):
        with pytest.raises(FileNotFoundError):
            initialise_settings(agent)

---------------
2025-04-17 12:37:40,375 - INFO - TEST CASE 5 Retry 2 - Result - Failed
2025-04-17 12:37:40,375 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________ test_initialise_settings_handles_file_not_found _______________
temp\temp.py:15: in test_initialise_settings_handles_file_not_found
    with pytest.raises(FileNotFoundError):
E   Failed: DID NOT RAISE <class 'FileNotFoundError'>
---------------------------- Captured stdout call -----------------------------
: No configuration path to the llm settings given.
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_settings_handles_file_not_found - Failed...
1 failed in 0.12s
2025-04-17 12:37:43,461 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:37:43,461 - INFO - Failed after all retries for test case 5
2025-04-17 12:37:43,462 - INFO - Before Improvement
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest


import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest


def test_initialise_prompt_returns_correct_prompt_structure():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_content = "key1: value1\nkey2: value2"
    prompt_content = "This is a test prompt with {$key1} and {$key2}."
    expected_prompt = "This is a test prompt with value1 and value2."

def test_initialise_prompt_handles_missing_placeholder():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_content = "key1: value1"
    prompt_content = "This is a test prompt with {$key1} and {$key2}."
    expected_prompt = "This is a test prompt with value1 and {$key2}."

def test_initialise_settings_returns_correct_settings():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    settings_content = "setting1: value1\nsetting2: value2"
    expected_settings = {"setting1": "value1", "setting2": "value2"}

2025-04-17 12:37:46,407 - INFO - After Improvement
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest


def test_initialise_prompt_returns_correct_prompt_structure():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_content = "key1: value1\nkey2: value2"
    prompt_content = "This is a test prompt with {$key1} and {$key2}."
    expected_prompt = "This is a test prompt with value1 and value2."

def test_initialise_prompt_handles_missing_placeholder():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_content = "key1: value1"
    prompt_content = "This is a test prompt with {$key1} and {$key2}."
    expected_prompt = "This is a test prompt with value1 and {$key2}."

def test_initialise_settings_returns_correct_settings():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    settings_content = "setting1: value1\nsetting2: value2"
    expected_settings = {"setting1": "value1", "setting2": "value2"}
2025-04-17 12:37:47,586 - INFO - Improvement of test cases processed successfully
2025-04-17 12:37:47,586 - INFO - run_each_pytest_function_individually complete
2025-04-17 12:37:47,586 - ERROR - Failed processing theory_evaluation\llm_utils.py: 'str' object has no attribute 'relative_to'
2025-04-17 12:37:47,586 - INFO - End Processing file: theory_evaluation\llm_utils.py

2025-04-17 12:37:47,586 - INFO - 
Start Processing file: theory_evaluation\models.py
2025-04-17 12:37:47,586 - INFO - Extraction of function and class start
2025-04-17 12:37:47,587 - INFO - extraction of function and class complete
2025-04-17 12:37:47,587 - INFO - Generate Unit Test Case starts
2025-04-17 12:37:47,587 - INFO - Extract unique import start
2025-04-17 12:37:48,785 - INFO - Extract unique import complete
2025-04-17 12:37:48,785 - INFO - Update relative import start
2025-04-17 12:37:48,786 - INFO - Update relative import complete
2025-04-17 12:38:01,712 - INFO - Generate Unit Test Case complete
2025-04-17 12:38:01,720 - INFO - run_each_pytest_function_individually start
2025-04-17 12:38:11,302 - INFO - Number of test case to process - 6
2025-04-17 12:38:11,303 - INFO - 
TEST CASE 1 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

def test_user_info_creation(session):
    # Arrange
    user = UserInfo(
        first_name="John",
        last_name="Doe",
        email="john.doe@example.com",
        github_username="johndoe",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )

---------------
2025-04-17 12:38:14,130 - INFO - TEST CASE 1 Retry 0 - Result - Failed
2025-04-17 12:38:14,130 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_user_info_creation __________________
temp\temp.py:22: in test_engine
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_user_info_creation - NameError: name 'Base' is not d...
1 warning, 1 error in 1.25s
2025-04-17 12:38:16,446 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:38:16,447 - INFO - 
TEST CASE 1 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from your_module import Base, UserInfo  # Replace 'your_module' with the actual module name

# Create an in-memory SQLite database for testing
engine = create_engine('sqlite:///:memory:')
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

@pytest.fixture
def session():
    session = Session()
    yield session
    session.close()

def test_user_info_creation(session):
    # Arrange
    user = UserInfo(
        first_name="John",
        last_name="Doe",
        email="john.doe@example.com",
        github_username="johndoe",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )
    
    # Act
    session.add(user)
    session.commit()
    
    # Assert
    retrieved_user = session.query(UserInfo).filter_by(email="john.doe@example.com").first()
    assert retrieved_user is not None
    assert retrieved_user.first_name == "John"
    assert retrieved_user.last_name == "Doe"
    assert retrieved_user.github_username == "johndoe"

---------------
2025-04-17 12:38:19,014 - INFO - TEST CASE 1 Retry 1 - Result - Failed
2025-04-17 12:38:19,014 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:39: in <module>
    from your_module import Base, UserInfo  # Replace 'your_module' with the actual module name
E   ModuleNotFoundError: No module named 'your_module'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 1.15s
2025-04-17 12:38:21,601 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:38:21,601 - INFO - 
TEST CASE 1 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from temp import Base, UserInfo  # Replace 'temp' with the actual module name if different

# Create an in-memory SQLite database for testing
engine = create_engine('sqlite:///:memory:')
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

@pytest.fixture
def session():
    session = Session()
    yield session
    session.close()

def test_user_info_creation(session):
    # Arrange
    user = UserInfo(
        first_name="John",
        last_name="Doe",
        email="john.doe@example.com",
        github_username="johndoe",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )
    
    # Act
    session.add(user)
    session.commit()
    
    # Assert
    retrieved_user = session.query(UserInfo).filter_by(email="john.doe@example.com").first()
    assert retrieved_user is not None
    assert retrieved_user.first_name == "John"
    assert retrieved_user.last_name == "Doe"
    assert retrieved_user.github_username == "johndoe"

---------------
2025-04-17 12:38:24,230 - INFO - TEST CASE 1 Retry 2 - Result - Failed
2025-04-17 12:38:24,231 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:39: in <module>
    from temp import Base, UserInfo  # Replace 'temp' with the actual module name if different
E   ImportError: cannot import name 'Base' from partially initialized module 'temp' (most likely due to a circular import) (C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py)
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 1.25s
2025-04-17 12:38:27,269 - INFO - Failed after all retries for test case 1
2025-04-17 12:38:27,270 - INFO - 
TEST CASE 2 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

def test_user_info_unique_email_constraint(session):
    # Arrange
    user1 = UserInfo(
        first_name="Jane",
        last_name="Doe",
        email="jane.doe@example.com",
        github_username="janedoe",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )
    user2 = UserInfo(
        first_name="Jane",
        last_name="Smith",
        email="jane.doe@example.com",
        github_username="janesmith",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )

---------------
2025-04-17 12:38:29,400 - INFO - TEST CASE 2 Retry 0 - Result - Failed
2025-04-17 12:38:29,400 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________ ERROR at setup of test_user_info_unique_email_constraint ___________
temp\temp.py:22: in test_engine
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_user_info_unique_email_constraint - NameError: name ...
1 warning, 1 error in 0.98s
2025-04-17 12:38:33,043 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:38:33,044 - INFO - 
TEST CASE 2 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import IntegrityError

# Assuming the Base and UserInfo are imported from the source code
# from your_module import Base, UserInfo

@pytest.fixture(scope="function")
def session():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_user_info_unique_email_constraint(session):
    # Arrange
    user1 = UserInfo(
        first_name="Jane",
        last_name="Doe",
        email="jane.doe@example.com",
        github_username="janedoe",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )
    user2 = UserInfo(
        first_name="Jane",
        last_name="Smith",
        email="jane.doe@example.com",
        github_username="janesmith",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )

    # Act
    session.add(user1)
    session.commit()

    # Assert
    session.add(user2)
    with pytest.raises(IntegrityError):
        session.commit()

---------------
2025-04-17 12:38:35,532 - INFO - TEST CASE 2 Retry 1 - Result - Failed
2025-04-17 12:38:35,533 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________ ERROR at setup of test_user_info_unique_email_constraint ___________
temp\temp.py:47: in session
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_user_info_unique_email_constraint - NameError: name ...
1 warning, 1 error in 1.18s
2025-04-17 12:38:38,815 - INFO - 
TEST CASE 2 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import IntegrityError
from your_module import Base, UserInfo  # Ensure this import matches your actual module path

@pytest.fixture(scope="function")
def session():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_user_info_unique_email_constraint(session):
    # Arrange
    user1 = UserInfo(
        first_name="Jane",
        last_name="Doe",
        email="jane.doe@example.com",
        github_username="janedoe",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )
    user2 = UserInfo(
        first_name="Jane",
        last_name="Smith",
        email="jane.doe@example.com",
        github_username="janesmith",
        payment_date=None,
        current_duration=0,
        course_duration=0,
        end_date=None,
        status=1
    )

    # Act
    session.add(user1)
    session.commit()

    # Assert
    session.add(user2)
    with pytest.raises(IntegrityError):
        session.commit()

---------------
2025-04-17 12:38:41,254 - INFO - TEST CASE 2 Retry 2 - Result - Failed
2025-04-17 12:38:41,254 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:40: in <module>
    from your_module import Base, UserInfo  # Ensure this import matches your actual module path
E   ModuleNotFoundError: No module named 'your_module'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 1.21s
2025-04-17 12:38:44,483 - INFO - Failed after all retries for test case 2
2025-04-17 12:38:44,483 - INFO - 
TEST CASE 3 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

def test_projects_creation(session):
    # Arrange
    project = Projects(
        repo_name="example_repo",
        problem_statement={"problem": "Solve X"},
        bloblink="http://example.com/blob",
        mini_project_flag=1
    )

---------------
2025-04-17 12:38:46,828 - INFO - TEST CASE 3 Retry 0 - Result - Failed
2025-04-17 12:38:46,829 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_projects_creation ___________________
temp\temp.py:22: in test_engine
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_projects_creation - NameError: name 'Base' is not de...
1 warning, 1 error in 1.29s
2025-04-17 12:38:48,507 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:38:48,508 - INFO - 
TEST CASE 3 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

def test_projects_creation(session):
    # Arrange
    project = Projects(
        repo_name="example_repo",
        problem_statement={"problem": "Solve X"},
        bloblink="http://example.com/blob",
        mini_project_flag=1
    )

    # Act
    session.add(project)
    session.commit()

    # Assert
    retrieved_project = session.query(Projects).filter_by(repo_name="example_repo").first()
    assert retrieved_project is not None
    assert retrieved_project.repo_name == "example_repo"
    assert retrieved_project.problem_statement == {"problem": "Solve X"}
    assert retrieved_project.bloblink == "http://example.com/blob"
    assert retrieved_project.mini_project_flag == 1

---------------
2025-04-17 12:38:51,035 - INFO - TEST CASE 3 Retry 1 - Result - Failed
2025-04-17 12:38:51,036 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_projects_creation ___________________
temp\temp.py:22: in test_engine
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_projects_creation - NameError: name 'Base' is not de...
1 warning, 1 error in 1.01s
2025-04-17 12:38:53,552 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:38:53,552 - INFO - 
TEST CASE 3 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

@pytest.fixture(scope="function")
def session():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_projects_creation(session):
    # Arrange
    project = Projects(
        repo_name="example_repo",
        problem_statement={"problem": "Solve X"},
        bloblink="http://example.com/blob",
        mini_project_flag=1
    )

    # Act
    session.add(project)
    session.commit()

    # Assert
    retrieved_project = session.query(Projects).filter_by(repo_name="example_repo").first()
    assert retrieved_project is not None
    assert retrieved_project.repo_name == "example_repo"
    assert retrieved_project.problem_statement == {"problem": "Solve X"}
    assert retrieved_project.bloblink == "http://example.com/blob"
    assert retrieved_project.mini_project_flag == 1

---------------
2025-04-17 12:38:56,113 - INFO - TEST CASE 3 Retry 2 - Result - Failed
2025-04-17 12:38:56,113 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
__________________ ERROR at setup of test_projects_creation ___________________
temp\temp.py:43: in session
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_projects_creation - NameError: name 'Base' is not de...
1 warning, 1 error in 1.06s
2025-04-17 12:38:59,019 - INFO - Failed after all retries for test case 3
2025-04-17 12:38:59,020 - INFO - 
TEST CASE 4 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

def test_user_repo_unique_constraint(session):
    # Arrange
    user_repo1 = UserRepo(
        user_id=1,
        psid=1,
        github_username="johndoe",
        repo_name="example_repo",
        github_url="http://github.com/johndoe/example_repo"
    )
    user_repo2 = UserRepo(
        user_id=2,
        psid=2,
        github_username="johndoe",
        repo_name="example_repo",
        github_url="http://github.com/johndoe/example_repo"
    )

---------------
2025-04-17 12:39:01,193 - INFO - TEST CASE 4 Retry 0 - Result - Failed
2025-04-17 12:39:01,194 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_____________ ERROR at setup of test_user_repo_unique_constraint ______________
temp\temp.py:22: in test_engine
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_user_repo_unique_constraint - NameError: name 'Base'...
1 warning, 1 error in 0.99s
2025-04-17 12:39:04,897 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:39:04,898 - INFO - 
TEST CASE 4 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import IntegrityError

# Assuming the Base and UserRepo are imported from the source code
# from your_module import Base, UserRepo

@pytest.fixture(scope="function")
def session():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_user_repo_unique_constraint(session):
    # Arrange
    user_repo1 = UserRepo(
        user_id=1,
        psid=1,
        github_username="johndoe",
        repo_name="example_repo",
        github_url="http://github.com/johndoe/example_repo"
    )
    user_repo2 = UserRepo(
        user_id=2,
        psid=2,
        github_username="johndoe",
        repo_name="example_repo",
        github_url="http://github.com/johndoe/example_repo"
    )

    # Act
    session.add(user_repo1)
    session.commit()

    # Assert
    session.add(user_repo2)
    with pytest.raises(IntegrityError):
        session.commit()

---------------
2025-04-17 12:39:07,276 - INFO - TEST CASE 4 Retry 1 - Result - Failed
2025-04-17 12:39:07,277 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_____________ ERROR at setup of test_user_repo_unique_constraint ______________
temp\temp.py:47: in session
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_user_repo_unique_constraint - NameError: name 'Base'...
1 warning, 1 error in 0.92s
2025-04-17 12:39:09,849 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:39:09,850 - INFO - 
TEST CASE 4 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import IntegrityError
from your_module import Base, UserRepo  # Ensure this import matches your actual module name

@pytest.fixture(scope="function")
def session():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_user_repo_unique_constraint(session):
    # Arrange
    user_repo1 = UserRepo(
        user_id=1,
        psid=1,
        github_username="johndoe",
        repo_name="example_repo",
        github_url="http://github.com/johndoe/example_repo"
    )
    user_repo2 = UserRepo(
        user_id=2,
        psid=2,
        github_username="johndoe",
        repo_name="example_repo",
        github_url="http://github.com/johndoe/example_repo"
    )

    # Act
    session.add(user_repo1)
    session.commit()

    # Assert
    session.add(user_repo2)
    with pytest.raises(IntegrityError):
        session.commit()

---------------
2025-04-17 12:39:12,493 - INFO - TEST CASE 4 Retry 2 - Result - Failed
2025-04-17 12:39:12,494 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:40: in <module>
    from your_module import Base, UserRepo  # Ensure this import matches your actual module name
E   ModuleNotFoundError: No module named 'your_module'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 1.07s
2025-04-17 12:39:16,747 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:39:16,748 - INFO - Failed after all retries for test case 4
2025-04-17 12:39:16,748 - INFO - 
TEST CASE 5 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

def test_curriculum_creation(session):
    # Arrange
    curriculum = Curriculum(
        question="What is Python?",
        marking_scheme="Detailed explanation required",
        model_answer="Python is a programming language."
    )

---------------
2025-04-17 12:39:18,680 - INFO - TEST CASE 5 Retry 0 - Result - Failed
2025-04-17 12:39:18,680 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_________________ ERROR at setup of test_curriculum_creation __________________
temp\temp.py:22: in test_engine
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_curriculum_creation - NameError: name 'Base' is not ...
1 warning, 1 error in 0.85s
2025-04-17 12:39:21,695 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:39:21,695 - INFO - 
TEST CASE 5 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from your_module import Base, Curriculum  # Adjust the import according to your module structure

@pytest.fixture(scope="function")
def session():
    engine = create_engine('sqlite:///:memory:')  # Use an in-memory SQLite database for testing
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_curriculum_creation(session):
    # Arrange
    curriculum = Curriculum(
        question="What is Python?",
        marking_scheme="Detailed explanation required",
        model_answer="Python is a programming language."
    )

    # Act
    session.add(curriculum)
    session.commit()

    # Assert
    retrieved_curriculum = session.query(Curriculum).filter_by(question="What is Python?").first()
    assert retrieved_curriculum is not None
    assert retrieved_curriculum.marking_scheme == "Detailed explanation required"
    assert retrieved_curriculum.model_answer == "Python is a programming language."

---------------
2025-04-17 12:39:23,714 - INFO - TEST CASE 5 Retry 1 - Result - Failed
2025-04-17 12:39:23,715 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:39: in <module>
    from your_module import Base, Curriculum  # Adjust the import according to your module structure
E   ModuleNotFoundError: No module named 'your_module'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.85s
2025-04-17 12:39:26,957 - INFO - 
TEST CASE 5 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from your_module import Base, Curriculum  # Ensure this import matches your actual module structure

@pytest.fixture(scope="function")
def session():
    engine = create_engine('sqlite:///:memory:')  # Use an in-memory SQLite database for testing
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()

def test_curriculum_creation(session):
    # Arrange
    curriculum = Curriculum(
        question="What is Python?",
        marking_scheme="Detailed explanation required",
        model_answer="Python is a programming language."
    )

    # Act
    session.add(curriculum)
    session.commit()

    # Assert
    retrieved_curriculum = session.query(Curriculum).filter_by(question="What is Python?").first()
    assert retrieved_curriculum is not None
    assert retrieved_curriculum.marking_scheme == "Detailed explanation required"
    assert retrieved_curriculum.model_answer == "Python is a programming language."

---------------
2025-04-17 12:39:29,017 - INFO - TEST CASE 5 Retry 2 - Result - Failed
2025-04-17 12:39:29,018 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:39: in <module>
    from your_module import Base, Curriculum  # Ensure this import matches your actual module structure
E   ModuleNotFoundError: No module named 'your_module'
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 warning, 1 error in 0.95s
2025-04-17 12:39:31,716 - INFO - Failed after all retries for test case 5
2025-04-17 12:39:31,716 - INFO - 
TEST CASE 6 Retry 0
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

def test_theory_eval_user_performance_creation(session):
    # Arrange
    performance = TheoryEvalUserPerformance(
        email="john.doe@example.com",
        question_id=uuid.uuid4(),
        user_response="Python is a programming language.",
        llm_evaluation="Correct",
        llm_score=95.0,
        user_grade="A",
        user_attempts=1,
        llm_evaluation_status=1
    )

---------------
2025-04-17 12:39:33,887 - INFO - TEST CASE 6 Retry 0 - Result - Failed
2025-04-17 12:39:33,887 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
________ ERROR at setup of test_theory_eval_user_performance_creation _________
temp\temp.py:22: in test_engine
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_theory_eval_user_performance_creation - NameError: n...
1 warning, 1 error in 0.78s
2025-04-17 12:39:36,128 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:39:36,129 - INFO - 
TEST CASE 6 Retry 1
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

def test_theory_eval_user_performance_creation(session, engine):
    # Arrange
    Base.metadata.create_all(engine)
    performance = TheoryEvalUserPerformance(
        email="john.doe@example.com",
        question_id=uuid.uuid4(),
        user_response="Python is a programming language.",
        llm_evaluation="Correct",
        llm_score=95.0,
        user_grade="A",
        user_attempts=1,
        llm_evaluation_status=1
    )
    session.add(performance)
    session.commit()

    # Act
    result = session.query(TheoryEvalUserPerformance).filter_by(email="john.doe@example.com").first()

    # Assert
    assert result is not None
    assert result.email == "john.doe@example.com"
    assert result.llm_evaluation == "Correct"
    assert result.llm_score == 95.0
    assert result.user_grade == "A"
    assert result.user_attempts == 1
    assert result.llm_evaluation_status == 1

---------------
2025-04-17 12:39:38,297 - INFO - TEST CASE 6 Retry 1 - Result - Failed
2025-04-17 12:39:38,297 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
________ ERROR at setup of test_theory_eval_user_performance_creation _________
temp\temp.py:22: in test_engine
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR temp/temp.py::test_theory_eval_user_performance_creation - NameError: n...
1 warning, 1 error in 0.89s
2025-04-17 12:39:41,798 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:39:41,800 - INFO - 
TEST CASE 6 Retry 2
---------------
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()

from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine
import uuid

# Assuming you have a database URL
DATABASE_URL = "sqlite:///:memory:"  # Example for an in-memory SQLite database

# Create an engine
engine = create_engine(DATABASE_URL)

# Create a configured "Session" class
Session = sessionmaker(bind=engine)

# Create a Session
session = Session()

def test_theory_eval_user_performance_creation():
    # Arrange
    Base.metadata.create_all(engine)
    performance = TheoryEvalUserPerformance(
        email="john.doe@example.com",
        question_id=uuid.uuid4(),
        user_response="Python is a programming language.",
        llm_evaluation="Correct",
        llm_score=95.0,
        user_grade="A",
        user_attempts=1,
        llm_evaluation_status=1
    )
    session.add(performance)
    session.commit()

    # Act
    result = session.query(TheoryEvalUserPerformance).filter_by(email="john.doe@example.com").first()

    # Assert
    assert result is not None
    assert result.email == "john.doe@example.com"
    assert result.llm_evaluation == "Correct"
    assert result.llm_score == 95.0
    assert result.user_grade == "A"
    assert result.user_attempts == 1
    assert result.llm_evaluation_status == 1

---------------
2025-04-17 12:39:43,840 - INFO - TEST CASE 6 Retry 2 - Result - Failed
2025-04-17 12:39:43,840 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________ test_theory_eval_user_performance_creation __________________
temp\temp.py:54: in test_theory_eval_user_performance_creation
    Base.metadata.create_all(engine)
E   NameError: name 'Base' is not defined
============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_theory_eval_user_performance_creation - NameError: ...
1 failed, 1 warning in 0.88s
2025-04-17 12:39:47,064 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 12:39:47,065 - INFO - Failed after all retries for test case 6
2025-04-17 12:39:47,065 - INFO - Before Improvement
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()
2025-04-17 12:39:50,053 - INFO - After Improvement
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()
2025-04-17 12:39:52,184 - INFO - Error in generating improved test cases
Test case:
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy import (
    Column,
    Integer,
    String,
    TIMESTAMP,
    create_engine,
    Float,
    ForeignKey,
    Text,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid
from theory_evaluation.models import ConsultantChat, CurrentUserTable, Curriculum, MentorChat, Projects, SprintIssues, TheoryEvalUserPerformance, UserInfo, UserRepo, UserScoreLog
import pytest

@pytest.fixture(scope='module')
def test_engine():
    engine = create_engine('sqlite:///:memory:')
    Base.metadata.create_all(engine)
    return engine

@pytest.fixture(scope='function')
def session(test_engine):
    connection = test_engine.connect()
    transaction = connection.begin()
    Session = sessionmaker(bind=connection)
    session = Session()
    yield session
    session.close()
    transaction.rollback()
    connection.close()
Test error:
c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

============================== warnings summary ===============================
theory_evaluation\models.py:17
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\models.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 warning in 0.92s
2025-04-17 12:39:52,184 - INFO - run_each_pytest_function_individually complete
2025-04-17 12:39:52,185 - ERROR - Failed processing theory_evaluation\models.py: 'str' object has no attribute 'relative_to'
2025-04-17 12:39:52,185 - INFO - End Processing file: theory_evaluation\models.py

2025-04-17 12:39:52,189 - INFO - 
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
|    | filename                          |   total_test_cases_passed |   total_test_cases |   percentage_passed (%) |
+====+===================================+===========================+====================+=========================+
|  1 | theory_evaluation\circle_utils.py |                         6 |                  6 |                 100     |
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
|  2 | theory_evaluation\llm_handler.py  |                        14 |                  6 |                 233.333 |
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
|  3 | theory_evaluation\llm_utils.py    |                         5 |                  3 |                 166.667 |
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
|  4 | theory_evaluation\models.py       |                         6 |                  0 |                   0     |
+----+-----------------------------------+---------------------------+--------------------+-------------------------+
