2025-04-15 08:38:50,611 - INFO - Loading environment variables...
2025-04-15 08:38:50,980 - INFO - [1mStart Processing file: theory_evaluation\llm_handler.py[0m
2025-04-15 08:39:00,970 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 08:39:00,976 - INFO - 

2025-04-15 08:39:00,977 - INFO - TEST CASE 1 Retry 0
2025-04-15 08:39:00,977 - INFO - ---------------
2025-04-15 08:39:00,977 - INFO - def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
        assert llm.azure_endpoint == 'mock_AZURE_OPENAI_ENDPOINT_SWEDEN'
        assert llm.api_version == 'mock_AZURE_OPENAI_API_VERSION'
        assert llm.model_name == 'mock_AZURE_OPENAI_DEPLOYMENT_NAME'
2025-04-15 08:39:00,980 - INFO - ---------------
2025-04-15 08:39:03,181 - INFO - Test Result 1- False
2025-04-15 08:39:03,181 - INFO - Test Error 1 - F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:8: in test_openai_llm_initialization
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - NameError: name 'patch'...
1 failed in 1.32s
2025-04-15 08:39:03,181 - INFO - TEST CASE 1 Retry 1
2025-04-15 08:39:03,181 - INFO - ---------------
2025-04-15 08:39:03,181 - INFO - def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
        assert llm.azure_endpoint == 'mock_AZURE_OPENAI_ENDPOINT_SWEDEN'
        assert llm.api_version == 'mock_AZURE_OPENAI_API_VERSION'
        assert llm.model_name == 'mock_AZURE_OPENAI_DEPLOYMENT_NAME'
2025-04-15 08:39:03,181 - INFO - ---------------
2025-04-15 08:39:05,459 - INFO - proposed_test_case 2-
from unittest.mock import patch

def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
        assert llm.azure_endpoint == 'mock_AZURE_OPENAI_ENDPOINT_SWEDEN'
        assert llm.api_version == 'mock_AZURE_OPENAI_API_VERSION'
        assert llm.model_name == 'mock_AZURE_OPENAI_DEPLOYMENT_NAME'
2025-04-15 08:39:07,511 - INFO - Test Result 2- False
2025-04-15 08:39:07,511 - INFO - Test Error 2 - F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:14: in test_openai_llm_initialization
    llm = OpenAI_llm(useAzureOpenAI=True)
E   NameError: name 'OpenAI_llm' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - NameError: name 'OpenAI...
1 failed in 1.33s
2025-04-15 08:39:07,511 - INFO - TEST CASE 1 Retry 2
2025-04-15 08:39:07,511 - INFO - ---------------
2025-04-15 08:39:07,511 - INFO - def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
        assert llm.azure_endpoint == 'mock_AZURE_OPENAI_ENDPOINT_SWEDEN'
        assert llm.api_version == 'mock_AZURE_OPENAI_API_VERSION'
        assert llm.model_name == 'mock_AZURE_OPENAI_DEPLOYMENT_NAME'
2025-04-15 08:39:07,511 - INFO - ---------------
2025-04-15 08:39:10,323 - INFO - proposed_test_case 3-
from unittest.mock import patch
from theory_evaluation.llm_handler import OpenAI_llm

def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
        assert llm.azure_endpoint == 'mock_AZURE_OPENAI_ENDPOINT_SWEDEN'
        assert llm.api_version == 'mock_AZURE_OPENAI_API_VERSION'
        assert llm.model_name == 'mock_AZURE_OPENAI_DEPLOYMENT_NAME'
2025-04-15 08:39:11,963 - INFO - Test Result 3- True
2025-04-15 08:39:11,963 - INFO - Test Error 3 - .                                                                        [100%]
1 passed in 0.97s
2025-04-15 08:39:11,963 - INFO - 

2025-04-15 08:39:11,963 - INFO - TEST CASE 2 Retry 0
2025-04-15 08:39:11,963 - INFO - ---------------
2025-04-15 08:39:11,963 - INFO - def test_openai_json_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:39:11,963 - INFO - ---------------
2025-04-15 08:39:14,110 - INFO - Test Result 1- False
2025-04-15 08:39:14,110 - INFO - Test Error 1 - F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_json_completion _________________________
temp\temp.py:8: in test_openai_json_completion
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_json_completion - NameError: name 'patch' is...
1 failed in 1.37s
2025-04-15 08:39:14,110 - INFO - TEST CASE 2 Retry 1
2025-04-15 08:39:14,110 - INFO - ---------------
2025-04-15 08:39:14,110 - INFO - def test_openai_json_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:39:14,110 - INFO - ---------------
2025-04-15 08:39:15,505 - INFO - proposed_test_case 2-
from unittest.mock import patch, AsyncMock
import json

def test_openai_json_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:39:17,595 - INFO - Test Result 2- False
2025-04-15 08:39:17,595 - INFO - Test Error 2 - F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_json_completion _________________________
temp\temp.py:11: in test_openai_json_completion
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_json_completion - AttributeError: <class 'th...
1 failed in 1.37s
2025-04-15 08:39:17,595 - INFO - TEST CASE 2 Retry 2
2025-04-15 08:39:17,595 - INFO - ---------------
2025-04-15 08:39:17,595 - INFO - def test_openai_json_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:39:17,595 - INFO - ---------------
2025-04-15 08:39:19,695 - INFO - proposed_test_case 3-
import unittest
from unittest.mock import patch, AsyncMock
import json
from theory_evaluation.llm_handler import OpenAI_llm

def test_openai_json_completion():
    with patch.object(OpenAI_llm, 'client', create=True) as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = json.dumps({"answer": "42", "explanation": "The answer to everything."})
        mock_client.chat.completions.create.return_value = mock_response

        llm = OpenAI_llm()
        result = asyncio.run(llm._OpenAI_JSON_Completion())
        assert result == {"answer": "42", "explanation": "The answer to everything."}
2025-04-15 08:39:22,346 - INFO - Test Result 3- False
2025-04-15 08:39:22,346 - INFO - Test Error 3 - F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_json_completion _________________________
temp\temp.py:21: in test_openai_json_completion
    assert result == {"answer": "42", "explanation": "The answer to everything."}
E   AssertionError: assert None == {'answer': '42', 'explanation': 'The answer to everything.'}
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_JSON_Completion: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_json_completion - AssertionError: assert Non...
1 failed in 1.72s
2025-04-15 08:39:22,346 - INFO - Failed after all retries for test case 2
2025-04-15 08:39:22,346 - INFO - 

2025-04-15 08:39:22,346 - INFO - TEST CASE 3 Retry 0
2025-04-15 08:39:22,346 - INFO - ---------------
2025-04-15 08:39:22,346 - INFO - def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_stream = AsyncMock()
        mock_chunk = AsyncMock()
        mock_chunk.choices = [AsyncMock()]
        mock_chunk.choices[0].delta.content = "streamed content"
        mock_stream.__aiter__.return_value = [mock_chunk]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:39:22,346 - INFO - ---------------
2025-04-15 08:39:24,405 - INFO - Test Result 1- False
2025-04-15 08:39:24,405 - INFO - Test Error 1 - F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_openai_streaming ____________________________
temp\temp.py:8: in test_openai_streaming
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_streaming - NameError: name 'patch' is not d...
1 failed in 1.28s
2025-04-15 08:39:24,405 - INFO - TEST CASE 3 Retry 1
2025-04-15 08:39:24,405 - INFO - ---------------
2025-04-15 08:39:24,405 - INFO - def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_stream = AsyncMock()
        mock_chunk = AsyncMock()
        mock_chunk.choices = [AsyncMock()]
        mock_chunk.choices[0].delta.content = "streamed content"
        mock_stream.__aiter__.return_value = [mock_chunk]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:39:24,405 - INFO - ---------------
2025-04-15 08:39:25,855 - INFO - proposed_test_case 2-
from unittest.mock import patch, AsyncMock

def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_stream = AsyncMock()
        mock_chunk = AsyncMock()
        mock_chunk.choices = [AsyncMock()]
        mock_chunk.choices[0].delta.content = "streamed content"
        mock_stream.__aiter__.return_value = [mock_chunk]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:39:27,733 - INFO - Test Result 2- False
2025-04-15 08:39:27,733 - INFO - Test Error 2 - F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_openai_streaming ____________________________
temp\temp.py:10: in test_openai_streaming
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_streaming - AttributeError: <class 'theory_e...
1 failed in 1.13s
2025-04-15 08:39:27,733 - INFO - TEST CASE 3 Retry 2
2025-04-15 08:39:27,733 - INFO - ---------------
2025-04-15 08:39:27,733 - INFO - def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_stream = AsyncMock()
        mock_chunk = AsyncMock()
        mock_chunk.choices = [AsyncMock()]
        mock_chunk.choices[0].delta.content = "streamed content"
        mock_stream.__aiter__.return_value = [mock_chunk]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:39:27,733 - INFO - ---------------
2025-04-15 08:39:29,232 - INFO - proposed_test_case 3-
from unittest.mock import patch, AsyncMock

def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_stream = AsyncMock()
        mock_chunk = AsyncMock()
        mock_chunk.choices = [AsyncMock()]
        mock_chunk.choices[0].delta.content = "streamed content"
        mock_stream.__aiter__.return_value = [mock_chunk]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:39:31,182 - INFO - Test Result 3- False
2025-04-15 08:39:31,183 - INFO - Test Error 3 - F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_openai_streaming ____________________________
temp\temp.py:10: in test_openai_streaming
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_streaming - AttributeError: <class 'theory_e...
1 failed in 1.30s
2025-04-15 08:39:31,183 - INFO - Failed after all retries for test case 3
2025-04-15 08:39:31,183 - INFO - 

2025-04-15 08:39:31,183 - INFO - TEST CASE 4 Retry 0
2025-04-15 08:39:31,183 - INFO - ---------------
2025-04-15 08:39:31,183 - INFO - def test_openai_chat_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = "chat completion content"
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:39:31,183 - INFO - ---------------
2025-04-15 08:39:32,872 - INFO - Test Result 1- False
2025-04-15 08:39:32,872 - INFO - Test Error 1 - F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_chat_completion _________________________
temp\temp.py:8: in test_openai_chat_completion
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_chat_completion - NameError: name 'patch' is...
1 failed in 1.02s
2025-04-15 08:39:32,872 - INFO - TEST CASE 4 Retry 1
2025-04-15 08:39:32,873 - INFO - ---------------
2025-04-15 08:39:32,873 - INFO - def test_openai_chat_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = "chat completion content"
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:39:32,873 - INFO - ---------------
2025-04-15 08:39:34,361 - INFO - proposed_test_case 2-
from unittest.mock import patch, AsyncMock

def test_openai_chat_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = "chat completion content"
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:39:36,589 - INFO - Test Result 2- False
2025-04-15 08:39:36,590 - INFO - Test Error 2 - F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_chat_completion _________________________
temp\temp.py:10: in test_openai_chat_completion
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_chat_completion - AttributeError: <class 'th...
1 failed in 1.37s
2025-04-15 08:39:36,590 - INFO - TEST CASE 4 Retry 2
2025-04-15 08:39:36,590 - INFO - ---------------
2025-04-15 08:39:36,590 - INFO - def test_openai_chat_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client') as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = "chat completion content"
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:39:36,590 - INFO - ---------------
2025-04-15 08:39:38,119 - INFO - proposed_test_case 3-
from unittest.mock import patch, AsyncMock
from theory_evaluation.llm_handler import OpenAI_llm

def test_openai_chat_completion():
    with patch.object(OpenAI_llm, 'client', create=True) as mock_client:
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock()]
        mock_response.choices[0].message.content = "chat completion content"
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:39:40,642 - INFO - Test Result 3- True
2025-04-15 08:39:40,643 - INFO - Test Error 3 - .                                                                        [100%]
1 passed in 1.64s
2025-04-15 08:39:40,643 - INFO - 

2025-04-15 08:39:40,643 - INFO - TEST CASE 5 Retry 0
2025-04-15 08:39:40,643 - INFO - ---------------
2025-04-15 08:39:40,643 - INFO - def test_execute_text_generation():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value.__aiter__.return_value = ["response content"]
2025-04-15 08:39:40,643 - INFO - ---------------
2025-04-15 08:39:42,907 - INFO - Test Result 1- False
2025-04-15 08:39:42,907 - INFO - Test Error 1 - F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_execute_text_generation _________________________
temp\temp.py:8: in test_execute_text_generation
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_text_generation - NameError: name 'patch' i...
1 failed in 1.37s
2025-04-15 08:39:42,907 - INFO - TEST CASE 5 Retry 1
2025-04-15 08:39:42,907 - INFO - ---------------
2025-04-15 08:39:42,907 - INFO - def test_execute_text_generation():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value.__aiter__.return_value = ["response content"]
2025-04-15 08:39:42,907 - INFO - ---------------
2025-04-15 08:39:44,154 - INFO - proposed_test_case 2-
from unittest.mock import patch, AsyncMock

def test_execute_text_generation():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value.__aiter__.return_value = ["response content"]
2025-04-15 08:39:45,960 - INFO - Test Result 2- True
2025-04-15 08:39:45,960 - INFO - Test Error 2 - .                                                                        [100%]
1 passed in 1.03s
2025-04-15 08:39:45,960 - INFO - 

2025-04-15 08:39:45,960 - INFO - TEST CASE 6 Retry 0
2025-04-15 08:39:45,960 - INFO - ---------------
2025-04-15 08:39:45,960 - INFO - def test_execute_vision():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value.__aiter__.return_value = ["response content"]
2025-04-15 08:39:45,961 - INFO - ---------------
2025-04-15 08:39:48,003 - INFO - Test Result 1- False
2025-04-15 08:39:48,003 - INFO - Test Error 1 - F                                                                        [100%]
================================== FAILURES ===================================
_____________________________ test_execute_vision _____________________________
temp\temp.py:8: in test_execute_vision
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_vision - NameError: name 'patch' is not def...
1 failed in 1.27s
2025-04-15 08:39:48,003 - INFO - TEST CASE 6 Retry 1
2025-04-15 08:39:48,003 - INFO - ---------------
2025-04-15 08:39:48,004 - INFO - def test_execute_vision():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value.__aiter__.return_value = ["response content"]
2025-04-15 08:39:48,004 - INFO - ---------------
2025-04-15 08:39:49,153 - INFO - proposed_test_case 2-
from unittest.mock import patch, AsyncMock

def test_execute_vision():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value.__aiter__.return_value = ["response content"]
2025-04-15 08:39:51,004 - INFO - Test Result 2- True
2025-04-15 08:39:51,004 - INFO - Test Error 2 - .                                                                        [100%]
1 passed in 1.02s
2025-04-15 08:39:51,005 - INFO - [1mEnd Processing file: theory_evaluation\llm_handler.py[0m

2025-04-15 08:39:51,005 - INFO - [1mStart Processing file: theory_evaluation\llm_utils.py[0m
2025-04-15 08:39:55,284 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 08:39:55,289 - INFO - 

2025-04-15 08:39:55,290 - INFO - TEST CASE 1 Retry 0
2025-04-15 08:39:55,291 - INFO - ---------------
2025-04-15 08:39:55,291 - INFO - def test_initialise_prompt():
    agent = "test_agent"
    config_yaml_content = "key: value"
    prompt_txt_content = "This is a {$key} prompt."
2025-04-15 08:39:55,292 - INFO - ---------------
2025-04-15 08:39:56,027 - INFO - Test Result 1- True
2025-04-15 08:39:56,028 - INFO - Test Error 1 - .                                                                        [100%]
1 passed in 0.14s
2025-04-15 08:39:56,028 - INFO - 

2025-04-15 08:39:56,028 - INFO - TEST CASE 2 Retry 0
2025-04-15 08:39:56,028 - INFO - ---------------
2025-04-15 08:39:56,028 - INFO - def test_initialise_settings():
    agent = "test_agent"
    llm_settings_content = '{"setting_key": "setting_value"}'
2025-04-15 08:39:56,028 - INFO - ---------------
2025-04-15 08:39:56,674 - INFO - Test Result 1- True
2025-04-15 08:39:56,674 - INFO - Test Error 1 - .                                                                        [100%]
1 passed in 0.13s
2025-04-15 08:39:56,675 - INFO - [1mEnd Processing file: theory_evaluation\llm_utils.py[0m

2025-04-15 08:39:56,675 - INFO - [1mStart Processing file: theory_evaluation\__init__.py[0m
2025-04-15 08:39:56,675 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

