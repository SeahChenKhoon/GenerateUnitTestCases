2025-04-16 10:59:42,316 - INFO - Loading environment variables start
2025-04-16 10:59:42,322 - INFO - Loading environment variables completes
2025-04-16 10:59:42,322 - INFO - Initialising of LLM start
2025-04-16 10:59:42,723 - INFO - Initialising of LLM completes
2025-04-16 10:59:42,723 - INFO - Getting python file starts
2025-04-16 10:59:42,725 - INFO - Getting python file completes
2025-04-16 10:59:42,725 - INFO - 
Start Processing file: theory_evaluation\llm_handler.py
2025-04-16 10:59:42,725 - INFO - Extraction of function and class start
2025-04-16 10:59:42,726 - INFO - extraction of function and class complete
2025-04-16 10:59:42,726 - INFO - Generate Unit Test Case starts
2025-04-16 10:59:42,726 - INFO - Extract unique import start
2025-04-16 10:59:44,384 - INFO - Extract unique import complete
2025-04-16 10:59:44,384 - INFO - Update relative import start
2025-04-16 10:59:44,385 - INFO - Update relative import complete
2025-04-16 10:59:53,224 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-16 10:59:53,225 - INFO - Generate Unit Test Case complete
2025-04-16 10:59:53,233 - INFO - run_each_pytest_function_individually start
2025-04-16 11:00:05,078 - INFO - Number of test case to process - 6
2025-04-16 11:00:05,079 - INFO - 

2025-04-16 11:00:05,079 - INFO - TEST CASE 1 Retry 0
2025-04-16 11:00:05,079 - INFO - ---------------
2025-04-16 11:00:05,079 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('theory_evaluation.llm_handler.os.getenv', side_effect=lambda key: f"{key}_value"):

2025-04-16 11:00:05,080 - INFO - ---------------
2025-04-16 11:00:06,069 - INFO - TEST CASE 1 Retry 0 - Result - Failed
2025-04-16 11:00:06,069 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
.venv\Lib\site-packages\_pytest\python.py:493: in importtestmodule
    mod = import_path(
.venv\Lib\site-packages\_pytest\pathlib.py:587: in import_path
    importlib.import_module(module_name)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
.venv\Lib\site-packages\_pytest\assertion\rewrite.py:176: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
.venv\Lib\site-packages\_pytest\assertion\rewrite.py:356: in _rewrite_test
    tree = ast.parse(source, filename=strfn)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\ast.py:54: in parse
    return compile(source, filename, mode, flags,
E     File "C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py", line 14
E       patch('theory_evaluation.llm_handler.os.getenv', side_effect=lambda key: f"{key}_value"):
E                                                                                                ^
E   IndentationError: expected an indented block after 'with' statement on line 12
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 0.23s
2025-04-16 11:00:08,200 - INFO - TEST CASE 1 Retry 1
2025-04-16 11:00:08,201 - INFO - ---------------
2025-04-16 11:00:08,201 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import patch

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('theory_evaluation.llm_handler.os.getenv', side_effect=lambda key: f"{key}_value"):
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.azure_endpoint == "AZURE_OPENAI_ENDPOINT_SWEDEN_value"
        assert llm.api_version == "AZURE_OPENAI_API_VERSION_value"
        assert llm.client == mock_azure_openai.return_value

2025-04-16 11:00:08,202 - INFO - ---------------
2025-04-16 11:00:10,125 - INFO - TEST CASE 1 Retry 1 - Result - Passed
2025-04-16 11:00:10,126 - INFO - Test Case 1 processed successfully
2025-04-16 11:00:10,126 - INFO - 

2025-04-16 11:00:10,126 - INFO - TEST CASE 2 Retry 0
2025-04-16 11:00:10,126 - INFO - ---------------
2025-04-16 11:00:10,126 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message.content = json.dumps({"answer": "test_answer", "explanation": "test_explanation"})

2025-04-16 11:00:10,126 - INFO - ---------------
2025-04-16 11:00:12,272 - INFO - TEST CASE 2 Retry 0 - Result - Failed
2025-04-16 11:00:12,273 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_json_completion _________________________
temp\temp.py:12: in test_openai_json_completion
    mock_response = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_json_completion - NameError: name 'MagicMock...
1 failed in 1.19s
2025-04-16 11:00:13,672 - INFO - TEST CASE 2 Retry 1
2025-04-16 11:00:13,674 - INFO - ---------------
2025-04-16 11:00:13,675 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message.content = json.dumps({"answer": "test_answer", "explanation": "test_explanation"})

2025-04-16 11:00:13,677 - INFO - ---------------
2025-04-16 11:00:16,669 - INFO - TEST CASE 2 Retry 1 - Result - Passed
2025-04-16 11:00:16,670 - INFO - Test Case 2 processed successfully
2025-04-16 11:00:16,670 - INFO - 

2025-04-16 11:00:16,670 - INFO - TEST CASE 3 Retry 0
2025-04-16 11:00:16,670 - INFO - ---------------
2025-04-16 11:00:16,670 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices = [MagicMock()]
    mock_chunk.choices[0].delta.content = "stream_content"

2025-04-16 11:00:16,671 - INFO - ---------------
2025-04-16 11:00:18,995 - INFO - TEST CASE 3 Retry 0 - Result - Failed
2025-04-16 11:00:18,995 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_openai_streaming ____________________________
temp\temp.py:12: in test_openai_streaming
    mock_chunk = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_streaming - NameError: name 'MagicMock' is n...
1 failed in 1.20s
2025-04-16 11:00:20,456 - INFO - TEST CASE 3 Retry 1
2025-04-16 11:00:20,456 - INFO - ---------------
2025-04-16 11:00:20,456 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock

2025-04-16 11:00:20,456 - INFO - ---------------
2025-04-16 11:00:21,932 - INFO - TEST CASE 3 Retry 1 - Result - Failed
2025-04-16 11:00:21,932 - INFO - Test Error 2 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

no tests ran in 0.79s
2025-04-16 11:00:21,932 - INFO - Failed after all retries for test case 3
2025-04-16 11:00:25,575 - INFO - TEST CASE 3 Retry 2
2025-04-16 11:00:25,576 - INFO - ---------------
2025-04-16 11:00:25,576 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
import pytest_asyncio
import pytest
import asyncio
from unittest.mock import MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest_asyncio.fixture
async def mock_openai_llm():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value = MagicMock(
        choices=[MagicMock(message=MagicMock(content='{"answer": "7", "explanation": "2+5 equals 7."}'))]
    )
    llm = OpenAI_llm(useAzureOpenAI=False)
    llm.client = mock_client
    return llm

@pytest.mark.asyncio
async def test_openai_json_completion(mock_openai_llm):
    response = await mock_openai_llm._OpenAI_JSON_Completion()
    assert response == {"answer": "7", "explanation": "2+5 equals 7."}

@pytest.mark.asyncio
async def test_openai_chat_completion(mock_openai_llm):
    response = await mock_openai_llm._OpenAI_Chat_Completion()
    assert response == '{"answer": "7", "explanation": "2+5 equals 7."}'

@pytest.mark.asyncio
async def test_execute(mock_openai_llm):
    responses = []
    async for response in mock_openai_llm.execute():
        responses.append(response)
    assert responses == ['{"answer": "7", "explanation": "2+5 equals 7."}']

2025-04-16 11:00:25,576 - INFO - ---------------
2025-04-16 11:00:28,278 - INFO - TEST CASE 3 Retry 2 - Result - Passed
2025-04-16 11:00:28,278 - INFO - Test Case 3 processed successfully
2025-04-16 11:00:28,279 - INFO - 

2025-04-16 11:00:28,279 - INFO - TEST CASE 4 Retry 0
2025-04-16 11:00:28,279 - INFO - ---------------
2025-04-16 11:00:28,279 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message.content = "chat_content"

2025-04-16 11:00:28,279 - INFO - ---------------
2025-04-16 11:00:30,281 - INFO - TEST CASE 4 Retry 0 - Result - Failed
2025-04-16 11:00:30,281 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_chat_completion _________________________
temp\temp.py:12: in test_openai_chat_completion
    mock_response = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_chat_completion - NameError: name 'MagicMock...
1 failed in 1.08s
2025-04-16 11:00:31,350 - INFO - TEST CASE 4 Retry 1
2025-04-16 11:00:31,350 - INFO - ---------------
2025-04-16 11:00:31,350 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message.content = "chat_content"

2025-04-16 11:00:31,351 - INFO - ---------------
2025-04-16 11:00:33,743 - INFO - TEST CASE 4 Retry 1 - Result - Passed
2025-04-16 11:00:33,744 - INFO - Test Case 4 processed successfully
2025-04-16 11:00:33,744 - INFO - 

2025-04-16 11:00:33,744 - INFO - TEST CASE 5 Retry 0
2025-04-16 11:00:33,744 - INFO - ---------------
2025-04-16 11:00:33,744 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message.content = "text_generation_content"

2025-04-16 11:00:33,744 - INFO - ---------------
2025-04-16 11:00:36,436 - INFO - TEST CASE 5 Retry 0 - Result - Failed
2025-04-16 11:00:36,436 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_execute_text_generation _________________________
temp\temp.py:12: in test_execute_text_generation
    mock_response = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_text_generation - NameError: name 'MagicMoc...
1 failed in 1.58s
2025-04-16 11:00:37,838 - INFO - TEST CASE 5 Retry 1
2025-04-16 11:00:37,839 - INFO - ---------------
2025-04-16 11:00:37,839 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock

2025-04-16 11:00:37,840 - INFO - ---------------
2025-04-16 11:00:39,583 - INFO - TEST CASE 5 Retry 1 - Result - Failed
2025-04-16 11:00:39,584 - INFO - Test Error 2 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

no tests ran in 0.88s
2025-04-16 11:00:39,584 - INFO - Failed after all retries for test case 5
2025-04-16 11:00:43,203 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-16 11:00:43,204 - INFO - TEST CASE 5 Retry 2
2025-04-16 11:00:43,204 - INFO - ---------------
2025-04-16 11:00:43,204 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
import pytest
import pytest_asyncio
from unittest.mock import MagicMock, patch

@pytest_asyncio.fixture(scope="function")
async def llm():
    with patch('openai.AzureOpenAI') as MockAzureOpenAI, \
         patch('openai.OpenAI') as MockOpenAI:
        MockAzureOpenAI.return_value = MagicMock()
        MockOpenAI.return_value = MagicMock()
        llm_instance = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=True,
            output="stream",
            verbose=True
        )
        yield llm_instance

@pytest.mark.asyncio
async def test_execute(llm):
    llm.client.chat.completions.create.return_value = MagicMock(
        choices=[MagicMock(delta=MagicMock(content="Test response"))]
    )
    responses = []
    async for response in llm.execute():
        responses.append(response)
    assert responses == ["Test response"]

2025-04-16 11:00:43,204 - INFO - ---------------
2025-04-16 11:00:45,131 - INFO - TEST CASE 5 Retry 2 - Result - Failed
2025-04-16 11:00:45,132 - INFO - Test Error 3 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
E                                                                        [100%]
=================================== ERRORS ====================================
_______________________ ERROR at setup of test_execute ________________________
temp\temp.py:21: in llm
    llm_instance = OpenAI_llm(
theory_evaluation\llm_handler.py:63: in __init__
    self.client = AzureOpenAI(
.venv\Lib\site-packages\openai\lib\azure.py:194: in __init__
    raise OpenAIError(
E   openai.OpenAIError: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.
=========================== short test summary info ===========================
ERROR temp/temp.py::test_execute - openai.OpenAIError: Missing credentials. P...
1 error in 1.10s
2025-04-16 11:00:45,132 - INFO - Failed after all retries for test case 5
2025-04-16 11:00:47,423 - INFO - TEST CASE 5 Retry 3
2025-04-16 11:00:47,424 - INFO - ---------------
2025-04-16 11:00:47,424 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch

@pytest_asyncio.fixture(scope="function")
async def llm():
    with patch('openai.AzureOpenAI') as MockAzureOpenAI, \
         patch('openai.OpenAI') as MockOpenAI:
        MockAzureOpenAI.return_value = MagicMock()
        MockOpenAI.return_value = MagicMock()
        llm_instance = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=True,
            output="stream",
            verbose=True
        )
        yield llm_instance

@pytest.mark.asyncio
async def test_execute(llm):
    llm.client.chat.completions.create.return_value = MagicMock(
        choices=[MagicMock(delta=MagicMock(content="Test response"))]
    )
    responses = []
    async for response in llm.execute():
        responses.append(response)
    assert responses == ["Test response"]

2025-04-16 11:00:47,424 - INFO - ---------------
2025-04-16 11:00:49,640 - INFO - TEST CASE 5 Retry 3 - Result - Failed
2025-04-16 11:00:49,640 - INFO - Test Error 4 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
temp\temp.py:13: in <module>
    @pytest_asyncio.fixture(scope="function")
E   NameError: name 'pytest_asyncio' is not defined
=========================== short test summary info ===========================
ERROR temp/temp.py - NameError: name 'pytest_asyncio' is not defined
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.15s
2025-04-16 11:00:49,641 - INFO - Failed after all retries for test case 5
2025-04-16 11:00:49,641 - INFO - 

2025-04-16 11:00:49,641 - INFO - TEST CASE 6 Retry 0
2025-04-16 11:00:49,641 - INFO - ---------------
2025-04-16 11:00:49,641 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message.content = "vision_content"

2025-04-16 11:00:49,642 - INFO - ---------------
2025-04-16 11:00:51,743 - INFO - TEST CASE 6 Retry 0 - Result - Failed
2025-04-16 11:00:51,743 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_____________________________ test_execute_vision _____________________________
temp\temp.py:12: in test_execute_vision
    mock_response = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_vision - NameError: name 'MagicMock' is not...
1 failed in 1.27s
2025-04-16 11:00:53,129 - INFO - TEST CASE 6 Retry 1
2025-04-16 11:00:53,130 - INFO - ---------------
2025-04-16 11:00:53,130 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock

@pytest.mark.asyncio
async def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message.content = "vision_content"

2025-04-16 11:00:53,130 - INFO - ---------------
2025-04-16 11:00:56,039 - INFO - TEST CASE 6 Retry 1 - Result - Passed
2025-04-16 11:00:56,039 - INFO - Test Case 6 processed successfully
2025-04-16 11:00:56,040 - INFO - run_each_pytest_function_individually complete
2025-04-16 11:00:56,043 - INFO - End Processing file: theory_evaluation\llm_handler.py

2025-04-16 11:00:56,044 - INFO - 
Start Processing file: theory_evaluation\__init__.py
2025-04-16 11:00:56,045 - INFO - Extraction of function and class start
2025-04-16 11:00:56,045 - INFO - extraction of function and class complete
2025-04-16 11:00:56,045 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

