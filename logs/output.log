2025-04-15 14:02:20,620 - INFO - Loading environment variables...
2025-04-15 14:02:20,943 - INFO - [1mStart Processing file: theory_evaluation\llm_handler.py[0m
2025-04-15 14:02:32,487 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 14:02:33,699 - INFO - pytest_fixture - 
@pytest.fixture
def mock_openai_client():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai

@pytest.fixture
def mock_azure_openai_client():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai

2025-04-15 14:02:33,700 - INFO - llm_test_cases_prompt - Extract all unit test cases excluding pytest fixtures from the following:
```
import asyncio
import json
import os
from unittest.mock import patch, AsyncMock, MagicMock

import pytest
from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.fixture
def mock_openai_client():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai

@pytest.fixture
def mock_azure_openai_client():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai

@pytest.mark.asyncio
async def test_openai_llm_initialization_with_openai(mock_openai_client):
    mock_openai_client.return_value = MagicMock()
    llm = OpenAI_llm(
        message="Test message",
        useAzureOpenAI=False,
        output="json",
        mode="text_generation"
    )
    assert llm.message == "Test message"
    assert llm.client is not None
    assert hasattr(llm.client, 'chat')

@pytest.mark.asyncio
async def test_openai_llm_initialization_with_azure_openai(mock_azure_openai_client):
    mock_azure_openai_client.return_value = MagicMock()
    llm = OpenAI_llm(
        message="Test message",
        useAzureOpenAI=True,
        output="json",
        mode="text_generation"
    )
    assert llm.message == "Test message"
    assert llm.client is not None
    assert hasattr(llm.client, 'chat')

@pytest.mark.asyncio
async def test_openai_json_completion(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"answer": "42"})))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False)
    result = await llm._OpenAI_JSON_Completion()
    assert result == {"answer": "42"}

@pytest.mark.asyncio
async def test_openai_streaming(mock_openai_client):
    mock_chunk = MagicMock()
    mock_chunk.choices = [MagicMock(delta=MagicMock(content="streaming content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=[mock_chunk])

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False, output="stream")
    async for content in llm._OpenAI_Streaming():
        assert content == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="chat completion content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False)
    result = await llm._OpenAI_Chat_Completion()
    assert result == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation_mode(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="text generation content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False)
    async for response in llm.execute():
        assert response == "text generation content"

@pytest.mark.asyncio
async def test_execute_vision_mode(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="vision mode content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False, mode="vision", image_input="image_data")
    async for response in llm.execute():
        assert response == "vision mode content"
```
. Output only unit test cases. No Markdown formatting, explanations, or docstrings. Do NOT wrap your output in backticks
2025-04-15 14:02:45,299 - INFO - test_cases_str - 
@pytest.mark.asyncio
async def test_openai_llm_initialization_with_openai(mock_openai_client):
    mock_openai_client.return_value = MagicMock()
    llm = OpenAI_llm(
        message="Test message",
        useAzureOpenAI=False,
        output="json",
        mode="text_generation"
    )
    assert llm.message == "Test message"
    assert llm.client is not None
    assert hasattr(llm.client, 'chat')

@pytest.mark.asyncio
async def test_openai_llm_initialization_with_azure_openai(mock_azure_openai_client):
    mock_azure_openai_client.return_value = MagicMock()
    llm = OpenAI_llm(
        message="Test message",
        useAzureOpenAI=True,
        output="json",
        mode="text_generation"
    )
    assert llm.message == "Test message"
    assert llm.client is not None
    assert hasattr(llm.client, 'chat')

@pytest.mark.asyncio
async def test_openai_json_completion(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"answer": "42"})))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False)
    result = await llm._OpenAI_JSON_Completion()
    assert result == {"answer": "42"}

@pytest.mark.asyncio
async def test_openai_streaming(mock_openai_client):
    mock_chunk = MagicMock()
    mock_chunk.choices = [MagicMock(delta=MagicMock(content="streaming content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=[mock_chunk])

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False, output="stream")
    async for content in llm._OpenAI_Streaming():
        assert content == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="chat completion content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False)
    result = await llm._OpenAI_Chat_Completion()
    assert result == "chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation_mode(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="text generation content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False)
    async for response in llm.execute():
        assert response == "text generation content"

@pytest.mark.asyncio
async def test_execute_vision_mode(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="vision mode content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

    llm = OpenAI_llm(message="Test message", useAzureOpenAI=False, mode="vision", image_input="image_data")
    async for response in llm.execute():
        assert response == "vision mode content"

2025-04-15 14:02:45,301 - INFO - 

2025-04-15 14:02:45,302 - INFO - TEST CASE 1 Retry 0
2025-04-15 14:02:45,302 - INFO - ---------------
2025-04-15 14:02:45,302 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

@pytest.fixture
def mock_openai_client():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai

@pytest.fixture
def mock_azure_openai_client():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai
@pytest.mark.asyncio
async def test_openai_llm_initialization_with_openai(mock_openai_client):
    mock_openai_client.return_value = MagicMock()
    llm = OpenAI_llm(
        message="Test message",
        useAzureOpenAI=False,
        output="json",
        mode="text_generation"
    )
    assert llm.message == "Test message"
    assert llm.client is not None
    assert hasattr(llm.client, 'chat')

2025-04-15 14:02:45,302 - INFO - ---------------
2025-04-15 14:02:45,302 - INFO - 

2025-04-15 14:02:45,303 - INFO - TEST CASE 2 Retry 0
2025-04-15 14:02:45,303 - INFO - ---------------
2025-04-15 14:02:45,303 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

@pytest.fixture
def mock_openai_client():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai

@pytest.fixture
def mock_azure_openai_client():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai
@pytest.mark.asyncio
async def test_openai_llm_initialization_with_azure_openai(mock_azure_openai_client):
    mock_azure_openai_client.return_value = MagicMock()
    llm = OpenAI_llm(
        message="Test message",
        useAzureOpenAI=True,
        output="json",
        mode="text_generation"
    )
    assert llm.message == "Test message"
    assert llm.client is not None
    assert hasattr(llm.client, 'chat')

2025-04-15 14:02:45,303 - INFO - ---------------
2025-04-15 14:02:45,303 - INFO - 

2025-04-15 14:02:45,303 - INFO - TEST CASE 3 Retry 0
2025-04-15 14:02:45,303 - INFO - ---------------
2025-04-15 14:02:45,303 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

@pytest.fixture
def mock_openai_client():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai

@pytest.fixture
def mock_azure_openai_client():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai
@pytest.mark.asyncio
async def test_openai_json_completion(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"answer": "42"})))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

2025-04-15 14:02:45,304 - INFO - ---------------
2025-04-15 14:02:45,304 - INFO - 

2025-04-15 14:02:45,304 - INFO - TEST CASE 4 Retry 0
2025-04-15 14:02:45,304 - INFO - ---------------
2025-04-15 14:02:45,304 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

@pytest.fixture
def mock_openai_client():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai

@pytest.fixture
def mock_azure_openai_client():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai
@pytest.mark.asyncio
async def test_openai_streaming(mock_openai_client):
    mock_chunk = MagicMock()
    mock_chunk.choices = [MagicMock(delta=MagicMock(content="streaming content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=[mock_chunk])

2025-04-15 14:02:45,304 - INFO - ---------------
2025-04-15 14:02:45,304 - INFO - 

2025-04-15 14:02:45,304 - INFO - TEST CASE 5 Retry 0
2025-04-15 14:02:45,305 - INFO - ---------------
2025-04-15 14:02:45,305 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

@pytest.fixture
def mock_openai_client():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai

@pytest.fixture
def mock_azure_openai_client():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai
@pytest.mark.asyncio
async def test_openai_chat_completion(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="chat completion content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

2025-04-15 14:02:45,305 - INFO - ---------------
2025-04-15 14:02:45,305 - INFO - 

2025-04-15 14:02:45,305 - INFO - TEST CASE 6 Retry 0
2025-04-15 14:02:45,305 - INFO - ---------------
2025-04-15 14:02:45,305 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

@pytest.fixture
def mock_openai_client():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai

@pytest.fixture
def mock_azure_openai_client():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai
@pytest.mark.asyncio
async def test_execute_text_generation_mode(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="text generation content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

2025-04-15 14:02:45,305 - INFO - ---------------
2025-04-15 14:02:45,305 - INFO - 

2025-04-15 14:02:45,306 - INFO - TEST CASE 7 Retry 0
2025-04-15 14:02:45,306 - INFO - ---------------
2025-04-15 14:02:45,306 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

@pytest.fixture
def mock_openai_client():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai

@pytest.fixture
def mock_azure_openai_client():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai
@pytest.mark.asyncio
async def test_execute_vision_mode(mock_openai_client):
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="vision mode content"))]
    mock_openai_client.return_value.chat.completions.create = AsyncMock(return_value=mock_response)

2025-04-15 14:02:45,306 - INFO - ---------------
2025-04-15 14:02:45,306 - INFO - [1mEnd Processing file: theory_evaluation\llm_handler.py[0m

2025-04-15 14:02:45,306 - INFO - [1mStart Processing file: theory_evaluation\llm_utils.py[0m
2025-04-15 14:02:53,383 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 14:02:53,795 - INFO - pytest_fixture - 


2025-04-15 14:02:53,796 - INFO - llm_test_cases_prompt - Extract all unit test cases excluding pytest fixtures from the following:
```
import os
import pytest
import yaml
import re
from unittest.mock import patch, mock_open
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings

def test_initialise_prompt_success():
    agent = "test_agent"
    mock_config_values = {'key1': 'value1', 'key2': 'value2'}
    mock_prompt_structure = "This is a {$key1} and {$key2} test."

    with patch("theory_evaluation.llm_utils.open", mock_open(read_data=yaml.dump(mock_config_values))) as mock_file:
        with patch("theory_evaluation.llm_utils.yaml.load", return_value=mock_config_values):
            with patch("theory_evaluation.llm_utils.re.finditer", return_value=re.finditer(r"\{\$(\w+)\}", mock_prompt_structure)):
                result = initialise_prompt(agent)
                expected_prompt = "This is a value1 and value2 test."
                assert result == expected_prompt
                mock_file.assert_has_calls([
                    patch("theory_evaluation.llm_utils.open", mock_open(read_data=yaml.dump(mock_config_values))).call(f"./theory_evaluation/evaluator/prompts/{agent}/config.yaml"),
                    patch("theory_evaluation.llm_utils.open", mock_open(read_data=mock_prompt_structure)).call(f"./theory_evaluation/evaluator/prompts/{agent}/prompt.txt", "r")
                ])

def test_initialise_prompt_file_not_found():
    agent = "non_existent_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_prompt(agent)
        assert result is None

def test_initialise_settings_success():
    agent = "test_agent"
    mock_settings = {'setting1': 'value1', 'setting2': 'value2'}

    with patch("theory_evaluation.llm_utils.open", mock_open(read_data=yaml.dump(mock_settings))) as mock_file:
        with patch("theory_evaluation.llm_utils.yaml.safe_load", return_value=mock_settings):
            result = initialise_settings(agent)
            assert result == mock_settings
            mock_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/llm_settings.yaml")

def test_initialise_settings_file_not_found():
    agent = "non_existent_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_settings(agent)
        assert result is None
```
. Output only unit test cases. No Markdown formatting, explanations, or docstrings. Do NOT wrap your output in backticks
2025-04-15 14:02:58,819 - INFO - test_cases_str - 
def test_initialise_prompt_success():
    agent = "test_agent"
    mock_config_values = {'key1': 'value1', 'key2': 'value2'}
    mock_prompt_structure = "This is a {$key1} and {$key2} test."

    with patch("theory_evaluation.llm_utils.open", mock_open(read_data=yaml.dump(mock_config_values))) as mock_file:
        with patch("theory_evaluation.llm_utils.yaml.load", return_value=mock_config_values):
            with patch("theory_evaluation.llm_utils.re.finditer", return_value=re.finditer(r"\{\$(\w+)\}", mock_prompt_structure)):
                result = initialise_prompt(agent)
                expected_prompt = "This is a value1 and value2 test."
                assert result == expected_prompt
                mock_file.assert_has_calls([
                    patch("theory_evaluation.llm_utils.open", mock_open(read_data=yaml.dump(mock_config_values))).call(f"./theory_evaluation/evaluator/prompts/{agent}/config.yaml"),
                    patch("theory_evaluation.llm_utils.open", mock_open(read_data=mock_prompt_structure)).call(f"./theory_evaluation/evaluator/prompts/{agent}/prompt.txt", "r")
                ])

def test_initialise_prompt_file_not_found():
    agent = "non_existent_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_prompt(agent)
        assert result is None

def test_initialise_settings_success():
    agent = "test_agent"
    mock_settings = {'setting1': 'value1', 'setting2': 'value2'}

    with patch("theory_evaluation.llm_utils.open", mock_open(read_data=yaml.dump(mock_settings))) as mock_file:
        with patch("theory_evaluation.llm_utils.yaml.safe_load", return_value=mock_settings):
            result = initialise_settings(agent)
            assert result == mock_settings
            mock_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/llm_settings.yaml")

def test_initialise_settings_file_not_found():
    agent = "non_existent_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_settings(agent)
        assert result is None

2025-04-15 14:02:58,821 - INFO - [1mEnd Processing file: theory_evaluation\llm_utils.py[0m

2025-04-15 14:02:58,821 - INFO - [1mStart Processing file: theory_evaluation\__init__.py[0m
2025-04-15 14:02:58,822 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

