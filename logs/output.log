2025-04-16 12:42:16,572 - INFO - Loading environment variables start
2025-04-16 12:42:16,577 - INFO - Loading environment variables completes
2025-04-16 12:42:16,577 - INFO - Initialising of LLM start
2025-04-16 12:42:16,958 - INFO - Initialising of LLM completes
2025-04-16 12:42:16,958 - INFO - Getting python file starts
2025-04-16 12:42:16,959 - INFO - Getting python file completes
2025-04-16 12:42:16,959 - INFO - 
Start Processing file: theory_evaluation\llm_handler.py
2025-04-16 12:42:16,959 - INFO - Extraction of function and class start
2025-04-16 12:42:16,960 - INFO - extraction of function and class complete
2025-04-16 12:42:16,960 - INFO - Generate Unit Test Case starts
2025-04-16 12:42:16,960 - INFO - Extract unique import start
2025-04-16 12:42:18,729 - INFO - Extract unique import complete
2025-04-16 12:42:18,730 - INFO - Update relative import start
2025-04-16 12:42:18,731 - INFO - Update relative import complete
2025-04-16 12:42:30,490 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-16 12:42:30,491 - INFO - Generate Unit Test Case complete
2025-04-16 12:42:30,496 - INFO - run_each_pytest_function_individually start
2025-04-16 12:42:41,501 - INFO - Number of test case to process - 4
2025-04-16 12:42:41,502 - INFO - 

2025-04-16 12:42:41,502 - INFO - TEST CASE 1 Retry 0
2025-04-16 12:42:41,502 - INFO - ---------------
2025-04-16 12:42:41,503 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_initialization():
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="https://example.com",
        message="Test message",
        image_input=None,
        api_version="v1",
        model_name="test-model",
        max_retries=3,
        output="json",
        mode="text_generation",
        config={"temperature": 0.5},
        verbose=True,
    )
    assert llm.message == "Test message"
    assert llm.azure_endpoint == "https://example.com"
    assert llm.api_version == "v1"
    assert llm.model_name == "test-model"
    assert llm.max_retries == 3
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True

2025-04-16 12:42:41,503 - INFO - ---------------
2025-04-16 12:42:43,430 - INFO - TEST CASE 1 Retry 0 - Result - Failed
2025-04-16 12:42:43,430 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:12: in test_openai_llm_initialization
    llm = OpenAI_llm(
theory_evaluation\llm_handler.py:63: in __init__
    self.client = AzureOpenAI(
.venv\Lib\site-packages\openai\lib\azure.py:194: in __init__
    raise OpenAIError(
E   openai.OpenAIError: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - openai.OpenAIError: Mis...
1 failed in 1.01s
2025-04-16 12:42:47,542 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-16 12:42:47,543 - INFO - TEST CASE 1 Retry 1
2025-04-16 12:42:47,543 - INFO - ---------------
2025-04-16 12:42:47,544 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
import pytest
import os
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_initialization(monkeypatch):
    monkeypatch.setenv("AZURE_OPENAI_API_KEY_SWEDEN", "test_api_key")
    monkeypatch.setenv("AZURE_OPENAI_ENDPOINT_SWEDEN", "https://example.com")
    monkeypatch.setenv("AZURE_OPENAI_API_VERSION", "v1")
    monkeypatch.setenv("AZURE_OPENAI_DEPLOYMENT_NAME", "test-model")

    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="https://example.com",
        message="Test message",
        image_input=None,
        api_version="v1",
        model_name="test-model",
        max_retries=3,
        output="json",
        mode="text_generation",
        config={"temperature": 0.5},
        verbose=True,
    )
    assert llm.message == "Test message"
    assert llm.azure_endpoint == "https://example.com"
    assert llm.api_version == "v1"
    assert llm.model_name == "test-model"
    assert llm.max_retries == 3
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True

2025-04-16 12:42:47,544 - INFO - ---------------
2025-04-16 12:42:50,913 - INFO - TEST CASE 1 Retry 1 - Result - Passed
2025-04-16 12:42:50,914 - INFO - Test Case 1 processed successfully
2025-04-16 12:42:50,914 - INFO - 

2025-04-16 12:42:50,914 - INFO - TEST CASE 2 Retry 0
2025-04-16 12:42:50,915 - INFO - ---------------
2025-04-16 12:42:50,915 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value.choices = [
        MagicMock(message=MagicMock(content='{"answer": "42", "explanation": "The answer is 42."}'))
    ]
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="json",
            mode="text_generation",
            verbose=False,
        )
        async for response in llm.execute():
            assert json.loads(response) == {"answer": "42", "explanation": "The answer is 42."}

2025-04-16 12:42:50,915 - INFO - ---------------
2025-04-16 12:42:53,490 - INFO - TEST CASE 2 Retry 0 - Result - Failed
2025-04-16 12:42:53,490 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:12: in test_openai_llm_execute_text_generation
    mock_client = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - NameError: nam...
1 failed in 1.65s
2025-04-16 12:42:57,079 - INFO - TEST CASE 2 Retry 1
2025-04-16 12:42:57,080 - INFO - ---------------
2025-04-16 12:42:57,080 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch
import pytest
import json

@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value.choices = [
        MagicMock(message=MagicMock(content='{"answer": "42", "explanation": "The answer is 42."}'))
    ]
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="json",
            mode="text_generation",
            verbose=False,
        )
        async for response in llm.execute():
            assert json.loads(response) == {"answer": "42", "explanation": "The answer is 42."}

2025-04-16 12:42:57,080 - INFO - ---------------
2025-04-16 12:42:59,242 - INFO - TEST CASE 2 Retry 1 - Result - Failed
2025-04-16 12:42:59,242 - INFO - Test Error 2 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:21: in test_openai_llm_execute_text_generation
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", mock_client):
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - AttributeError...
1 failed in 1.33s
2025-04-16 12:42:59,242 - INFO - Failed after all retries for test case 2
2025-04-16 12:43:02,818 - INFO - TEST CASE 2 Retry 2
2025-04-16 12:43:02,818 - INFO - ---------------
2025-04-16 12:43:02,819 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch
import pytest
import json
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value.choices = [
        MagicMock(message=MagicMock(content='{"answer": "42", "explanation": "The answer is 42."}'))
    ]
    with patch.object(OpenAI_llm, 'client', mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="json",
            mode="text_generation",
            verbose=False,
        )
        async for response in llm.execute():
            assert json.loads(response) == {"answer": "42", "explanation": "The answer is 42."}

2025-04-16 12:43:02,819 - INFO - ---------------
2025-04-16 12:43:05,144 - INFO - TEST CASE 2 Retry 2 - Result - Failed
2025-04-16 12:43:05,145 - INFO - Test Error 3 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:22: in test_openai_llm_execute_text_generation
    with patch.object(OpenAI_llm, 'client', mock_client):
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - AttributeError...
1 failed in 1.41s
2025-04-16 12:43:05,145 - INFO - Failed after all retries for test case 2
2025-04-16 12:43:08,151 - INFO - TEST CASE 2 Retry 3
2025-04-16 12:43:08,151 - INFO - ---------------
2025-04-16 12:43:08,152 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch
import pytest
import json
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value.choices = [
        MagicMock(message=MagicMock(content='{"answer": "42", "explanation": "The answer is 42."}'))
    ]
    with patch.object(OpenAI_llm, 'client', new_callable=MagicMock) as mock_client_attr:
        mock_client_attr.return_value = mock_client
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="json",
            mode="text_generation",
            verbose=False,
        )
        async for response in llm.execute():
            assert json.loads(response) == {"answer": "42", "explanation": "The answer is 42."}

2025-04-16 12:43:08,152 - INFO - ---------------
2025-04-16 12:43:11,106 - INFO - TEST CASE 2 Retry 3 - Result - Failed
2025-04-16 12:43:11,106 - INFO - Test Error 4 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:22: in test_openai_llm_execute_text_generation
    with patch.object(OpenAI_llm, 'client', new_callable=MagicMock) as mock_client_attr:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - AttributeError...
1 failed in 1.59s
2025-04-16 12:43:11,108 - INFO - Failed after all retries for test case 2
2025-04-16 12:43:11,108 - INFO - 

2025-04-16 12:43:11,109 - INFO - TEST CASE 3 Retry 0
2025-04-16 12:43:11,109 - INFO - ---------------
2025-04-16 12:43:11,109 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value.choices = [
        MagicMock(message=MagicMock(content='{"answer": "Image processed", "explanation": "Processed image data."}'))
    ]
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="json",
            mode="vision",
            image_input="base64image",
            verbose=False,
        )
        async for response in llm.execute():
            assert json.loads(response) == {"answer": "Image processed", "explanation": "Processed image data."}

2025-04-16 12:43:11,109 - INFO - ---------------
2025-04-16 12:43:13,619 - INFO - TEST CASE 3 Retry 0 - Result - Failed
2025-04-16 12:43:13,620 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:12: in test_openai_llm_execute_vision
    mock_client = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - NameError: name 'MagicM...
1 failed in 1.39s
2025-04-16 12:43:17,336 - INFO - TEST CASE 3 Retry 1
2025-04-16 12:43:17,337 - INFO - ---------------
2025-04-16 12:43:17,338 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch
import pytest
import json

@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value.choices = [
        MagicMock(message=MagicMock(content='{"answer": "Image processed", "explanation": "Processed image data."}'))
    ]
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="json",
            mode="vision",
            image_input="base64image",
            verbose=False,
        )
        async for response in llm.execute():
            assert json.loads(response) == {"answer": "Image processed", "explanation": "Processed image data."}

2025-04-16 12:43:17,338 - INFO - ---------------
2025-04-16 12:43:19,787 - INFO - TEST CASE 3 Retry 1 - Result - Failed
2025-04-16 12:43:19,787 - INFO - Test Error 2 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:21: in test_openai_llm_execute_vision
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", mock_client):
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - AttributeError: <class ...
1 failed in 1.17s
2025-04-16 12:43:19,787 - INFO - Failed after all retries for test case 3
2025-04-16 12:43:22,389 - INFO - TEST CASE 3 Retry 2
2025-04-16 12:43:22,389 - INFO - ---------------
2025-04-16 12:43:22,390 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch
import pytest
import json
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value.choices = [
        MagicMock(message=MagicMock(content='{"answer": "Image processed", "explanation": "Processed image data."}'))
    ]
    with patch.object(OpenAI_llm, 'client', mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="json",
            mode="vision",
            image_input="base64image",
            verbose=False,
        )
        async for response in llm.execute():
            assert json.loads(response) == {"answer": "Image processed", "explanation": "Processed image data."}

2025-04-16 12:43:22,390 - INFO - ---------------
2025-04-16 12:43:24,708 - INFO - TEST CASE 3 Retry 2 - Result - Failed
2025-04-16 12:43:24,709 - INFO - Test Error 3 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:22: in test_openai_llm_execute_vision
    with patch.object(OpenAI_llm, 'client', mock_client):
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - AttributeError: <class ...
1 failed in 1.42s
2025-04-16 12:43:24,709 - INFO - Failed after all retries for test case 3
2025-04-16 12:43:29,054 - INFO - TEST CASE 3 Retry 3
2025-04-16 12:43:29,055 - INFO - ---------------
2025-04-16 12:43:29,056 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch
import pytest
import json
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value.choices = [
        MagicMock(message=MagicMock(content='{"answer": "Image processed", "explanation": "Processed image data."}'))
    ]
    with patch.object(OpenAI_llm, 'client', new_callable=MagicMock) as mock_client:
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="json",
            mode="vision",
            image_input="base64image",
            verbose=False,
        )
        async for response in llm.execute():
            assert json.loads(response) == {"answer": "Image processed", "explanation": "Processed image data."}

2025-04-16 12:43:29,056 - INFO - ---------------
2025-04-16 12:43:31,801 - INFO - TEST CASE 3 Retry 3 - Result - Failed
2025-04-16 12:43:31,801 - INFO - Test Error 4 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:22: in test_openai_llm_execute_vision
    with patch.object(OpenAI_llm, 'client', new_callable=MagicMock) as mock_client:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - AttributeError: <class ...
1 failed in 1.64s
2025-04-16 12:43:31,801 - INFO - Failed after all retries for test case 3
2025-04-16 12:43:31,801 - INFO - 

2025-04-16 12:43:31,801 - INFO - TEST CASE 4 Retry 0
2025-04-16 12:43:31,802 - INFO - ---------------
2025-04-16 12:43:31,802 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


@pytest.mark.asyncio
async def test_openai_llm_execute_streaming():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value = iter([
        MagicMock(choices=[MagicMock(delta=MagicMock(content="streaming data"))]),
        MagicMock(choices=[MagicMock(delta=MagicMock(content="more streaming data"))]),
    ])
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="stream",
            mode="text_generation",
            verbose=False,
        )
        responses = []
        async for response in llm.execute():
            responses.append(response)
        assert responses == ["streaming data", "more streaming data"]

2025-04-16 12:43:31,802 - INFO - ---------------
2025-04-16 12:43:33,915 - INFO - TEST CASE 4 Retry 0 - Result - Failed
2025-04-16 12:43:33,916 - INFO - Test Error 1 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_execute_streaming ______________________
temp\temp.py:12: in test_openai_llm_execute_streaming
    mock_client = MagicMock()
E   NameError: name 'MagicMock' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_streaming - NameError: name 'Mag...
1 failed in 1.29s
2025-04-16 12:43:37,316 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-16 12:43:37,316 - INFO - TEST CASE 4 Retry 1
2025-04-16 12:43:37,316 - INFO - ---------------
2025-04-16 12:43:37,316 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch
import pytest

@pytest.mark.asyncio
async def test_openai_llm_execute_streaming():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value = iter([
        MagicMock(choices=[MagicMock(delta=MagicMock(content="streaming data"))]),
        MagicMock(choices=[MagicMock(delta=MagicMock(content="more streaming data"))]),
    ])
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="stream",
            mode="text_generation",
            verbose=False,
        )
        responses = []
        async for response in llm.execute():
            responses.append(response)
        assert responses == ["streaming data", "more streaming data"]

2025-04-16 12:43:37,316 - INFO - ---------------
2025-04-16 12:43:39,308 - INFO - TEST CASE 4 Retry 1 - Result - Failed
2025-04-16 12:43:39,309 - INFO - Test Error 2 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_execute_streaming ______________________
temp\temp.py:21: in test_openai_llm_execute_streaming
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", mock_client):
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_streaming - AttributeError: <cla...
1 failed in 1.15s
2025-04-16 12:43:39,309 - INFO - Failed after all retries for test case 4
2025-04-16 12:43:43,974 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-16 12:43:43,975 - INFO - TEST CASE 4 Retry 2
2025-04-16 12:43:43,976 - INFO - ---------------
2025-04-16 12:43:43,977 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch
import pytest
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute_streaming():
    mock_client = MagicMock()
    mock_client.chat.completions.create.return_value = iter([
        MagicMock(choices=[MagicMock(delta=MagicMock(content="streaming data"))]),
        MagicMock(choices=[MagicMock(delta=MagicMock(content="more streaming data"))]),
    ])
    with patch.object(OpenAI_llm, 'client', mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="stream",
            mode="text_generation",
            verbose=False,
        )
        responses = []
        async for response in llm.execute():
            responses.append(response)
        assert responses == ["streaming data", "more streaming data"]

2025-04-16 12:43:43,977 - INFO - ---------------
2025-04-16 12:43:46,373 - INFO - TEST CASE 4 Retry 2 - Result - Failed
2025-04-16 12:43:46,373 - INFO - Test Error 3 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_execute_streaming ______________________
temp\temp.py:22: in test_openai_llm_execute_streaming
    with patch.object(OpenAI_llm, 'client', mock_client):
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_streaming - AttributeError: <cla...
1 failed in 1.43s
2025-04-16 12:43:46,374 - INFO - Failed after all retries for test case 4
2025-04-16 12:43:49,183 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-16 12:43:49,184 - INFO - TEST CASE 4 Retry 3
2025-04-16 12:43:49,185 - INFO - ---------------
2025-04-16 12:43:49,185 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest


# New Test Case
from unittest.mock import MagicMock, patch, PropertyMock
import pytest
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_execute_streaming():
    mock_client = MagicMock()
    type(mock_client).chat = PropertyMock(return_value=MagicMock())
    mock_client.chat.completions.create.return_value = iter([
        MagicMock(choices=[MagicMock(delta=MagicMock(content="streaming data"))]),
        MagicMock(choices=[MagicMock(delta=MagicMock(content="more streaming data"))]),
    ])
    with patch.object(OpenAI_llm, 'client', new_callable=PropertyMock, return_value=mock_client):
        llm = OpenAI_llm(
            message="Test message",
            useAzureOpenAI=False,
            output="stream",
            mode="text_generation",
            verbose=False,
        )
        responses = []
        async for response in llm.execute():
            responses.append(response)
        assert responses == ["streaming data", "more streaming data"]

2025-04-16 12:43:49,186 - INFO - ---------------
2025-04-16 12:43:52,728 - INFO - TEST CASE 4 Retry 3 - Result - Failed
2025-04-16 12:43:52,728 - INFO - Test Error 4 - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_execute_streaming ______________________
temp\temp.py:23: in test_openai_llm_execute_streaming
    with patch.object(OpenAI_llm, 'client', new_callable=PropertyMock, return_value=mock_client):
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1495: in __enter__
    original, local = self.get_original()
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\unittest\mock.py:1465: in get_original
    raise AttributeError(
E   AttributeError: <class 'theory_evaluation.llm_handler.OpenAI_llm'> does not have the attribute 'client'
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_streaming - AttributeError: <cla...
1 failed in 2.00s
2025-04-16 12:43:52,728 - INFO - Failed after all retries for test case 4
2025-04-16 12:43:52,728 - INFO - run_each_pytest_function_individually complete
2025-04-16 12:43:52,729 - INFO - End Processing file: theory_evaluation\llm_handler.py

2025-04-16 12:43:52,729 - INFO - 
Start Processing file: theory_evaluation\__init__.py
2025-04-16 12:43:52,729 - INFO - Extraction of function and class start
2025-04-16 12:43:52,730 - INFO - extraction of function and class complete
2025-04-16 12:43:52,730 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

