2025-04-15 10:39:36,552 - INFO - Loading environment variables...
2025-04-15 10:39:36,895 - INFO - [1mStart Processing file: theory_evaluation\llm_handler.py[0m
2025-04-15 10:39:47,601 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:39:47,607 - INFO - Hello World 1
2025-04-15 10:39:47,608 - INFO - Hello World 1 - formatted_prompt - Extract pytest fixture from the following unit test file:
```
import pytest
import asyncio
from unittest.mock import patch, AsyncMock, MagicMock
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True, message="Test message")
        assert llm.message == "Test message"
        assert llm.client == mock_azure_openai.return_value

        llm = OpenAI_llm(useAzureOpenAI=False, message="Test message")
        assert llm.client == mock_openai.return_value

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = '{"answer": "42", "explanation": "The answer to life."}'
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_JSON_Completion()
        assert result == {"answer": "42", "explanation": "The answer to life."}

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        llm = OpenAI_llm(message="Test message", output="stream")
        async for content in llm._OpenAI_Streaming():
            assert content == "streaming content"

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "Chat completion content"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_Chat_Completion()
        assert result == "Chat completion content"

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "Generated text"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="text_generation")
        async for content in llm.execute():
            assert content == "Generated text"

@pytest.mark.asyncio
async def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "Vision response"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image")
        async for content in llm.execute():
            assert content == "Vision response"
```
. Output only pytest functions. No Markdown formatting, explanations, or docstrings. Do NOT wrap your output in backticks
2025-04-15 10:39:49,653 - INFO - pytest_fixture - import pytest
from unittest.mock import patch, MagicMock

@pytest.fixture
def mock_openai_llm_client():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        yield mock_client

@pytest.fixture
def mock_env_vars():
    with patch('os.getenv', side_effect=lambda key: f"mock_{key}"):
        yield

@pytest.fixture
def mock_azure_openai():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai:
        yield mock_azure_openai

@pytest.fixture
def mock_openai():
    with patch('theory_evaluation.llm_handler.OpenAI') as mock_openai:
        yield mock_openai
2025-04-15 10:39:49,655 - INFO - 

2025-04-15 10:39:49,656 - INFO - TEST CASE 1 Retry 0
2025-04-15 10:39:49,656 - INFO - ---------------
2025-04-15 10:39:49,656 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True, message="Test message")
        assert llm.message == "Test message"
        assert llm.client == mock_azure_openai.return_value

2025-04-15 10:39:49,656 - INFO - ---------------
2025-04-15 10:39:52,007 - INFO - Test Result 1- False
2025-04-15 10:39:52,008 - INFO - Test Error 1 - F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:8: in test_openai_llm_initialization
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - NameError: name 'patch'...
1 failed in 1.39s
2025-04-15 10:39:53,811 - INFO - TEST CASE 1 Retry 1
2025-04-15 10:39:53,812 - INFO - ---------------
2025-04-15 10:39:53,812 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

from unittest.mock import patch

def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True, message="Test message")
        assert llm.message == "Test message"
        assert llm.client == mock_azure_openai.return_value

2025-04-15 10:39:53,812 - INFO - ---------------
2025-04-15 10:39:55,729 - INFO - Test Result 2- False
2025-04-15 10:39:55,730 - INFO - Test Error 2 - 
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:14: in test_openai_llm_initialization
    llm = OpenAI_llm(useAzureOpenAI=True, message="Test message")
E   NameError: name 'OpenAI_llm' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - NameError: name 'OpenAI...
1 failed in 1.03s
2025-04-15 10:39:57,353 - INFO - TEST CASE 1 Retry 2
2025-04-15 10:39:57,354 - INFO - ---------------
2025-04-15 10:39:57,354 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

from unittest.mock import patch
from llm_handler import OpenAI_llm

def test_openai_llm_initialization():
    with patch('llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(useAzureOpenAI=True, message="Test message")
        assert llm.message == "Test message"
        assert llm.client == mock_azure_openai.return_value

2025-04-15 10:39:57,354 - INFO - ---------------
2025-04-15 10:39:59,334 - INFO - Test Result 3- False
2025-04-15 10:39:59,334 - INFO - Test Error 3 - 
=================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:8: in <module>
    from llm_handler import OpenAI_llm
E   ModuleNotFoundError: No module named 'llm_handler'
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.19s
2025-04-15 10:39:59,334 - INFO - Failed after all retries for test case 1
2025-04-15 10:39:59,335 - INFO - 

2025-04-15 10:39:59,335 - INFO - TEST CASE 2 Retry 0
2025-04-15 10:39:59,335 - INFO - ---------------
2025-04-15 10:39:59,335 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = '{"answer": "42", "explanation": "The answer to life."}'
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_JSON_Completion()
        assert result == {"answer": "42", "explanation": "The answer to life."}

2025-04-15 10:39:59,335 - INFO - ---------------
2025-04-15 10:39:59,993 - INFO - Test Result 1- False
2025-04-15 10:39:59,993 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:493: in importtestmodule
    mod = import_path(
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\pathlib.py:587: in import_path
    importlib.import_module(module_name)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:176: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:358: in _rewrite_test
    co = compile(tree, strfn, "exec", dont_inherit=True)
E     File "C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py", line 14
E       result = await llm._OpenAI_JSON_Completion()
E                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   SyntaxError: 'await' outside async function
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 0.15s
2025-04-15 10:40:03,305 - INFO - TEST CASE 2 Retry 1
2025-04-15 10:40:03,305 - INFO - ---------------
2025-04-15 10:40:03,306 - INFO - 
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI

import pytest
import asyncio
from unittest.mock import MagicMock, patch
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = '{"answer": "42", "explanation": "The answer to life."}'
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_JSON_Completion()
        assert result == {"answer": "42", "explanation": "The answer to life."}

2025-04-15 10:40:03,306 - INFO - ---------------
2025-04-15 10:40:06,067 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:40:06,068 - INFO - New import Statements 2- import pytest
from unittest.mock import MagicMock, patch
from theory_evaluation.llm_handler import OpenAI_llm
2025-04-15 10:40:06,068 - INFO - Test Result 2- True
2025-04-15 10:40:06,068 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp\temp.py:12
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:12: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_json_completion
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 2 warnings in 1.16s
2025-04-15 10:40:06,068 - INFO - 

2025-04-15 10:40:06,069 - INFO - TEST CASE 3 Retry 0
2025-04-15 10:40:06,069 - INFO - ---------------
2025-04-15 10:40:06,069 - INFO - 
import pytest
from unittest.mock import MagicMock, patch
from theory_evaluation.llm_handler import OpenAI_llm
def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        llm = OpenAI_llm(message="Test message", output="stream")
        async for content in llm._OpenAI_Streaming():
            assert content == "streaming content"

2025-04-15 10:40:06,069 - INFO - ---------------
2025-04-15 10:40:07,387 - INFO - Test Result 1- False
2025-04-15 10:40:07,387 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:493: in importtestmodule
    mod = import_path(
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\pathlib.py:587: in import_path
    importlib.import_module(module_name)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:176: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:358: in _rewrite_test
    co = compile(tree, strfn, "exec", dont_inherit=True)
E     File "C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py", line 11
E       async for content in llm._OpenAI_Streaming():
E       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   SyntaxError: 'async for' outside async function
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 0.19s
2025-04-15 10:40:10,056 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:40:10,057 - INFO - TEST CASE 3 Retry 1
2025-04-15 10:40:10,058 - INFO - ---------------
2025-04-15 10:40:10,058 - INFO - 
import pytest
from unittest.mock import MagicMock, patch
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
import asyncio
from unittest.mock import MagicMock, patch
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streaming content"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = [mock_chunk]
        llm = OpenAI_llm(message="Test message", output="stream")
        async for content in llm._OpenAI_Streaming():
            assert content == "streaming content"

2025-04-15 10:40:10,058 - INFO - ---------------
2025-04-15 10:40:12,456 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:40:12,457 - INFO - New import Statements 2- import asyncio
2025-04-15 10:40:12,457 - INFO - Test Result 2- True
2025-04-15 10:40:12,457 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp\temp.py:9
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:9: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_streaming
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 2 warnings in 0.99s
2025-04-15 10:40:12,457 - INFO - 

2025-04-15 10:40:12,457 - INFO - TEST CASE 4 Retry 0
2025-04-15 10:40:12,457 - INFO - ---------------
2025-04-15 10:40:12,458 - INFO - 
import asyncio
def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "Chat completion content"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_Chat_Completion()
        assert result == "Chat completion content"

2025-04-15 10:40:12,458 - INFO - ---------------
2025-04-15 10:40:13,219 - INFO - Test Result 1- False
2025-04-15 10:40:13,219 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:493: in importtestmodule
    mod = import_path(
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\pathlib.py:587: in import_path
    importlib.import_module(module_name)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:176: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:358: in _rewrite_test
    co = compile(tree, strfn, "exec", dont_inherit=True)
E     File "C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py", line 9
E       result = await llm._OpenAI_Chat_Completion()
E                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   SyntaxError: 'await' outside async function
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 0.18s
2025-04-15 10:40:14,967 - INFO - TEST CASE 4 Retry 1
2025-04-15 10:40:14,967 - INFO - ---------------
2025-04-15 10:40:14,967 - INFO - 
import asyncio
import pytest
from unittest.mock import MagicMock, patch
import asyncio

@pytest.mark.asyncio
async def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "Chat completion content"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message")
        result = await llm._OpenAI_Chat_Completion()
        assert result == "Chat completion content"

2025-04-15 10:40:14,967 - INFO - ---------------
2025-04-15 10:40:16,169 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:40:16,169 - INFO - New import Statements 2- import pytest
from unittest.mock import MagicMock, patch
2025-04-15 10:40:16,169 - INFO - Test Result 2- True
2025-04-15 10:40:16,170 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp\temp.py:6
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:6: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_openai_chat_completion
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 2 warnings in 0.06s
2025-04-15 10:40:16,170 - INFO - 

2025-04-15 10:40:16,170 - INFO - TEST CASE 5 Retry 0
2025-04-15 10:40:16,170 - INFO - ---------------
2025-04-15 10:40:16,170 - INFO - 
import pytest
from unittest.mock import MagicMock, patch
def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "Generated text"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="text_generation")
        async for content in llm.execute():
            assert content == "Generated text"

2025-04-15 10:40:16,170 - INFO - ---------------
2025-04-15 10:40:17,029 - INFO - Test Result 1- False
2025-04-15 10:40:17,029 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:493: in importtestmodule
    mod = import_path(
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\pathlib.py:587: in import_path
    importlib.import_module(module_name)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:176: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:358: in _rewrite_test
    co = compile(tree, strfn, "exec", dont_inherit=True)
E     File "C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py", line 10
E       async for content in llm.execute():
E       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   SyntaxError: 'async for' outside async function
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 0.22s
2025-04-15 10:40:19,550 - INFO - TEST CASE 5 Retry 1
2025-04-15 10:40:19,551 - INFO - ---------------
2025-04-15 10:40:19,551 - INFO - 
import pytest
from unittest.mock import MagicMock, patch
import pytest
import asyncio
from unittest.mock import MagicMock, patch
from theory_evaluation.llm_handler import OpenAI_llm

@pytest.mark.asyncio
async def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "Generated text"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="text_generation")
        async for content in llm.execute():
            assert content == "Generated text"

2025-04-15 10:40:19,551 - INFO - ---------------
2025-04-15 10:40:22,073 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:40:22,074 - INFO - New import Statements 2- import asyncio
from theory_evaluation.llm_handler import OpenAI_llm
2025-04-15 10:40:22,074 - INFO - Test Result 2- True
2025-04-15 10:40:22,074 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp\temp.py:8
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:8: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_execute_text_generation
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 2 warnings in 1.02s
2025-04-15 10:40:22,074 - INFO - 

2025-04-15 10:40:22,074 - INFO - TEST CASE 6 Retry 0
2025-04-15 10:40:22,075 - INFO - ---------------
2025-04-15 10:40:22,075 - INFO - 
import asyncio
from theory_evaluation.llm_handler import OpenAI_llm
def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "Vision response"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image")
        async for content in llm.execute():
            assert content == "Vision response"

2025-04-15 10:40:22,075 - INFO - ---------------
2025-04-15 10:40:22,955 - INFO - Test Result 1- False
2025-04-15 10:40:22,955 - INFO - Test Error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:493: in importtestmodule
    mod = import_path(
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\pathlib.py:587: in import_path
    importlib.import_module(module_name)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:176: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\assertion\rewrite.py:358: in _rewrite_test
    co = compile(tree, strfn, "exec", dont_inherit=True)
E     File "C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py", line 10
E       async for content in llm.execute():
E       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   SyntaxError: 'async for' outside async function
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 0.18s
2025-04-15 10:40:25,224 - INFO - TEST CASE 6 Retry 1
2025-04-15 10:40:25,225 - INFO - ---------------
2025-04-15 10:40:25,225 - INFO - 
import asyncio
from theory_evaluation.llm_handler import OpenAI_llm
from unittest.mock import MagicMock, patch
import pytest
import asyncio

@pytest.mark.asyncio
async def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "Vision response"
    
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=MagicMock) as mock_client:
        mock_client.chat.completions.create.return_value = mock_response
        llm = OpenAI_llm(message="Test message", mode="vision", image_input="mock_image")
        responses = []
        async for content in llm.execute():
            responses.append(content)
        assert responses == ["Vision response"]

2025-04-15 10:40:25,225 - INFO - ---------------
2025-04-15 10:40:28,896 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:40:28,897 - INFO - New import Statements 2- from unittest.mock import MagicMock, patch
import pytest
2025-04-15 10:40:28,897 - INFO - Test Result 2- True
2025-04-15 10:40:28,898 - INFO - Test Error 2 - 
s                                                                        [100%]
============================== warnings summary ===============================
temp\temp.py:7
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:7: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.asyncio

temp/temp.py::test_execute_vision
  C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\_pytest\python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
  You need to install a suitable plugin for your async framework, for example:
    - anyio
    - pytest-asyncio
    - pytest-tornasync
    - pytest-trio
    - pytest-twisted
    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
1 skipped, 2 warnings in 1.74s
2025-04-15 10:40:28,901 - INFO - [1mEnd Processing file: theory_evaluation\llm_handler.py[0m

2025-04-15 10:40:28,902 - INFO - [1mStart Processing file: theory_evaluation\llm_utils.py[0m
2025-04-15 10:40:35,361 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:40:35,364 - INFO - Hello World 1
2025-04-15 10:40:35,364 - INFO - Hello World 1 - formatted_prompt - Extract pytest fixture from the following unit test file:
```
import os
import pytest
import yaml
import re
from unittest.mock import patch, mock_open
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings

def test_initialise_prompt_success():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$key} prompt."
    expected_prompt = "This is a value prompt."

    with patch("theory_evaluation.llm_utils.open", mock_open(read_data=config_yaml)) as mock_file:
        with patch("theory_evaluation.llm_utils.yaml.load", return_value={"key": "value"}):
            with patch("theory_evaluation.llm_utils.re.finditer", return_value=re.finditer(r"\{\$(\w+)\}", prompt_txt)):
                with patch("theory_evaluation.llm_utils.re.sub", side_effect=lambda pattern, repl, string: string.replace("{$key}", "value")):
                    result = initialise_prompt(agent)
                    assert result == expected_prompt
                    mock_file.assert_has_calls([
                        patch("theory_evaluation.llm_utils.open", f"./theory_evaluation/evaluator/prompts/{agent}/config.yaml"),
                        patch("theory_evaluation.llm_utils.open", f"./theory_evaluation/evaluator/prompts/{agent}/prompt.txt", "r")
                    ])

def test_initialise_prompt_file_not_found():
    agent = "non_existent_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_prompt(agent)
        assert result is None

def test_initialise_settings_success():
    agent = "test_agent"
    settings_yaml = "key: value"
    expected_settings = {"key": "value"}

    with patch("theory_evaluation.llm_utils.open", mock_open(read_data=settings_yaml)) as mock_file:
        with patch("theory_evaluation.llm_utils.yaml.safe_load", return_value=expected_settings):
            result = initialise_settings(agent)
            assert result == expected_settings
            mock_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/llm_settings.yaml")

def test_initialise_settings_file_not_found():
    agent = "non_existent_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_settings(agent)
        assert result is None
```
. Output only pytest functions. No Markdown formatting, explanations, or docstrings. Do NOT wrap your output in backticks
2025-04-15 10:40:38,192 - INFO - pytest_fixture - import pytest
from unittest.mock import patch, mock_open

@pytest.fixture
def mock_open_success():
    with patch("theory_evaluation.llm_utils.open", mock_open(read_data="key: value")) as mock_file:
        yield mock_file

@pytest.fixture
def mock_open_file_not_found():
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        yield

@pytest.fixture
def mock_yaml_load():
    with patch("theory_evaluation.llm_utils.yaml.load", return_value={"key": "value"}):
        yield

@pytest.fixture
def mock_yaml_safe_load():
    with patch("theory_evaluation.llm_utils.yaml.safe_load", return_value={"key": "value"}):
        yield

@pytest.fixture
def mock_re_finditer():
    with patch("theory_evaluation.llm_utils.re.finditer", return_value=re.finditer(r"\{\$(\w+)\}", "This is a {$key} prompt.")):
        yield

@pytest.fixture
def mock_re_sub():
    with patch("theory_evaluation.llm_utils.re.sub", side_effect=lambda pattern, repl, string: string.replace("{$key}", "value")):
        yield
2025-04-15 10:40:38,193 - INFO - 

2025-04-15 10:40:38,193 - INFO - TEST CASE 1 Retry 0
2025-04-15 10:40:38,193 - INFO - ---------------
2025-04-15 10:40:38,194 - INFO - 
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
def test_initialise_prompt_success():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$key} prompt."
    expected_prompt = "This is a value prompt."

2025-04-15 10:40:38,194 - INFO - ---------------
2025-04-15 10:40:39,002 - INFO - Test Result 1- True
2025-04-15 10:40:39,002 - INFO - Test Error 1 - .                                                                        [100%]
1 passed in 0.06s
2025-04-15 10:40:39,002 - INFO - 

2025-04-15 10:40:39,002 - INFO - TEST CASE 2 Retry 0
2025-04-15 10:40:39,002 - INFO - ---------------
2025-04-15 10:40:39,002 - INFO - 
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
def test_initialise_prompt_file_not_found():
    agent = "non_existent_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_prompt(agent)
        assert result is None

2025-04-15 10:40:39,002 - INFO - ---------------
2025-04-15 10:40:39,657 - INFO - Test Result 1- False
2025-04-15 10:40:39,657 - INFO - Test Error 1 - F                                                                        [100%]
================================== FAILURES ===================================
____________________ test_initialise_prompt_file_not_found ____________________
temp\temp.py:7: in test_initialise_prompt_file_not_found
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_prompt_file_not_found - NameError: name ...
1 failed in 0.06s
2025-04-15 10:40:40,654 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:40:40,655 - INFO - TEST CASE 2 Retry 1
2025-04-15 10:40:40,655 - INFO - ---------------
2025-04-15 10:40:40,655 - INFO - 
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
from unittest.mock import patch

def test_initialise_prompt_file_not_found():
    agent = "non_existent_agent"
    with patch("builtins.open", side_effect=FileNotFoundError):
        result = initialise_prompt(agent)
        assert result is None

2025-04-15 10:40:40,655 - INFO - ---------------
2025-04-15 10:40:42,187 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 10:40:42,188 - INFO - New import Statements 2- import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
from unittest.mock import patch
2025-04-15 10:40:42,188 - INFO - Test Result 2- True
2025-04-15 10:40:42,188 - INFO - Test Error 2 - 
.                                                                        [100%]
1 passed in 0.11s
2025-04-15 10:40:42,188 - INFO - 

2025-04-15 10:40:42,189 - INFO - TEST CASE 3 Retry 0
2025-04-15 10:40:42,189 - INFO - ---------------
2025-04-15 10:40:42,189 - INFO - 
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
from unittest.mock import patch
def test_initialise_settings_success():
    agent = "test_agent"
    settings_yaml = "key: value"
    expected_settings = {"key": "value"}

2025-04-15 10:40:42,189 - INFO - ---------------
2025-04-15 10:40:42,957 - INFO - Test Result 1- True
2025-04-15 10:40:42,957 - INFO - Test Error 1 - .                                                                        [100%]
1 passed in 0.09s
2025-04-15 10:40:42,957 - INFO - 

2025-04-15 10:40:42,957 - INFO - TEST CASE 4 Retry 0
2025-04-15 10:40:42,957 - INFO - ---------------
2025-04-15 10:40:42,957 - INFO - 
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
from unittest.mock import patch
def test_initialise_settings_file_not_found():
    agent = "non_existent_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_settings(agent)
        assert result is None

2025-04-15 10:40:42,957 - INFO - ---------------
2025-04-15 10:40:43,618 - INFO - Test Result 1- True
2025-04-15 10:40:43,618 - INFO - Test Error 1 - .                                                                        [100%]
1 passed in 0.11s
2025-04-15 10:40:43,619 - INFO - [1mEnd Processing file: theory_evaluation\llm_utils.py[0m

2025-04-15 10:40:43,619 - INFO - [1mStart Processing file: theory_evaluation\__init__.py[0m
2025-04-15 10:40:43,620 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

