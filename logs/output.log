2025-04-17 11:45:29,731 - INFO - Loading environment variables start
2025-04-17 11:45:29,736 - INFO - Loading environment variables completes
2025-04-17 11:45:29,736 - INFO - Initialising of LLM start
2025-04-17 11:45:30,065 - INFO - Initialising of LLM completes
2025-04-17 11:45:30,065 - INFO - Getting python file starts
2025-04-17 11:45:30,065 - INFO - Getting python file completes
2025-04-17 11:45:30,066 - INFO - 
Start Processing file: theory_evaluation\circle_utils.py
2025-04-17 11:45:30,066 - INFO - Extraction of function and class start
2025-04-17 11:45:30,066 - INFO - extraction of function and class complete
2025-04-17 11:45:30,066 - INFO - Generate Unit Test Case starts
2025-04-17 11:45:30,066 - INFO - Extract unique import start
2025-04-17 11:45:31,548 - INFO - Extract unique import complete
2025-04-17 11:45:31,550 - INFO - Update relative import start
2025-04-17 11:45:31,552 - INFO - Update relative import complete
2025-04-17 11:45:34,961 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 11:45:34,961 - INFO - Generate Unit Test Case complete
2025-04-17 11:45:34,965 - INFO - run_each_pytest_function_individually start
2025-04-17 11:45:38,079 - INFO - Number of test case to process - 6
2025-04-17 11:45:38,080 - INFO - 
TEST CASE 1 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_area_positive_radius():
    radius = 5
    expected_area = math.pi * radius ** 2
    assert circle_area(radius) == pytest.approx(expected_area)

---------------
2025-04-17 11:45:38,933 - INFO - TEST CASE 1 Retry 0 - Result - Passed
2025-04-17 11:45:38,933 - INFO - 
TEST CASE 2 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_area_zero_radius():
    radius = 0
    expected_area = 0
    assert circle_area(radius) == pytest.approx(expected_area)

---------------
2025-04-17 11:45:39,544 - INFO - TEST CASE 2 Retry 0 - Result - Passed
2025-04-17 11:45:39,544 - INFO - 
TEST CASE 3 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_area_negative_radius():
    with pytest.raises(ValueError, match="Radius cannot be negative."):
        circle_area(-1)

---------------
2025-04-17 11:45:40,077 - INFO - TEST CASE 3 Retry 0 - Result - Passed
2025-04-17 11:45:40,077 - INFO - 
TEST CASE 4 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_circumference_positive_radius():
    radius = 5
    expected_circumference = 2 * math.pi * radius
    assert circle_circumference(radius) == pytest.approx(expected_circumference)

---------------
2025-04-17 11:45:40,600 - INFO - TEST CASE 4 Retry 0 - Result - Passed
2025-04-17 11:45:40,600 - INFO - 
TEST CASE 5 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_circumference_zero_radius():
    radius = 0
    expected_circumference = 0
    assert circle_circumference(radius) == pytest.approx(expected_circumference)

---------------
2025-04-17 11:45:41,146 - INFO - TEST CASE 5 Retry 0 - Result - Passed
2025-04-17 11:45:41,146 - INFO - 
TEST CASE 6 Retry 0
---------------
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest



def test_circle_circumference_negative_radius():
    with pytest.raises(ValueError, match="Radius cannot be negative."):
        circle_circumference(-1)

---------------
2025-04-17 11:45:41,793 - INFO - TEST CASE 6 Retry 0 - Result - Passed
2025-04-17 11:45:41,793 - INFO - Before Improvement
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest


import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest


def test_circle_area_positive_radius():
    radius = 5
    expected_area = math.pi * radius ** 2
    assert circle_area(radius) == pytest.approx(expected_area)

def test_circle_area_zero_radius():
    radius = 0
    expected_area = 0
    assert circle_area(radius) == pytest.approx(expected_area)

def test_circle_area_negative_radius():
    with pytest.raises(ValueError, match="Radius cannot be negative."):
        circle_area(-1)

def test_circle_circumference_positive_radius():
    radius = 5
    expected_circumference = 2 * math.pi * radius
    assert circle_circumference(radius) == pytest.approx(expected_circumference)

def test_circle_circumference_zero_radius():
    radius = 0
    expected_circumference = 0
    assert circle_circumference(radius) == pytest.approx(expected_circumference)

def test_circle_circumference_negative_radius():
    with pytest.raises(ValueError, match="Radius cannot be negative."):
        circle_circumference(-1)

2025-04-17 11:45:45,004 - INFO - After Improvement
import math
from theory_evaluation.circle_utils import circle_area, circle_circumference
import pytest

def test_circle_area_positive_radius():
    radius = 5
    expected_area = math.pi * radius ** 2
    assert circle_area(radius) == pytest.approx(expected_area)

def test_circle_area_zero_radius():
    radius = 0
    expected_area = 0
    assert circle_area(radius) == pytest.approx(expected_area)

def test_circle_area_negative_radius():
    with pytest.raises(ValueError, match="Radius cannot be negative."):
        circle_area(-1)

def test_circle_circumference_positive_radius():
    radius = 5
    expected_circumference = 2 * math.pi * radius
    assert circle_circumference(radius) == pytest.approx(expected_circumference)

def test_circle_circumference_zero_radius():
    radius = 0
    expected_circumference = 0
    assert circle_circumference(radius) == pytest.approx(expected_circumference)

def test_circle_circumference_negative_radius():
    with pytest.raises(ValueError, match="Radius cannot be negative."):
        circle_circumference(-1)
2025-04-17 11:45:45,941 - INFO - Improvement of test cases processed successfully
2025-04-17 11:45:45,941 - INFO - run_each_pytest_function_individually complete
2025-04-17 11:45:45,941 - INFO - End Processing file: theory_evaluation\circle_utils.py

2025-04-17 11:45:45,941 - INFO - 
Start Processing file: theory_evaluation\llm_handler.py
2025-04-17 11:45:45,941 - INFO - Extraction of function and class start
2025-04-17 11:45:45,942 - INFO - extraction of function and class complete
2025-04-17 11:45:45,942 - INFO - Generate Unit Test Case starts
2025-04-17 11:45:45,942 - INFO - Extract unique import start
2025-04-17 11:45:46,595 - INFO - Extract unique import complete
2025-04-17 11:45:46,596 - INFO - Update relative import start
2025-04-17 11:45:46,597 - INFO - Update relative import complete
2025-04-17 11:46:02,932 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 11:46:02,932 - INFO - Generate Unit Test Case complete
2025-04-17 11:46:02,937 - INFO - run_each_pytest_function_individually start
2025-04-17 11:46:11,748 - INFO - Number of test case to process - 6
2025-04-17 11:46:11,749 - INFO - 
TEST CASE 1 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        assert llm.message == "Test message"
        assert llm.output == "json"
        assert llm.mode == "text_generation"
        assert llm.verbose is True
        assert llm.azure_endpoint == "mock_AZURE_OPENAI_ENDPOINT_SWEDEN"
        assert llm.api_version == "mock_AZURE_OPENAI_API_VERSION"
        assert llm.client is not None

---------------
2025-04-17 11:46:14,076 - INFO - TEST CASE 1 Retry 0 - Result - Failed
2025-04-17 11:46:14,076 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:23: in test_openai_llm_initialization
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - NameError: name 'patch'...
1 failed in 1.18s
2025-04-17 11:46:16,465 - INFO - 
TEST CASE 1 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

from unittest.mock import patch

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        assert llm.message == "Test message"
        assert llm.output == "json"
        assert llm.mode == "text_generation"
        assert llm.verbose is True
        assert llm.azure_endpoint == "mock_AZURE_OPENAI_ENDPOINT_SWEDEN"
        assert llm.api_version == "mock_AZURE_OPENAI_API_VERSION"
        assert llm.client is not None

---------------
2025-04-17 11:46:18,675 - INFO - TEST CASE 1 Retry 1 - Result - Passed
2025-04-17 11:46:18,675 - INFO - 
TEST CASE 2 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm, '_run', new_callable=AsyncMock) as mock_run:
            mock_run.return_value.__aiter__.return_value = [{'answer': 'mock_answer'}]
            responses = [response async for response in llm.execute()]
            assert responses == [{'answer': 'mock_answer'}]

---------------
2025-04-17 11:46:20,524 - INFO - TEST CASE 2 Retry 0 - Result - Failed
2025-04-17 11:46:20,525 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:23: in test_openai_llm_execute_text_generation
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - NameError: nam...
1 failed in 1.04s
2025-04-17 11:46:22,089 - INFO - 
TEST CASE 2 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

import pytest
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm, '_run', new_callable=AsyncMock) as mock_run:
            mock_run.return_value.__aiter__.return_value = [{'answer': 'mock_answer'}]
            responses = [response async for response in llm.execute()]
            assert responses == [{'answer': 'mock_answer'}]

---------------
2025-04-17 11:46:24,246 - INFO - TEST CASE 2 Retry 1 - Result - Failed
2025-04-17 11:46:24,247 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:37: in test_openai_llm_execute_text_generation
    assert responses == [{'answer': 'mock_answer'}]
E   assert ["Error: 'asy...ot coroutine"] == [{'answer': 'mock_answer'}]
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != {'answer': 'mock_answer'}
E     Use -v to get more diff
---------------------------- Captured stdout call -----------------------------
{
    "role": "system",
    "content": "Test message"
}
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_execute_text_generation
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - assert ["Error...
1 failed, 1 warning in 1.33s
2025-04-17 11:46:26,710 - INFO - 
TEST CASE 2 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

import pytest
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm, '_run', new_callable=AsyncMock) as mock_run:
            mock_run.return_value = AsyncMock()
            mock_run.return_value.__aiter__.return_value = [{'answer': 'mock_answer'}]
            responses = [response async for response in llm.execute()]
            assert responses == [{'answer': 'mock_answer'}]

---------------
2025-04-17 11:46:28,767 - INFO - TEST CASE 2 Retry 2 - Result - Failed
2025-04-17 11:46:28,767 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_execute_text_generation ___________________
temp\temp.py:38: in test_openai_llm_execute_text_generation
    assert responses == [{'answer': 'mock_answer'}]
E   assert ["Error: 'asy...ot coroutine"] == [{'answer': 'mock_answer'}]
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != {'answer': 'mock_answer'}
E     Use -v to get more diff
---------------------------- Captured stdout call -----------------------------
{
    "role": "system",
    "content": "Test message"
}
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_execute_text_generation
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_text_generation - assert ["Error...
1 failed, 1 warning in 1.26s
2025-04-17 11:46:30,908 - INFO - Failed after all retries for test case 2
2025-04-17 11:46:30,908 - INFO - 
TEST CASE 3 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            image_input="mock_image_input",
            output="json",
            mode="vision",
            verbose=True
        )
        with patch.object(llm, '_run', new_callable=AsyncMock) as mock_run:
            mock_run.return_value.__aiter__.return_value = [{'answer': 'mock_answer'}]
            responses = [response async for response in llm.execute()]
            assert responses == [{'answer': 'mock_answer'}]

---------------
2025-04-17 11:46:32,790 - INFO - TEST CASE 3 Retry 0 - Result - Failed
2025-04-17 11:46:32,790 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:23: in test_openai_llm_execute_vision
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - NameError: name 'patch'...
1 failed in 1.10s
2025-04-17 11:46:35,009 - INFO - 
TEST CASE 3 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            image_input="mock_image_input",
            output="json",
            mode="vision",
            verbose=True
        )
        with patch.object(llm, '_run', new_callable=AsyncMock) as mock_run:
            mock_run.return_value.__aiter__.return_value = [{'answer': 'mock_answer'}]
            responses = [response async for response in llm.execute()]
            assert responses == [{'answer': 'mock_answer'}]

---------------
2025-04-17 11:46:37,252 - INFO - TEST CASE 3 Retry 1 - Result - Failed
2025-04-17 11:46:37,253 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:38: in test_openai_llm_execute_vision
    assert responses == [{'answer': 'mock_answer'}]
E   assert ["Error: 'asy...ot coroutine"] == [{'answer': 'mock_answer'}]
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != {'answer': 'mock_answer'}
E     Use -v to get more diff
---------------------------- Captured stdout call -----------------------------
{
    "role": "system",
    "content": "Test message"
}
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_execute_vision
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - assert ["Error: 'asy......
1 failed, 1 warning in 1.51s
2025-04-17 11:46:39,504 - INFO - 
TEST CASE 3 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            image_input="mock_image_input",
            output="json",
            mode="vision",
            verbose=True
        )
        with patch.object(llm, '_run', new_callable=AsyncMock) as mock_run:
            mock_run.return_value = AsyncMock()
            mock_run.return_value.__aiter__.return_value = [{'answer': 'mock_answer'}]
            responses = [response async for response in llm.execute()]
            assert responses == [{'answer': 'mock_answer'}]

---------------
2025-04-17 11:46:41,836 - INFO - TEST CASE 3 Retry 2 - Result - Failed
2025-04-17 11:46:41,836 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_execute_vision ________________________
temp\temp.py:39: in test_openai_llm_execute_vision
    assert responses == [{'answer': 'mock_answer'}]
E   assert ["Error: 'asy...ot coroutine"] == [{'answer': 'mock_answer'}]
E     
E     At index 0 diff: "Error: 'async for' requires an object with __aiter__ method, got coroutine" != {'answer': 'mock_answer'}
E     Use -v to get more diff
---------------------------- Captured stdout call -----------------------------
{
    "role": "system",
    "content": "Test message"
}
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_execute_vision
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py:164: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async for response in self._run(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_execute_vision - assert ["Error: 'asy......
1 failed, 1 warning in 1.49s
2025-04-17 11:46:44,184 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 11:46:44,185 - INFO - Failed after all retries for test case 3
2025-04-17 11:46:44,185 - INFO - 
TEST CASE 4 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

@pytest.mark.asyncio
async def test_openai_llm_openai_json_completion():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content=json.dumps({'answer': 'mock_answer'})))]
            content = await llm._OpenAI_JSON_Completion()
            assert content == {'answer': 'mock_answer'}

---------------
2025-04-17 11:46:46,221 - INFO - TEST CASE 4 Retry 0 - Result - Failed
2025-04-17 11:46:46,221 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_json_completion ____________________
temp\temp.py:23: in test_openai_llm_openai_json_completion
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_json_completion - NameError: name...
1 failed in 1.26s
2025-04-17 11:46:48,524 - INFO - 
TEST CASE 4 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

from unittest.mock import patch, AsyncMock
import pytest
import json

@pytest.mark.asyncio
async def test_openai_llm_openai_json_completion():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content=json.dumps({'answer': 'mock_answer'})))]
            content = await llm._OpenAI_JSON_Completion()
            assert content == {'answer': 'mock_answer'}

---------------
2025-04-17 11:46:50,713 - INFO - TEST CASE 4 Retry 1 - Result - Failed
2025-04-17 11:46:50,713 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_json_completion ____________________
temp\temp.py:38: in test_openai_llm_openai_json_completion
    assert content == {'answer': 'mock_answer'}
E   AssertionError: assert None == {'answer': 'mock_answer'}
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_JSON_Completion: 'coroutine' object has no attribute 'choices'
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_openai_json_completion
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:37: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    content = await llm._OpenAI_JSON_Completion()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_json_completion - AssertionError:...
1 failed, 1 warning in 1.36s
2025-04-17 11:46:52,925 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 11:46:52,925 - INFO - 
TEST CASE 4 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

from unittest.mock import patch, AsyncMock
import pytest
import json

@pytest.mark.asyncio
async def test_openai_llm_openai_json_completion():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = AsyncMock(choices=[AsyncMock(message=AsyncMock(content=json.dumps({'answer': 'mock_answer'})))])
            content = await llm._OpenAI_JSON_Completion()
            assert content == {'answer': 'mock_answer'}

---------------
2025-04-17 11:46:55,036 - INFO - TEST CASE 4 Retry 2 - Result - Failed
2025-04-17 11:46:55,036 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_json_completion ____________________
temp\temp.py:38: in test_openai_llm_openai_json_completion
    assert content == {'answer': 'mock_answer'}
E   AssertionError: assert None == {'answer': 'mock_answer'}
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_JSON_Completion: 'coroutine' object has no attribute 'choices'
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_openai_json_completion
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:37: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    content = await llm._OpenAI_JSON_Completion()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_json_completion - AssertionError:...
1 failed, 1 warning in 1.37s
2025-04-17 11:46:57,747 - INFO - Failed after all retries for test case 4
2025-04-17 11:46:57,748 - INFO - 
TEST CASE 5 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

@pytest.mark.asyncio
async def test_openai_llm_openai_streaming():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="stream",
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value.__aiter__.return_value = [AsyncMock(choices=[AsyncMock(delta=AsyncMock(content='mock_content'))])]
            stream_data = [chunk async for chunk in llm._OpenAI_Streaming()]
            assert stream_data == ['mock_content']

---------------
2025-04-17 11:46:59,759 - INFO - TEST CASE 5 Retry 0 - Result - Failed
2025-04-17 11:46:59,759 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_openai_streaming _______________________
temp\temp.py:23: in test_openai_llm_openai_streaming
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_streaming - NameError: name 'patc...
1 failed in 1.12s
2025-04-17 11:47:05,672 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 11:47:05,672 - INFO - 
TEST CASE 5 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

import pytest
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_openai_llm_openai_streaming():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="stream",
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value.__aiter__.return_value = [AsyncMock(choices=[AsyncMock(delta=AsyncMock(content='mock_content'))])]
            stream_data = [chunk async for chunk in llm._OpenAI_Streaming()]
            assert stream_data == ['mock_content']

---------------
2025-04-17 11:47:08,080 - INFO - TEST CASE 5 Retry 1 - Result - Failed
2025-04-17 11:47:08,080 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_openai_streaming _______________________
temp\temp.py:37: in test_openai_llm_openai_streaming
    assert stream_data == ['mock_content']
E   assert ['Failed in _...not iterable'] == ['mock_content']
E     
E     At index 0 diff: "Failed in _OpenAI_Streaming: 'coroutine' object is not iterable" != 'mock_content'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_openai_streaming
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    stream_data = [chunk async for chunk in llm._OpenAI_Streaming()]
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_streaming - assert ['Failed in _....
1 failed, 1 warning in 1.58s
2025-04-17 11:47:10,448 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 11:47:10,448 - INFO - 
TEST CASE 5 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

import pytest
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_openai_llm_openai_streaming():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="stream",
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value.__aiter__.return_value = [AsyncMock(choices=[AsyncMock(delta=AsyncMock(content='mock_content'))])]
            stream_data = [chunk async for chunk in llm._OpenAI_Streaming()]
            assert stream_data == ['mock_content']

---------------
2025-04-17 11:47:12,593 - INFO - TEST CASE 5 Retry 2 - Result - Failed
2025-04-17 11:47:12,593 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_openai_llm_openai_streaming _______________________
temp\temp.py:37: in test_openai_llm_openai_streaming
    assert stream_data == ['mock_content']
E   assert ['Failed in _...not iterable'] == ['mock_content']
E     
E     At index 0 diff: "Failed in _OpenAI_Streaming: 'coroutine' object is not iterable" != 'mock_content'
E     Use -v to get more diff
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_openai_streaming
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    stream_data = [chunk async for chunk in llm._OpenAI_Streaming()]
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_streaming - assert ['Failed in _....
1 failed, 1 warning in 1.28s
2025-04-17 11:47:21,397 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 11:47:21,397 - INFO - Failed after all retries for test case 5
2025-04-17 11:47:21,398 - INFO - 
TEST CASE 6 Retry 0
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

@pytest.mark.asyncio
async def test_openai_llm_openai_chat_completion():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output=None,
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content='mock_content'))]
            content = await llm._OpenAI_Chat_Completion()
            assert content == 'mock_content'

---------------
2025-04-17 11:47:23,257 - INFO - TEST CASE 6 Retry 0 - Result - Failed
2025-04-17 11:47:23,257 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_chat_completion ____________________
temp\temp.py:23: in test_openai_llm_openai_chat_completion
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_chat_completion - NameError: name...
1 failed in 1.07s
2025-04-17 11:47:25,466 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 11:47:25,467 - INFO - 
TEST CASE 6 Retry 1
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_openai_llm_openai_chat_completion():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output=None,
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content='mock_content'))]
            content = await llm._OpenAI_Chat_Completion()
            assert content == 'mock_content'

---------------
2025-04-17 11:47:27,648 - INFO - TEST CASE 6 Retry 1 - Result - Failed
2025-04-17 11:47:27,649 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_chat_completion ____________________
temp\temp.py:37: in test_openai_llm_openai_chat_completion
    assert content == 'mock_content'
E   AssertionError: assert None == 'mock_content'
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_Chat_Completion: 'coroutine' object has no attribute 'choices'
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_openai_chat_completion
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:36: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    content = await llm._OpenAI_Chat_Completion()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_chat_completion - AssertionError:...
1 failed, 1 warning in 1.31s
2025-04-17 11:47:30,895 - INFO - 
TEST CASE 6 Retry 2
---------------
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

from unittest.mock import patch, AsyncMock
import pytest

@pytest.mark.asyncio
async def test_openai_llm_openai_chat_completion():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output=None,
            mode="text_generation",
            verbose=True
        )
        with patch.object(llm.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = AsyncMock()
            mock_create.return_value.choices = [AsyncMock(message=AsyncMock(content='mock_content'))]
            content = await llm._OpenAI_Chat_Completion()
            assert content == 'mock_content'

---------------
2025-04-17 11:47:33,231 - INFO - TEST CASE 6 Retry 2 - Result - Failed
2025-04-17 11:47:33,232 - INFO - Test Error - c:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\.venv\Lib\site-packages\pytest_asyncio\plugin.py:217: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
F                                                                        [100%]
================================== FAILURES ===================================
___________________ test_openai_llm_openai_chat_completion ____________________
temp\temp.py:38: in test_openai_llm_openai_chat_completion
    assert content == 'mock_content'
E   AssertionError: assert None == 'mock_content'
---------------------------- Captured stdout call -----------------------------
Failed in _OpenAI_Chat_Completion: 'coroutine' object has no attribute 'choices'
============================== warnings summary ===============================
temp/temp.py::test_openai_llm_openai_chat_completion
  C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py:37: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    content = await llm._OpenAI_Chat_Completion()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_openai_chat_completion - AssertionError:...
1 failed, 1 warning in 1.45s
2025-04-17 11:47:36,573 - INFO - Failed after all retries for test case 6
2025-04-17 11:47:36,574 - INFO - Before Improvement
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm
import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import OpenAI_llm
import pytest

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm
from unittest.mock import patch

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        assert llm.message == "Test message"
        assert llm.output == "json"
        assert llm.mode == "text_generation"
        assert llm.verbose is True
        assert llm.azure_endpoint == "mock_AZURE_OPENAI_ENDPOINT_SWEDEN"
        assert llm.api_version == "mock_AZURE_OPENAI_API_VERSION"
        assert llm.client is not None

2025-04-17 11:47:40,947 - INFO - After Improvement
import asyncio
from theory_evaluation.llm_handler import OpenAI_llm
import pytest
from unittest.mock import patch

@pytest.fixture
def mock_openai_llm():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
    return llm

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message="Test message",
            output="json",
            mode="text_generation",
            verbose=True
        )
        assert llm.message == "Test message"
        assert llm.output == "json"
        assert llm.mode == "text_generation"
        assert llm.verbose is True
        assert llm.azure_endpoint == "mock_AZURE_OPENAI_ENDPOINT_SWEDEN"
        assert llm.api_version == "mock_AZURE_OPENAI_API_VERSION"
        assert llm.client is not None
2025-04-17 11:47:43,143 - INFO - Improvement of test cases processed successfully
2025-04-17 11:47:43,143 - INFO - run_each_pytest_function_individually complete
2025-04-17 11:47:43,143 - ERROR - Failed processing theory_evaluation\llm_handler.py: 'str' object has no attribute 'relative_to'
2025-04-17 11:47:43,143 - INFO - End Processing file: theory_evaluation\llm_handler.py

2025-04-17 11:47:43,143 - INFO - 
Start Processing file: theory_evaluation\llm_utils.py
2025-04-17 11:47:43,143 - INFO - Extraction of function and class start
2025-04-17 11:47:43,143 - INFO - extraction of function and class complete
2025-04-17 11:47:43,143 - INFO - Generate Unit Test Case starts
2025-04-17 11:47:43,143 - INFO - Extract unique import start
2025-04-17 11:47:43,772 - INFO - Extract unique import complete
2025-04-17 11:47:43,772 - INFO - Update relative import start
2025-04-17 11:47:43,772 - INFO - Update relative import complete
2025-04-17 11:47:49,558 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-17 11:47:49,559 - INFO - Generate Unit Test Case complete
2025-04-17 11:47:49,563 - INFO - run_each_pytest_function_individually start
2025-04-17 11:47:59,178 - INFO - Number of test case to process - 5
2025-04-17 11:47:59,178 - INFO - 
TEST CASE 1 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_prompt_success():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$key} test."
    expected_prompt = "This is a value test."

---------------
2025-04-17 11:47:59,789 - INFO - TEST CASE 1 Retry 0 - Result - Passed
2025-04-17 11:47:59,789 - INFO - 
TEST CASE 2 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_prompt_missing_placeholder():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$missing_key} test."
    expected_prompt = "This is a {$missing_key} test."

---------------
2025-04-17 11:48:00,319 - INFO - TEST CASE 2 Retry 0 - Result - Passed
2025-04-17 11:48:00,319 - INFO - 
TEST CASE 3 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_prompt_file_not_found():
    agent = "non_existent_agent"

---------------
2025-04-17 11:48:00,969 - INFO - TEST CASE 3 Retry 0 - Result - Passed
2025-04-17 11:48:00,969 - INFO - 
TEST CASE 4 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_settings_success():
    agent = "test_agent"
    settings_yaml = "setting_key: setting_value"
    expected_settings = {"setting_key": "setting_value"}

---------------
2025-04-17 11:48:01,639 - INFO - TEST CASE 4 Retry 0 - Result - Passed
2025-04-17 11:48:01,639 - INFO - 
TEST CASE 5 Retry 0
---------------
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest



def test_initialise_settings_file_not_found():
    agent = "non_existent_agent"

---------------
2025-04-17 11:48:02,270 - INFO - TEST CASE 5 Retry 0 - Result - Passed
2025-04-17 11:48:02,270 - INFO - Before Improvement
import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest


import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest


def test_initialise_prompt_success():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$key} test."
    expected_prompt = "This is a value test."

def test_initialise_prompt_missing_placeholder():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$missing_key} test."
    expected_prompt = "This is a {$missing_key} test."

def test_initialise_prompt_file_not_found():
    agent = "non_existent_agent"

def test_initialise_settings_success():
    agent = "test_agent"
    settings_yaml = "setting_key: setting_value"
    expected_settings = {"setting_key": "setting_value"}

def test_initialise_settings_file_not_found():
    agent = "non_existent_agent"

2025-04-17 11:48:05,349 - INFO - After Improvement
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
import pytest


def test_initialise_prompt_success():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$key} test."
    expected_prompt = "This is a value test."

def test_initialise_prompt_missing_placeholder():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$missing_key} test."
    expected_prompt = "This is a {$missing_key} test."

def test_initialise_prompt_file_not_found():
    agent = "non_existent_agent"

def test_initialise_settings_success():
    agent = "test_agent"
    settings_yaml = "setting_key: setting_value"
    expected_settings = {"setting_key": "setting_value"}

def test_initialise_settings_file_not_found():
    agent = "non_existent_agent"
2025-04-17 11:48:06,034 - INFO - Improvement of test cases processed successfully
2025-04-17 11:48:06,035 - INFO - run_each_pytest_function_individually complete
2025-04-17 11:48:06,035 - INFO - End Processing file: theory_evaluation\llm_utils.py

2025-04-17 11:48:06,041 - INFO - +----+-----------------------------------+---------------------------+--------------------+---------------------+
|    | filename                          |   total_test_cases_passed |   total_test_cases |   percentage_passed |
+====+===================================+===========================+====================+=====================+
|  0 | theory_evaluation\circle_utils.py |                         6 |                  6 |            100      |
+----+-----------------------------------+---------------------------+--------------------+---------------------+
|  1 | theory_evaluation\llm_handler.py  |                         1 |                  6 |             16.6667 |
+----+-----------------------------------+---------------------------+--------------------+---------------------+
|  2 | theory_evaluation\llm_utils.py    |                         5 |                  5 |            100      |
+----+-----------------------------------+---------------------------+--------------------+---------------------+
