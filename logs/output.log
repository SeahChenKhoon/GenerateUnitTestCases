2025-04-15 08:25:13,513 - INFO - Loading environment variables...
2025-04-15 08:25:13,882 - INFO - [1mStart Processing file: theory_evaluation\llm_handler.py[0m
2025-04-15 08:25:25,608 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 08:25:25,615 - INFO - 

2025-04-15 08:25:25,616 - INFO - TEST CASE 1 Retry 1
2025-04-15 08:25:25,616 - INFO - ---------------
2025-04-15 08:25:25,616 - INFO - def test_openai_llm_initialization():
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="test_endpoint",
        message="Test message",
        image_input=None,
        api_version="v1",
        model_name="test_model",
        max_retries=2,
        output="json",
        mode="text_generation",
        config={"temperature": 0.5},
        verbose=True
    )
    assert llm.message == "Test message"
    assert llm.azure_endpoint == "test_endpoint"
    assert llm.api_version == "v1"
    assert llm.model_name == "test_model"
    assert llm.max_retries == 2
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True
2025-04-15 08:25:25,620 - INFO - ---------------
2025-04-15 08:25:27,872 - INFO - passed 1- False
2025-04-15 08:25:27,872 - INFO - test_case_error 1 - F                                                                        [100%]
================================== FAILURES ===================================
_______________________ test_openai_llm_initialization ________________________
temp\temp.py:8: in test_openai_llm_initialization
    llm = OpenAI_llm(
E   NameError: name 'OpenAI_llm' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_llm_initialization - NameError: name 'OpenAI...
1 failed in 1.43s
2025-04-15 08:25:27,872 - INFO - TEST CASE 1 Retry 2
2025-04-15 08:25:27,872 - INFO - ---------------
2025-04-15 08:25:27,873 - INFO - def test_openai_llm_initialization():
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="test_endpoint",
        message="Test message",
        image_input=None,
        api_version="v1",
        model_name="test_model",
        max_retries=2,
        output="json",
        mode="text_generation",
        config={"temperature": 0.5},
        verbose=True
    )
    assert llm.message == "Test message"
    assert llm.azure_endpoint == "test_endpoint"
    assert llm.api_version == "v1"
    assert llm.model_name == "test_model"
    assert llm.max_retries == 2
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True
2025-04-15 08:25:27,873 - INFO - ---------------
2025-04-15 08:25:29,913 - INFO - resolved_test_case 2- import sys
sys.path.append('temp')
from llm_handler import OpenAI_llm

def test_openai_llm_initialization():
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="test_endpoint",
        message="Test message",
        image_input=None,
        api_version="v1",
        model_name="test_model",
        max_retries=2,
        output="json",
        mode="text_generation",
        config={"temperature": 0.5},
        verbose=True
    )
    assert llm.message == "Test message"
    assert llm.azure_endpoint == "test_endpoint"
    assert llm.api_version == "v1"
    assert llm.model_name == "test_model"
    assert llm.max_retries == 2
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True
2025-04-15 08:25:32,114 - INFO - TEST CASE 1 Retry 3
2025-04-15 08:25:32,115 - INFO - ---------------
2025-04-15 08:25:32,115 - INFO - def test_openai_llm_initialization():
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="test_endpoint",
        message="Test message",
        image_input=None,
        api_version="v1",
        model_name="test_model",
        max_retries=2,
        output="json",
        mode="text_generation",
        config={"temperature": 0.5},
        verbose=True
    )
    assert llm.message == "Test message"
    assert llm.azure_endpoint == "test_endpoint"
    assert llm.api_version == "v1"
    assert llm.model_name == "test_model"
    assert llm.max_retries == 2
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True
2025-04-15 08:25:32,115 - INFO - ---------------
2025-04-15 08:25:34,931 - INFO - resolved_test_case 3- import sys
sys.path.append('temp')
from llm_handler import OpenAI_llm

def test_openai_llm_initialization():
    llm = OpenAI_llm(
        useAzureOpenAI=True,
        azure_endpoint="test_endpoint",
        message="Test message",
        image_input=None,
        api_version="v1",
        model_name="test_model",
        max_retries=2,
        output="json",
        mode="text_generation",
        config={"temperature": 0.5},
        verbose=True
    )
    assert llm.message == "Test message"
    assert llm.azure_endpoint == "test_endpoint"
    assert llm.api_version == "v1"
    assert llm.model_name == "test_model"
    assert llm.max_retries == 2
    assert llm.output == "json"
    assert llm.mode == "text_generation"
    assert llm.config == {"temperature": 0.5}
    assert llm.verbose is True
2025-04-15 08:25:37,036 - INFO - Failed after all retries for test case 1
2025-04-15 08:25:37,036 - INFO - 

2025-04-15 08:25:37,036 - INFO - TEST CASE 2 Retry 1
2025-04-15 08:25:37,036 - INFO - ---------------
2025-04-15 08:25:37,036 - INFO - def test_openai_json_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"answer": "42", "explanation": "Simple math"})))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:25:37,036 - INFO - ---------------
2025-04-15 08:25:39,661 - INFO - passed 1- False
2025-04-15 08:25:39,661 - INFO - test_case_error 1 - F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_json_completion _________________________
temp\temp.py:8: in test_openai_json_completion
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_json_completion - NameError: name 'patch' is...
1 failed in 1.77s
2025-04-15 08:25:39,661 - INFO - TEST CASE 2 Retry 2
2025-04-15 08:25:39,661 - INFO - ---------------
2025-04-15 08:25:39,661 - INFO - def test_openai_json_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"answer": "42", "explanation": "Simple math"})))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:25:39,661 - INFO - ---------------
2025-04-15 08:25:41,301 - INFO - resolved_test_case 2- from unittest.mock import patch, MagicMock, AsyncMock
import json

def test_openai_json_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"answer": "42", "explanation": "Simple math"})))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:25:43,299 - INFO - TEST CASE 2 Retry 3
2025-04-15 08:25:43,299 - INFO - ---------------
2025-04-15 08:25:43,299 - INFO - def test_openai_json_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"answer": "42", "explanation": "Simple math"})))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:25:43,299 - INFO - ---------------
2025-04-15 08:25:44,911 - INFO - resolved_test_case 3- from unittest.mock import patch, AsyncMock, MagicMock
import json

def test_openai_json_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"answer": "42", "explanation": "Simple math"})))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:25:46,895 - INFO - Failed after all retries for test case 2
2025-04-15 08:25:46,895 - INFO - 

2025-04-15 08:25:46,895 - INFO - TEST CASE 3 Retry 1
2025-04-15 08:25:46,895 - INFO - ---------------
2025-04-15 08:25:46,895 - INFO - def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_stream = [MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk1"))]),
                       MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk2"))])]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:25:46,895 - INFO - ---------------
2025-04-15 08:25:49,301 - INFO - passed 1- False
2025-04-15 08:25:49,301 - INFO - test_case_error 1 - F                                                                        [100%]
================================== FAILURES ===================================
____________________________ test_openai_streaming ____________________________
temp\temp.py:8: in test_openai_streaming
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_streaming - NameError: name 'patch' is not d...
1 failed in 1.44s
2025-04-15 08:25:49,301 - INFO - TEST CASE 3 Retry 2
2025-04-15 08:25:49,301 - INFO - ---------------
2025-04-15 08:25:49,301 - INFO - def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_stream = [MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk1"))]),
                       MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk2"))])]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:25:49,301 - INFO - ---------------
2025-04-15 08:25:50,736 - INFO - resolved_test_case 2- from unittest.mock import patch, MagicMock, AsyncMock

def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_stream = [MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk1"))]),
                       MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk2"))])]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:25:53,152 - INFO - TEST CASE 3 Retry 3
2025-04-15 08:25:53,152 - INFO - ---------------
2025-04-15 08:25:53,152 - INFO - def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_stream = [MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk1"))]),
                       MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk2"))])]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:25:53,152 - INFO - ---------------
2025-04-15 08:25:54,342 - INFO - resolved_test_case 3- from unittest.mock import patch, MagicMock, AsyncMock

def test_openai_streaming():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_stream = [MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk1"))]),
                       MagicMock(choices=[MagicMock(delta=MagicMock(content="chunk2"))])]
        mock_client.chat.completions.create.return_value = mock_stream
2025-04-15 08:25:56,150 - INFO - Failed after all retries for test case 3
2025-04-15 08:25:56,150 - INFO - 

2025-04-15 08:25:56,150 - INFO - TEST CASE 4 Retry 1
2025-04-15 08:25:56,150 - INFO - ---------------
2025-04-15 08:25:56,150 - INFO - def test_openai_chat_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content="Chat response"))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:25:56,150 - INFO - ---------------
2025-04-15 08:25:58,049 - INFO - passed 1- False
2025-04-15 08:25:58,049 - INFO - test_case_error 1 - F                                                                        [100%]
================================== FAILURES ===================================
_________________________ test_openai_chat_completion _________________________
temp\temp.py:8: in test_openai_chat_completion
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_openai_chat_completion - NameError: name 'patch' is...
1 failed in 1.18s
2025-04-15 08:25:58,049 - INFO - TEST CASE 4 Retry 2
2025-04-15 08:25:58,049 - INFO - ---------------
2025-04-15 08:25:58,049 - INFO - def test_openai_chat_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content="Chat response"))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:25:58,049 - INFO - ---------------
2025-04-15 08:25:59,309 - INFO - resolved_test_case 2- from unittest.mock import patch, MagicMock, AsyncMock

def test_openai_chat_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content="Chat response"))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:26:01,420 - INFO - TEST CASE 4 Retry 3
2025-04-15 08:26:01,420 - INFO - ---------------
2025-04-15 08:26:01,420 - INFO - def test_openai_chat_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content="Chat response"))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:26:01,420 - INFO - ---------------
2025-04-15 08:26:03,879 - INFO - resolved_test_case 3- from unittest.mock import patch, MagicMock, AsyncMock

def test_openai_chat_completion():
    with patch('theory_evaluation.llm_handler.OpenAI_llm.client', new_callable=AsyncMock) as mock_client:
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content="Chat response"))]
        mock_client.chat.completions.create.return_value = mock_response
2025-04-15 08:26:05,799 - INFO - Failed after all retries for test case 4
2025-04-15 08:26:05,799 - INFO - 

2025-04-15 08:26:05,799 - INFO - TEST CASE 5 Retry 1
2025-04-15 08:26:05,799 - INFO - ---------------
2025-04-15 08:26:05,799 - INFO - def test_execute_text_generation():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:05,799 - INFO - ---------------
2025-04-15 08:26:08,474 - INFO - passed 1- False
2025-04-15 08:26:08,474 - INFO - test_case_error 1 - F                                                                        [100%]
================================== FAILURES ===================================
________________________ test_execute_text_generation _________________________
temp\temp.py:8: in test_execute_text_generation
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_text_generation - NameError: name 'patch' i...
1 failed in 1.59s
2025-04-15 08:26:08,475 - INFO - TEST CASE 5 Retry 2
2025-04-15 08:26:08,475 - INFO - ---------------
2025-04-15 08:26:08,475 - INFO - def test_execute_text_generation():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:08,475 - INFO - ---------------
2025-04-15 08:26:09,674 - INFO - resolved_test_case 2- from unittest.mock import patch, AsyncMock

def test_execute_text_generation():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:11,814 - INFO - TEST CASE 5 Retry 3
2025-04-15 08:26:11,815 - INFO - ---------------
2025-04-15 08:26:11,815 - INFO - def test_execute_text_generation():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:11,815 - INFO - ---------------
2025-04-15 08:26:13,174 - INFO - resolved_test_case 3- from unittest.mock import patch, AsyncMock

def test_execute_text_generation():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:15,159 - INFO - Failed after all retries for test case 5
2025-04-15 08:26:15,159 - INFO - 

2025-04-15 08:26:15,159 - INFO - TEST CASE 6 Retry 1
2025-04-15 08:26:15,159 - INFO - ---------------
2025-04-15 08:26:15,159 - INFO - def test_execute_vision():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:15,160 - INFO - ---------------
2025-04-15 08:26:17,270 - INFO - passed 1- False
2025-04-15 08:26:17,270 - INFO - test_case_error 1 - F                                                                        [100%]
================================== FAILURES ===================================
_____________________________ test_execute_vision _____________________________
temp\temp.py:8: in test_execute_vision
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_execute_vision - NameError: name 'patch' is not def...
1 failed in 1.33s
2025-04-15 08:26:17,271 - INFO - TEST CASE 6 Retry 2
2025-04-15 08:26:17,271 - INFO - ---------------
2025-04-15 08:26:17,271 - INFO - def test_execute_vision():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:17,271 - INFO - ---------------
2025-04-15 08:26:18,344 - INFO - resolved_test_case 2- from unittest.mock import patch, AsyncMock

def test_execute_vision():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:20,624 - INFO - TEST CASE 6 Retry 3
2025-04-15 08:26:20,625 - INFO - ---------------
2025-04-15 08:26:20,625 - INFO - def test_execute_vision():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:20,625 - INFO - ---------------
2025-04-15 08:26:21,810 - INFO - resolved_test_case 3- from unittest.mock import patch, AsyncMock

def test_execute_vision():
    with patch('theory_evaluation.llm_handler.OpenAI_llm._run', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock()
        mock_run.return_value.__aiter__.return_value = iter(["response1", "response2"])
2025-04-15 08:26:23,544 - INFO - Failed after all retries for test case 6
2025-04-15 08:26:23,548 - INFO - [1mEnd Processing file: theory_evaluation\llm_handler.py[0m

2025-04-15 08:26:23,548 - INFO - [1mStart Processing file: theory_evaluation\llm_utils.py[0m
2025-04-15 08:26:29,464 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 08:26:29,469 - INFO - 

2025-04-15 08:26:29,470 - INFO - TEST CASE 1 Retry 1
2025-04-15 08:26:29,470 - INFO - ---------------
2025-04-15 08:26:29,471 - INFO - def test_initialise_prompt():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    config_yaml = "key: value"
    prompt_txt = "This is a {$key} prompt."
2025-04-15 08:26:29,471 - INFO - ---------------
2025-04-15 08:26:30,195 - INFO - passed 1- True
2025-04-15 08:26:30,195 - INFO - test_case_error 1 - .                                                                        [100%]
1 passed in 0.11s
2025-04-15 08:26:30,195 - INFO - 

2025-04-15 08:26:30,195 - INFO - TEST CASE 2 Retry 1
2025-04-15 08:26:30,195 - INFO - ---------------
2025-04-15 08:26:30,195 - INFO - def test_initialise_settings():
    agent = "test_agent"
    config_path = "./theory_evaluation/evaluator/prompts"
    settings_yaml = "key: value"
2025-04-15 08:26:30,195 - INFO - ---------------
2025-04-15 08:26:30,841 - INFO - passed 1- True
2025-04-15 08:26:30,841 - INFO - test_case_error 1 - .                                                                        [100%]
1 passed in 0.11s
2025-04-15 08:26:30,844 - INFO - [1mEnd Processing file: theory_evaluation\llm_utils.py[0m

2025-04-15 08:26:30,845 - INFO - [1mStart Processing file: theory_evaluation\__init__.py[0m
2025-04-15 08:26:30,846 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

