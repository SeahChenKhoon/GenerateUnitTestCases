2025-04-15 11:02:58,645 - INFO - Loading environment variables...
2025-04-15 11:02:58,957 - INFO - [1mStart Processing file: theory_evaluation\llm_handler.py[0m
2025-04-15 11:03:10,541 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 11:03:10,546 - INFO - Hello World 1
2025-04-15 11:03:14,255 - INFO - pytest_fixture - 
@pytest.fixture
def mock_env_vars():
    with patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"mock_{key}"):
        yield

@pytest.fixture
def mock_openai_llm_client():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", new_callable=AsyncMock) as mock_client:
        yield mock_client

@pytest.fixture
def mock_openai_llm_run():
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
        yield mock_run

2025-04-15 11:03:14,258 - INFO - Hello World 1
2025-04-15 11:03:22,750 - INFO - pytest_fixture - 
import asyncio
import json
import os
import pytest
from unittest.mock import patch, AsyncMock
from theory_evaluation.llm_handler import OpenAI_llm, DEFAULT_MESSAGE, DEFAULT_CONFIG

@pytest.mark.asyncio
async def test_openai_llm_initialization():
    with patch("theory_evaluation.llm_handler.AzureOpenAI") as mock_azure_openai, \
         patch("theory_evaluation.llm_handler.OpenAI") as mock_openai, \
         patch("theory_evaluation.llm_handler.os.getenv", side_effect=lambda key: f"mock_{key}"):
        
        llm = OpenAI_llm(
            useAzureOpenAI=True,
            message=DEFAULT_MESSAGE,
            output="json",
            mode="text_generation",
            verbose=True
        )
        
        assert llm.message == DEFAULT_MESSAGE
        assert llm.output == "json"
        assert llm.mode == "text_generation"
        assert llm.verbose is True
        assert hasattr(llm, "client")
        assert mock_azure_openai.called

@pytest.mark.asyncio
async def test_openai_llm_openai_json_completion():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", new_callable=AsyncMock) as mock_client:
        mock_client.chat.completions.create.return_value.choices = [
            AsyncMock(message=AsyncMock(content=json.dumps({"answer": "42", "explanation": "The answer to life"})))
        ]
        
        llm = OpenAI_llm(message=DEFAULT_MESSAGE)
        result = await llm._OpenAI_JSON_Completion(messages=[{"role": "system", "content": DEFAULT_MESSAGE}])
        
        assert result == {"answer": "42", "explanation": "The answer to life"}

@pytest.mark.asyncio
async def test_openai_llm_openai_streaming():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", new_callable=AsyncMock) as mock_client:
        mock_client.chat.completions.create.return_value = [
            AsyncMock(choices=[AsyncMock(delta=AsyncMock(content="streaming content"))])
        ]
        
        llm = OpenAI_llm(message=DEFAULT_MESSAGE, output="stream")
        result = [chunk async for chunk in llm._OpenAI_Streaming(messages=[{"role": "system", "content": DEFAULT_MESSAGE}])]
        
        assert result == ["streaming content"]

@pytest.mark.asyncio
async def test_openai_llm_openai_chat_completion():
    with patch("theory_evaluation.llm_handler.OpenAI_llm.client", new_callable=AsyncMock) as mock_client:
        mock_client.chat.completions.create.return_value.choices = [
            AsyncMock(message=AsyncMock(content="chat completion content"))
        ]
        
        llm = OpenAI_llm(message=DEFAULT_MESSAGE)
        result = await llm._OpenAI_Chat_Completion(messages=[{"role": "system", "content": DEFAULT_MESSAGE}])
        
        assert result == "chat completion content"

@pytest.mark.asyncio
async def test_openai_llm_execute_text_generation():
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock(__aiter__=lambda s: iter(["response content"]))
        
        llm = OpenAI_llm(message=DEFAULT_MESSAGE, output="json", mode="text_generation")
        result = [response async for response in llm.execute()]
        
        assert result == ["response content"]

@pytest.mark.asyncio
async def test_openai_llm_execute_vision():
    with patch("theory_evaluation.llm_handler.OpenAI_llm._run", new_callable=AsyncMock) as mock_run:
        mock_run.return_value = AsyncMock(__aiter__=lambda s: iter(["vision response content"]))
        
        llm = OpenAI_llm(message=DEFAULT_MESSAGE, output="json", mode="vision", image_input="mock_image_data")
        result = [response async for response in llm.execute()]
        
        assert result == ["vision response content"]

2025-04-15 11:03:22,750 - INFO - [1mEnd Processing file: theory_evaluation\llm_handler.py[0m

2025-04-15 11:03:22,751 - INFO - [1mStart Processing file: theory_evaluation\llm_utils.py[0m
2025-04-15 11:03:29,660 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-15 11:03:29,667 - INFO - Hello World 1
2025-04-15 11:03:36,977 - INFO - pytest_fixture - 
import pytest
from unittest.mock import patch, mock_open
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings

@pytest.fixture
def mock_open_file():
    def _mock_open_file(file_data):
        return patch("theory_evaluation.llm_utils.open", mock_open(read_data=file_data))
    return _mock_open_file

def test_initialise_prompt_success(mock_open_file):
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$key} test."
    expected_prompt = "This is a value test."

    with mock_open_file(config_yaml) as mock_file:
        with mock_open_file(prompt_txt) as mock_prompt_file:
            result = initialise_prompt(agent)
            assert result == expected_prompt
            mock_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/config.yaml")
            mock_prompt_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/prompt.txt", "r")

def test_initialise_prompt_missing_placeholder(mock_open_file):
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$missing_key} test."
    expected_prompt = "This is a {$missing_key} test."

    with mock_open_file(config_yaml) as mock_file:
        with mock_open_file(prompt_txt) as mock_prompt_file:
            result = initialise_prompt(agent)
            assert result == expected_prompt
            mock_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/config.yaml")
            mock_prompt_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/prompt.txt", "r")

def test_initialise_prompt_file_not_found():
    agent = "test_agent"

    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_prompt(agent)
        assert result is None

def test_initialise_settings_success(mock_open_file):
    agent = "test_agent"
    settings_yaml = "key: value"
    expected_settings = {"key": "value"}

    with mock_open_file(settings_yaml) as mock_file:
        result = initialise_settings(agent)
        assert result == expected_settings
        mock_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/llm_settings.yaml")

def test_initialise_settings_file_not_found():
    agent = "test_agent"

    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_settings(agent)
        assert result is None

2025-04-15 11:03:36,977 - INFO - Hello World 1
2025-04-15 11:03:42,408 - INFO - pytest_fixture - 
def test_initialise_prompt_success():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$key} test."
    expected_prompt = "This is a value test."

    with patch("theory_evaluation.llm_utils.open", mock_open(read_data=config_yaml)) as mock_file:
        with patch("theory_evaluation.llm_utils.open", mock_open(read_data=prompt_txt)) as mock_prompt_file:
            result = initialise_prompt(agent)
            assert result == expected_prompt
            mock_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/config.yaml")
            mock_prompt_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/prompt.txt", "r")

def test_initialise_prompt_missing_placeholder():
    agent = "test_agent"
    config_yaml = "key: value"
    prompt_txt = "This is a {$missing_key} test."
    expected_prompt = "This is a {$missing_key} test."

    with patch("theory_evaluation.llm_utils.open", mock_open(read_data=config_yaml)) as mock_file:
        with patch("theory_evaluation.llm_utils.open", mock_open(read_data=prompt_txt)) as mock_prompt_file:
            result = initialise_prompt(agent)
            assert result == expected_prompt
            mock_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/config.yaml")
            mock_prompt_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/prompt.txt", "r")

def test_initialise_prompt_file_not_found():
    agent = "test_agent"

    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_prompt(agent)
        assert result is None

def test_initialise_settings_success():
    agent = "test_agent"
    settings_yaml = "key: value"
    expected_settings = {"key": "value"}

    with patch("theory_evaluation.llm_utils.open", mock_open(read_data=settings_yaml)) as mock_file:
        result = initialise_settings(agent)
        assert result == expected_settings
        mock_file.assert_called_once_with(f"./theory_evaluation/evaluator/prompts/{agent}/llm_settings.yaml")

def test_initialise_settings_file_not_found():
    agent = "test_agent"

    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_settings(agent)
        assert result is None

2025-04-15 11:03:42,409 - INFO - [1mEnd Processing file: theory_evaluation\llm_utils.py[0m

2025-04-15 11:03:42,409 - INFO - [1mStart Processing file: theory_evaluation\__init__.py[0m
2025-04-15 11:03:42,410 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

