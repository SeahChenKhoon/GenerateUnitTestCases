2025-04-14 17:33:02,789 - INFO - Loading environment variables...
2025-04-14 17:33:03,108 - INFO - [1mStart Processing file: theory_evaluation\llm_handler.py[0m
2025-04-14 17:33:13,122 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-14 17:33:13,129 - INFO - 

2025-04-14 17:33:13,130 - INFO - TEST CASE 1 Retry 1
2025-04-14 17:33:13,130 - INFO - ---------------
2025-04-14 17:33:13,130 - INFO - def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
2025-04-14 17:33:13,133 - INFO - ---------------
2025-04-14 17:33:15,102 - INFO - passed 1- False
2025-04-14 17:33:15,102 - INFO - test_case_error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:6: in <module>
    from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
E   ImportError: cannot import name '_OpenAI_Chat_Completion' from 'theory_evaluation.llm_handler' (C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py)
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.06s
2025-04-14 17:33:15,102 - INFO - TEST CASE 1 Retry 2
2025-04-14 17:33:15,102 - INFO - ---------------
2025-04-14 17:33:15,102 - INFO - def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
2025-04-14 17:33:15,102 - INFO - ---------------
2025-04-14 17:33:15,651 - INFO - missing_import_statement 2- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:15,651 - INFO - new import statement 2- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:18,263 - INFO - TEST CASE 1 Retry 3
2025-04-14 17:33:18,263 - INFO - ---------------
2025-04-14 17:33:18,263 - INFO - def test_openai_llm_initialization():
    with patch('theory_evaluation.llm_handler.AzureOpenAI') as mock_azure_openai, \
         patch('theory_evaluation.llm_handler.OpenAI') as mock_openai, \
         patch('os.getenv', side_effect=lambda key: f'mock_{key}'):
        
        llm = OpenAI_llm(useAzureOpenAI=True)
        assert llm.client == mock_azure_openai.return_value
2025-04-14 17:33:18,263 - INFO - ---------------
2025-04-14 17:33:18,809 - INFO - missing_import_statement 3- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:18,809 - INFO - new import statement 3- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:21,620 - INFO - Failed after all retries for test case 1
2025-04-14 17:33:21,620 - INFO - 

2025-04-14 17:33:21,620 - INFO - TEST CASE 2 Retry 1
2025-04-14 17:33:21,620 - INFO - ---------------
2025-04-14 17:33:21,620 - INFO - def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = '{"key": "value"}'
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:21,620 - INFO - ---------------
2025-04-14 17:33:23,684 - INFO - passed 1- False
2025-04-14 17:33:23,684 - INFO - test_case_error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:6: in <module>
    from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
E   ImportError: cannot import name '_OpenAI_Chat_Completion' from 'theory_evaluation.llm_handler' (C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py)
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.17s
2025-04-14 17:33:23,684 - INFO - TEST CASE 2 Retry 2
2025-04-14 17:33:23,684 - INFO - ---------------
2025-04-14 17:33:23,684 - INFO - def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = '{"key": "value"}'
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:23,684 - INFO - ---------------
2025-04-14 17:33:24,386 - INFO - missing_import_statement 2- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:24,387 - INFO - new import statement 2- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:26,543 - INFO - TEST CASE 2 Retry 3
2025-04-14 17:33:26,543 - INFO - ---------------
2025-04-14 17:33:26,543 - INFO - def test_openai_json_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = '{"key": "value"}'
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:26,543 - INFO - ---------------
2025-04-14 17:33:27,099 - INFO - missing_import_statement 3- from theory_evaluation.llm_handler import _OpenAI_JSON_Completion
2025-04-14 17:33:27,099 - INFO - new import statement 3- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_JSON_Completion

2025-04-14 17:33:29,037 - INFO - Failed after all retries for test case 2
2025-04-14 17:33:29,038 - INFO - 

2025-04-14 17:33:29,038 - INFO - TEST CASE 3 Retry 1
2025-04-14 17:33:29,038 - INFO - ---------------
2025-04-14 17:33:29,038 - INFO - def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streamed content"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = [mock_chunk]
2025-04-14 17:33:29,038 - INFO - ---------------
2025-04-14 17:33:30,923 - INFO - passed 1- False
2025-04-14 17:33:30,923 - INFO - test_case_error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:6: in <module>
    from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
E   ImportError: cannot import name '_OpenAI_Chat_Completion' from 'theory_evaluation.llm_handler' (C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py)
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.21s
2025-04-14 17:33:30,923 - INFO - TEST CASE 3 Retry 2
2025-04-14 17:33:30,923 - INFO - ---------------
2025-04-14 17:33:30,923 - INFO - def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streamed content"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = [mock_chunk]
2025-04-14 17:33:30,923 - INFO - ---------------
2025-04-14 17:33:31,527 - INFO - missing_import_statement 2- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:31,528 - INFO - new import statement 2- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_JSON_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:33,469 - INFO - TEST CASE 3 Retry 3
2025-04-14 17:33:33,469 - INFO - ---------------
2025-04-14 17:33:33,469 - INFO - def test_openai_streaming():
    mock_chunk = MagicMock()
    mock_chunk.choices[0].delta.content = "streamed content"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = [mock_chunk]
2025-04-14 17:33:33,469 - INFO - ---------------
2025-04-14 17:33:34,203 - INFO - missing_import_statement 3- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:34,203 - INFO - new import statement 3- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_JSON_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:36,041 - INFO - Failed after all retries for test case 3
2025-04-14 17:33:36,041 - INFO - 

2025-04-14 17:33:36,041 - INFO - TEST CASE 4 Retry 1
2025-04-14 17:33:36,041 - INFO - ---------------
2025-04-14 17:33:36,041 - INFO - def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat content"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:36,041 - INFO - ---------------
2025-04-14 17:33:37,802 - INFO - passed 1- False
2025-04-14 17:33:37,802 - INFO - test_case_error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:6: in <module>
    from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
E   ImportError: cannot import name '_OpenAI_Chat_Completion' from 'theory_evaluation.llm_handler' (C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py)
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.12s
2025-04-14 17:33:37,802 - INFO - TEST CASE 4 Retry 2
2025-04-14 17:33:37,803 - INFO - ---------------
2025-04-14 17:33:37,803 - INFO - def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat content"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:37,803 - INFO - ---------------
2025-04-14 17:33:38,405 - INFO - missing_import_statement 2- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:38,405 - INFO - new import statement 2- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_JSON_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:40,928 - INFO - TEST CASE 4 Retry 3
2025-04-14 17:33:40,928 - INFO - ---------------
2025-04-14 17:33:40,928 - INFO - def test_openai_chat_completion():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "chat content"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:40,929 - INFO - ---------------
2025-04-14 17:33:41,557 - INFO - missing_import_statement 3- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:41,558 - INFO - new import statement 3- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_JSON_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:44,139 - INFO - Failed after all retries for test case 4
2025-04-14 17:33:44,140 - INFO - 

2025-04-14 17:33:44,140 - INFO - TEST CASE 5 Retry 1
2025-04-14 17:33:44,140 - INFO - ---------------
2025-04-14 17:33:44,140 - INFO - def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "generated text"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:44,140 - INFO - ---------------
2025-04-14 17:33:46,304 - INFO - passed 1- False
2025-04-14 17:33:46,305 - INFO - test_case_error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:6: in <module>
    from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
E   ImportError: cannot import name '_OpenAI_Chat_Completion' from 'theory_evaluation.llm_handler' (C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py)
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.35s
2025-04-14 17:33:46,305 - INFO - TEST CASE 5 Retry 2
2025-04-14 17:33:46,305 - INFO - ---------------
2025-04-14 17:33:46,305 - INFO - def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "generated text"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:46,305 - INFO - ---------------
2025-04-14 17:33:46,987 - INFO - missing_import_statement 2- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:46,988 - INFO - new import statement 2- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_JSON_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:48,955 - INFO - TEST CASE 5 Retry 3
2025-04-14 17:33:48,955 - INFO - ---------------
2025-04-14 17:33:48,955 - INFO - def test_execute_text_generation():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "generated text"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:48,955 - INFO - ---------------
2025-04-14 17:33:49,523 - INFO - missing_import_statement 3- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:49,523 - INFO - new import statement 3- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_JSON_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:51,486 - INFO - Failed after all retries for test case 5
2025-04-14 17:33:51,487 - INFO - 

2025-04-14 17:33:51,487 - INFO - TEST CASE 6 Retry 1
2025-04-14 17:33:51,487 - INFO - ---------------
2025-04-14 17:33:51,488 - INFO - def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:51,488 - INFO - ---------------
2025-04-14 17:33:53,506 - INFO - passed 1- False
2025-04-14 17:33:53,506 - INFO - test_case_error 1 - =================================== ERRORS ====================================
________________________ ERROR collecting temp/temp.py ________________________
ImportError while importing test module 'C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\temp\temp.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
temp\temp.py:6: in <module>
    from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
E   ImportError: cannot import name '_OpenAI_Chat_Completion' from 'theory_evaluation.llm_handler' (C:\ChenKhoon\JupyterNotebook\GenerateUnitTestCases\theory_evaluation\llm_handler.py)
=========================== short test summary info ===========================
ERROR temp/temp.py
!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
1 error in 1.10s
2025-04-14 17:33:53,506 - INFO - TEST CASE 6 Retry 2
2025-04-14 17:33:53,506 - INFO - ---------------
2025-04-14 17:33:53,506 - INFO - def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:53,506 - INFO - ---------------
2025-04-14 17:33:54,133 - INFO - missing_import_statement 2- from theory_evaluation.llm_handler import _OpenAI_Chat_Completion
2025-04-14 17:33:54,134 - INFO - new import statement 2- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_JSON_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

2025-04-14 17:33:55,997 - INFO - TEST CASE 6 Retry 3
2025-04-14 17:33:55,997 - INFO - ---------------
2025-04-14 17:33:55,998 - INFO - def test_execute_vision():
    mock_response = MagicMock()
    mock_response.choices[0].message.content = "vision response"
    mock_client = AsyncMock()
    mock_client.chat.completions.create.return_value = mock_response
2025-04-14 17:33:55,998 - INFO - ---------------
2025-04-14 17:33:56,588 - INFO - missing_import_statement 3- from unittest.mock import MagicMock, AsyncMock
2025-04-14 17:33:56,589 - INFO - new import statement 3- import asyncio
import json
import os

from openai import AzureOpenAI, OpenAI
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion, _OpenAI_JSON_Completion, _OpenAI_Streaming, __init__, _run, execute, main
from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_JSON_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from theory_evaluation.llm_handler import _OpenAI_Chat_Completion

from unittest.mock import MagicMock, AsyncMock

2025-04-14 17:33:58,787 - INFO - Failed after all retries for test case 6
2025-04-14 17:33:58,791 - INFO - [1mEnd Processing file: theory_evaluation\llm_handler.py[0m

2025-04-14 17:33:58,791 - INFO - [1mStart Processing file: theory_evaluation\llm_utils.py[0m
2025-04-14 17:34:06,280 - WARNING - Stripped Markdown-style triple backtick fences from LLM output.
2025-04-14 17:34:06,284 - INFO - 

2025-04-14 17:34:06,285 - INFO - TEST CASE 1 Retry 1
2025-04-14 17:34:06,285 - INFO - ---------------
2025-04-14 17:34:06,285 - INFO - def test_initialise_prompt_success():
    agent = "test_agent"
    config_values = {'key1': 'value1', 'key2': 'value2'}
    prompt_structure = "This is a test prompt with {$key1} and {$key2}."
2025-04-14 17:34:06,285 - INFO - ---------------
2025-04-14 17:34:06,898 - INFO - passed 1- True
2025-04-14 17:34:06,899 - INFO - test_case_error 1 - .                                                                        [100%]
1 passed in 0.04s
2025-04-14 17:34:06,899 - INFO - 

2025-04-14 17:34:06,899 - INFO - TEST CASE 2 Retry 1
2025-04-14 17:34:06,899 - INFO - ---------------
2025-04-14 17:34:06,899 - INFO - def test_initialise_prompt_missing_placeholder():
    agent = "test_agent"
    config_values = {'key1': 'value1'}
    prompt_structure = "This is a test prompt with {$key1} and {$key2}."
2025-04-14 17:34:06,899 - INFO - ---------------
2025-04-14 17:34:07,692 - INFO - passed 1- True
2025-04-14 17:34:07,693 - INFO - test_case_error 1 - .                                                                        [100%]
1 passed in 0.16s
2025-04-14 17:34:07,693 - INFO - 

2025-04-14 17:34:07,693 - INFO - TEST CASE 3 Retry 1
2025-04-14 17:34:07,693 - INFO - ---------------
2025-04-14 17:34:07,693 - INFO - def test_initialise_prompt_exception():
    agent = "test_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_prompt(agent)
        assert result is None
2025-04-14 17:34:07,693 - INFO - ---------------
2025-04-14 17:34:08,378 - INFO - passed 1- False
2025-04-14 17:34:08,379 - INFO - test_case_error 1 - F                                                                        [100%]
================================== FAILURES ===================================
______________________ test_initialise_prompt_exception _______________________
temp\temp.py:8: in test_initialise_prompt_exception
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
E   NameError: name 'patch' is not defined
=========================== short test summary info ===========================
FAILED temp/temp.py::test_initialise_prompt_exception - NameError: name 'patc...
1 failed in 0.12s
2025-04-14 17:34:08,379 - INFO - TEST CASE 3 Retry 2
2025-04-14 17:34:08,379 - INFO - ---------------
2025-04-14 17:34:08,380 - INFO - def test_initialise_prompt_exception():
    agent = "test_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_prompt(agent)
        assert result is None
2025-04-14 17:34:08,380 - INFO - ---------------
2025-04-14 17:34:08,821 - INFO - missing_import_statement 2- from unittest.mock import patch
2025-04-14 17:34:08,821 - INFO - new import statement 2- import os
import re
import yaml
from theory_evaluation.llm_utils import initialise_prompt, initialise_settings
from unittest.mock import patch

2025-04-14 17:34:09,695 - INFO - passed 2- True
2025-04-14 17:34:09,695 - INFO - test_case_error 2 - .                                                                        [100%]
1 passed in 0.12s
2025-04-14 17:34:09,695 - INFO - 

2025-04-14 17:34:09,695 - INFO - TEST CASE 4 Retry 1
2025-04-14 17:34:09,695 - INFO - ---------------
2025-04-14 17:34:09,695 - INFO - def test_initialise_settings_success():
    agent = "test_agent"
    settings_data = {'setting1': 'value1', 'setting2': 'value2'}
2025-04-14 17:34:09,695 - INFO - ---------------
2025-04-14 17:34:10,391 - INFO - passed 1- True
2025-04-14 17:34:10,391 - INFO - test_case_error 1 - .                                                                        [100%]
1 passed in 0.08s
2025-04-14 17:34:10,391 - INFO - 

2025-04-14 17:34:10,391 - INFO - TEST CASE 5 Retry 1
2025-04-14 17:34:10,391 - INFO - ---------------
2025-04-14 17:34:10,391 - INFO - def test_initialise_settings_exception():
    agent = "test_agent"
    with patch("theory_evaluation.llm_utils.open", side_effect=FileNotFoundError):
        result = initialise_settings(agent)
        assert result is None
2025-04-14 17:34:10,391 - INFO - ---------------
2025-04-14 17:34:10,966 - INFO - passed 1- True
2025-04-14 17:34:10,966 - INFO - test_case_error 1 - .                                                                        [100%]
1 passed in 0.09s
2025-04-14 17:34:10,968 - INFO - [1mEnd Processing file: theory_evaluation\llm_utils.py[0m

2025-04-14 17:34:10,968 - INFO - [1mStart Processing file: theory_evaluation\__init__.py[0m
2025-04-14 17:34:10,968 - WARNING - No public functions found in theory_evaluation\__init__.py. Skipping test generation.

